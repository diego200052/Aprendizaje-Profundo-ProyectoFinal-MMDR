{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVgkMGRfg2Zl"
      },
      "source": [
        "# Preparación del entorno para entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8op6FLXF-52"
      },
      "source": [
        "#### Se clona el repositorio del paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAv61YbWg2Zz",
        "outputId": "5c88107f-c71f-4ed3-b5e1-804071e05975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SymbolicMathematics'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 28 (delta 2), reused 0 (delta 0), pack-reused 20\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf SymbolicMathematics\n",
        "!git clone https://github.com/facebookresearch/SymbolicMathematics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vs0v329g2Z4"
      },
      "source": [
        "#### Se descarga el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXddHOOFg2Z6",
        "outputId": "7f5305c3-3466-46ed-a73c-6bef2b9ab12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 04:43:11--  https://docs.google.com/uc?export=download&confirm=t&id=15pcZS5Rg5_BvLxmW4De7RTBbSamMChad\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.148.101, 142.250.148.102, 142.250.148.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.148.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p744tr2o29t4lttpv81kg6vul94oqsgm/1670215350000/01049389482803376309/*/15pcZS5Rg5_BvLxmW4De7RTBbSamMChad?e=download&uuid=a472353d-b509-44cd-9735-f026bb2891a7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-05 04:43:11--  https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p744tr2o29t4lttpv81kg6vul94oqsgm/1670215350000/01049389482803376309/*/15pcZS5Rg5_BvLxmW4De7RTBbSamMChad?e=download&uuid=a472353d-b509-44cd-9735-f026bb2891a7\n",
            "Resolving doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)... 74.125.70.132, 2607:f8b0:4001:c02::84\n",
            "Connecting to doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)|74.125.70.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1710930049 (1.6G) [application/gzip]\n",
            "Saving to: ‘prim_fwd.tar.gz’\n",
            "\n",
            "prim_fwd.tar.gz     100%[===================>]   1.59G   205MB/s    in 8.2s    \n",
            "\n",
            "2022-12-05 04:43:19 (199 MB/s) - ‘prim_fwd.tar.gz’ saved [1710930049/1710930049]\n",
            "\n",
            "prim_fwd.train\n",
            "prim_fwd.valid\n",
            "prim_fwd.test\n"
          ]
        }
      ],
      "source": [
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/data/prim_fwd.tar.gz\n",
        "!rm -f prim_fwd.tar.gz\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=15pcZS5Rg5_BvLxmW4De7RTBbSamMChad' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=15pcZS5Rg5_BvLxmW4De7RTBbSamMChad\" -O prim_fwd.tar.gz && rm -rf /tmp/cookies.txt\n",
        "\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/data/prim_bwd.tar.gz\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/data/prim_ibp.tar.gz\n",
        "\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/data/ode1.tar.gz\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/data/ode2.tar.gz\n",
        "\n",
        "!tar -xvf prim_fwd.tar.gz\n",
        "#!tar -xvf prim_bwd.tar.gz\n",
        "#!tar -xvf prim_ibp.tar.gz\n",
        "\n",
        "#!tar -xvf ode1.tar.gz\n",
        "#!tar -xvf ode2.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS1m03-7g2aA"
      },
      "source": [
        "#### Se descarga el modelo preentrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-xWaq4Rg2aB",
        "outputId": "f37e8e50-1758-4146-c726-bb682e1b6f41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 04:44:24--  https://docs.google.com/uc?export=download&confirm=t&id=1plWyWdmpmx4IQ-oU0Ykwu0h1rVcaPBFm\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.148.102, 142.250.148.100, 142.250.148.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.148.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-14-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/cibj0pl18lhb5r3b5u6qff6kn9i1gieo/1670215425000/01049389482803376309/*/1plWyWdmpmx4IQ-oU0Ykwu0h1rVcaPBFm?e=download&uuid=c938b1b6-1f5f-49f1-9a24-f86691d8ebe0 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-05 04:44:24--  https://doc-14-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/cibj0pl18lhb5r3b5u6qff6kn9i1gieo/1670215425000/01049389482803376309/*/1plWyWdmpmx4IQ-oU0Ykwu0h1rVcaPBFm?e=download&uuid=c938b1b6-1f5f-49f1-9a24-f86691d8ebe0\n",
            "Resolving doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)... 74.125.70.132, 2607:f8b0:4001:c02::84\n",
            "Connecting to doc-14-bk-docs.googleusercontent.com (doc-14-bk-docs.googleusercontent.com)|74.125.70.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1849506684 (1.7G) [application/octet-stream]\n",
            "Saving to: ‘fwd_bwd_ibp.pth’\n",
            "\n",
            "fwd_bwd_ibp.pth     100%[===================>]   1.72G   176MB/s    in 9.2s    \n",
            "\n",
            "2022-12-05 04:44:34 (193 MB/s) - ‘fwd_bwd_ibp.pth’ saved [1849506684/1849506684]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd.pth\n",
        "!rm -f bwd.pth\n",
        "!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/bwd.pth\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ibp.pth\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ibp_bwd.pth\n",
        "\n",
        "!rm -f fwd_bwd_ibp.pth\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd_ibp.pth\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1plWyWdmpmx4IQ-oU0Ykwu0h1rVcaPBFm' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1plWyWdmpmx4IQ-oU0Ykwu0h1rVcaPBFm\" -O fwd_bwd_ibp.pth && rm -rf /tmp/cookies.txt\n",
        "\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ode1.pth\n",
        "#!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/ode2.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbUxcrN4g2aE",
        "outputId": "eaab8c54-0ebc-4ad0-94a6-982242e0ffb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fwd_bwd_ibp.pth  prim_fwd.test\t prim_fwd.valid  SymbolicMathematics\n",
            "prim_fwd.tar.gz  prim_fwd.train  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedX-JosGs9V"
      },
      "source": [
        "## Instalación de nvidia apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAz4j62Sg2aG",
        "outputId": "2631e797-0089-46ba-a330-5a98f63429bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzCO4pw6lEgU",
        "outputId": "528c1c73-8e70-4c1a-83d1-1d1eb63518a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  5 04:44:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0    51W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XMrNg9-g2aL",
        "outputId": "efa379d0-c763-4e39-fe16-550b1ace216a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.3\n",
            "1.12.1+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.version.cuda)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPls9Wadg2aQ",
        "outputId": "aaf77136-dce3-4505-e4a3-0b73adf9b339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "TORCH_MAJOR = int(torch.__version__.split('.')[0])\n",
        "TORCH_MINOR = int(torch.__version__.split('.')[1])\n",
        "\n",
        "print(TORCH_MAJOR)\n",
        "print(TORCH_MINOR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XyJjW4Fg2aW",
        "outputId": "5452b22e-832e-4046-a098-f057a62345cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10667, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 10667 (delta 107), reused 120 (delta 67), pack-reused 10478\u001b[K\n",
            "Receiving objects: 100% (10667/10667), 15.18 MiB | 28.32 MiB/s, done.\n",
            "Resolving deltas: 100% (7334/7334), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf apex\n",
        "!git clone https://github.com/NVIDIA/apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27DIAfy1g2aa",
        "outputId": "0746744b-8c45-4633-9108-6a70bd7a24c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex  docs\tLICENSE    requirements_dev.txt  setup.py\n",
            "csrc  examples\tREADME.md  requirements.txt\t tests\n"
          ]
        }
      ],
      "source": [
        "!ls ./apex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8CDlyqnJL2f"
      },
      "source": [
        "Elimina una restricción del setup que impide instalar apex debido a las versiones de cuda y pytorch+cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKHXlgyTI1d9",
        "outputId": "da5dcd9d-f2fd-425e-be5a-5cf6c6bb5e34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'setup.py': No such file or directory\n",
            "--2022-12-05 04:44:39--  https://docs.google.com/uc?export=download&confirm=t&id=1uS9UO0tFKZiaJU2FahDQuhA8zu337iGG\n",
            "Resolving docs.google.com (docs.google.com)... 142.250.148.102, 142.250.148.100, 142.250.148.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.250.148.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p2aqm221ntbecub7cmptl4irhoeg196r/1670215425000/01049389482803376309/*/1uS9UO0tFKZiaJU2FahDQuhA8zu337iGG?e=download&uuid=560395bd-a9dc-4bd2-b90e-ec1ef79de780 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-12-05 04:44:40--  https://doc-0s-bk-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/p2aqm221ntbecub7cmptl4irhoeg196r/1670215425000/01049389482803376309/*/1uS9UO0tFKZiaJU2FahDQuhA8zu337iGG?e=download&uuid=560395bd-a9dc-4bd2-b90e-ec1ef79de780\n",
            "Resolving doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)... 74.125.70.132, 2607:f8b0:4001:c02::84\n",
            "Connecting to doc-0s-bk-docs.googleusercontent.com (doc-0s-bk-docs.googleusercontent.com)|74.125.70.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32913 (32K) [text/plain]\n",
            "Saving to: ‘setup.py’\n",
            "\n",
            "setup.py            100%[===================>]  32.14K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-05 04:44:40 (105 MB/s) - ‘setup.py’ saved [32913/32913]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!rm setup.py\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1uS9UO0tFKZiaJU2FahDQuhA8zu337iGG' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1uS9UO0tFKZiaJU2FahDQuhA8zu337iGG\" -O setup.py && rm -rf /tmp/cookies.txt\n",
        "!rm ./apex/setup.py\n",
        "!mv setup.py ./apex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbM8R6rzJVYs"
      },
      "source": [
        "Comienza la instalación de nvidia apex desde el código fuente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WDuXc4-g2af",
        "outputId": "1eaabd5b-7c36-4e65-ff68-a04eed3ba16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-lnxiexct\n",
            "Created temporary directory: /tmp/pip-req-tracker-9fptdtpg\n",
            "Initialized build tracking at /tmp/pip-req-tracker-9fptdtpg\n",
            "Created build tracker: /tmp/pip-req-tracker-9fptdtpg\n",
            "Entered build tracker: /tmp/pip-req-tracker-9fptdtpg\n",
            "Created temporary directory: /tmp/pip-install-ogg87wls\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-0rs37kqw\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-9fptdtpg'\n",
            "    Running setup.py (path:/tmp/pip-req-build-0rs37kqw/setup.py) egg_info for package from file:///content/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-in1lyb2e\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.12.1+cu113\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/dependency_links.txt\n",
            "    writing requirements to /tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/requires.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-in1lyb2e/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-0rs37kqw has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-9fptdtpg'\n",
            "Requirement already satisfied: packaging>20.6 in /usr/local/lib/python3.8/dist-packages (from apex==0.1) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>20.6->apex==0.1) (3.0.9)\n",
            "Created temporary directory: /tmp/pip-unpack-xb16dom2\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.8/apex\n",
            "  sysconfig: /usr/include/python3.8/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "  Created temporary directory: /tmp/pip-record-6onnkqbf\n",
            "    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-0rs37kqw/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-0rs37kqw/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-6onnkqbf/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.12.1+cu113\n",
            "\n",
            "\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "    Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "    Cuda compilation tools, release 11.2, V11.2.152\n",
            "    Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.8\n",
            "    creating build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/_ucc_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/cudnn_gbn\n",
            "    copying apex/contrib/cudnn_gbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/cudnn_gbn\n",
            "    copying apex/contrib/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/cudnn_gbn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/_transducer_ref.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test\n",
            "    copying apex/contrib/test/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/fmha\n",
            "    copying apex/contrib/test/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/fmha\n",
            "    copying apex/contrib/test/fmha/test_fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/fmha\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/cudnn_gbn\n",
            "    copying apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/cudnn_gbn\n",
            "    copying apex/contrib/test/cudnn_gbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/cudnn_gbn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/layer_norm\n",
            "    copying apex/contrib/test/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/layer_norm\n",
            "    copying apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/transducer\n",
            "    copying apex/contrib/test/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/transducer\n",
            "    copying apex/contrib/test/transducer/test_transducer_joint.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/transducer\n",
            "    copying apex/contrib/test/transducer/test_transducer_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/transducer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/xentropy\n",
            "    copying apex/contrib/test/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/xentropy\n",
            "    copying apex/contrib/test/xentropy/test_label_smoothing.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/xentropy\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/conv_bias_relu\n",
            "    copying apex/contrib/test/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/conv_bias_relu\n",
            "    copying apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/conv_bias_relu\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/peer_memory\n",
            "    copying apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/peer_memory\n",
            "    copying apex/contrib/test/peer_memory/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/peer_memory\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/index_mul_2d\n",
            "    copying apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/index_mul_2d\n",
            "    copying apex/contrib/test/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/index_mul_2d\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    copying apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers\n",
            "    copying apex/contrib/test/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers\n",
            "    copying apex/contrib/test/optimizers/test_dist_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers\n",
            "    copying apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/focal_loss\n",
            "    copying apex/contrib/test/focal_loss/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/focal_loss\n",
            "    copying apex/contrib/test/focal_loss/test_focal_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/focal_loss\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/bottleneck\n",
            "    copying apex/contrib/test/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/bottleneck\n",
            "    copying apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/test/clip_grad\n",
            "    copying apex/contrib/test/clip_grad/test_clip_grad.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/clip_grad\n",
            "    copying apex/contrib/test/clip_grad/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/test/clip_grad\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:411: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:813: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.3). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.8\n",
            "    creating build/temp.linux-x86_64-3.8/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.8/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/welford.cu -o build/temp.linux-x86_64-3.8/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/syncbn.o build/temp.linux-x86_64-3.8/csrc/welford.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:157:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:178:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:179:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:180:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:202:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:270:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:271:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:272:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:273:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:274:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:275:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm(at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:313:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:332:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:333:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:353:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor rms_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:390:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:391:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:392:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:413:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:414:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:415:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:205:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:460:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:416:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.8/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:86: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "                                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "                                                               ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/mlp.o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_dense_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:30:63: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                   ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:33:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:35:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:64:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "                                                                         ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:68:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias = at::empty({out_features}, input.type());\n",
            "                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:70:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:73:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:75:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:106:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:107:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:108:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:111:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:113:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:149:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "                                                                             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:150:74: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "                                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:151:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:152:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias2 = at::empty({out_features}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:153:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:154:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                            ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:157:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:159:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:278:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                               \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/DeviceGuard.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:213:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:280:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                      \\\n",
            "                                                            ^\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:178:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:283:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Double, double, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:284:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Float, float, __VA_ARGS__)    \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:68:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "         return __VA_ARGS__();                                                        \\\n",
            "                ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:285:7: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "           AT_PRIVATE_CASE_TYPE(NAME, at::ScalarType::Half, at::Half, __VA_ARGS__)  \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/fused_dense.o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.8/csrc/megatron\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'generic_scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_weight_gradient_mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-0rs37kqw/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/SymInt.h(84): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex\n",
            "    copying build/lib.linux-x86_64-3.8/apex/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/_autocast_utils.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/cudnn_gbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/cudnn_gbn/batch_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/cudnn_gbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/cudnn_gbn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/cudnn_gbn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/_transducer_ref.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/conv_bias_relu.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_memory.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/index_mul_2d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/fmha/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/fmha/test_fmha.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/fmha\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/cudnn_gbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/cudnn_gbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/cudnn_gbn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/cudnn_gbn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/layer_norm/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/layer_norm\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/transducer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/transducer/test_transducer_joint.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/transducer/test_transducer_loss.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/xentropy/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/xentropy/test_label_smoothing.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/xentropy\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/conv_bias_relu/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/conv_bias_relu\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/peer_memory/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/peer_memory\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/index_mul_2d/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/index_mul_2d\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers/test_dist_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/focal_loss/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/focal_loss/test_focal_loss.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/focal_loss\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/bottleneck/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/bottleneck/test_bottleneck_module.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/bottleneck\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/test/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/clip_grad/test_clip_grad.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/test/clip_grad/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/test/clip_grad\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/focal_loss.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_lib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/halo_exchangers.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/clip_grad.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/cells.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/models.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_mixed_precision_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/_batchsampler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/enums.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/grad_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_ucc_util.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/parallel_state.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/log_util.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_bert.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_transformer_lm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/commons.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/distributed_test_base.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/microbatches.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/rnn_compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__version__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/amp.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_initialize.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/wrap.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_amp_state.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/handle.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/opt.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/frontend.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/LARC.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/distributed.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/multiproc.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/mlp.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/cudnn_gbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/cudnn_gbn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/_transducer_ref.py to _transducer_ref.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/fmha/test_fmha.py to test_fmha.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py to test_cudnn_gbn_with_two_gpus.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/cudnn_gbn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/layer_norm/test_fast_layer_norm.py to test_fast_layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer/test_transducer_joint.py to test_transducer_joint.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/transducer/test_transducer_loss.py to test_transducer_loss.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/xentropy/test_label_smoothing.py to test_label_smoothing.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py to test_conv_bias_relu.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py to test_peer_halo_exchange_module.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/index_mul_2d/test_index_mul_2d.py to test_index_mul_2d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py to test_encdec_multihead_attn_norm_add.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_self_multihead_attn.py to test_self_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py to test_mha_fused_softmax.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py to test_fast_self_multihead_attn_bias.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py to test_self_multihead_attn_norm_add.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py to test_encdec_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers/test_dist_adam.py to test_dist_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/optimizers/test_distributed_fused_lamb.py to test_distributed_fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/focal_loss/test_focal_loss.py to test_focal_loss.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/bottleneck/test_bottleneck_module.py to test_bottleneck_module.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/clip_grad/test_clip_grad.py to test_clip_grad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/test/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/cells.py to cells.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/models.py to models.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/enums.py to enums.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_ucc_util.py to _ucc_util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/compat.py to compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/amp.py to amp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/handle.py to handle.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/opt.py to opt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing requirements to apex.egg-info/requires.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-6onnkqbf/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-9fptdtpg'\n"
          ]
        }
      ],
      "source": [
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YddN-sFyg2ak"
      },
      "outputs": [],
      "source": [
        "# No lo reconoce instalado python\n",
        "#!conda install -y -c conda-forge nvidia-apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNNE_cCng2am"
      },
      "outputs": [],
      "source": [
        "import apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwm01Cuzg2ao"
      },
      "outputs": [],
      "source": [
        "import collections.abc as container_abcs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEkehL-mg2ap"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "from apex import amp "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjBjvqYQG0P3"
      },
      "source": [
        "## Instalación de bibliotecas necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1YylXAjg2as",
        "outputId": "c9d4e5c0-5981-4240-feb9-8cc58b34ae53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sympy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Pw_nveG4jm"
      },
      "source": [
        "# Generación del conjunto de datos\n",
        "\n",
        "Se toman los datos de las integrales por el método forward. El método forward genera una función aleatoria y calcula su integral. Este conjunto tiene fórmulas de entrada pequeñas y de salida grandes.\n",
        "\n",
        "Se creará una nueva tarea, cálculo de derivadas. Para ello, la fórmula de entrada ahora corresponde a la fórmula objetivo, y la fórmula objetivo ahora es la fórmula de entrada. Así el conjunto de datos posee fórmulas de entrada grandes y de salida pequeñas.\n",
        "\n",
        "Se empleará el modelo preentrenado por el método backward. Ya que fue entrenado con fórmulas de entrada grandes y soluciones pequeñas. Además, que eso hace una comparación justa al tener fórmulas que probablemente no ha visto ese modelo en su mayoría.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f bwd.pth\n",
        "!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/bwd.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DfmeLEsmor2",
        "outputId": "f8691011-2a44-45b9-bf0b-c7077e12ae4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 05:02:45--  https://dl.fbaipublicfiles.com/SymbolicMathematics/models/bwd.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1849505599 (1.7G) [application/octet-stream]\n",
            "Saving to: ‘bwd.pth’\n",
            "\n",
            "bwd.pth             100%[===================>]   1.72G  48.3MB/s    in 32s     \n",
            "\n",
            "2022-12-05 05:03:19 (54.3 MB/s) - ‘bwd.pth’ saved [1849505599/1849505599]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_1v-sKXHFOx",
        "outputId": "c569e9a9-cea9-4838-e84c-434b908c9a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t fwd_bwd_ibp.pth  prim_fwd.test   prim_fwd.valid  SymbolicMathematics\n",
            "bwd.pth  prim_fwd.tar.gz  prim_fwd.train  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('prim_fwd.test', 'r')\n",
        "file2 = open('deriv_fwd.test', 'w')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "number = 0\n",
        "for line in Lines:\n",
        "    if(number > 2000): #2,000 max\n",
        "      break\n",
        "    firstSplit = line.split(\"|\")\n",
        "    count = firstSplit[0]\n",
        "    eq = firstSplit[1].split(\"\\t\")\n",
        "    IntegralEQ = str(eq[1]).rstrip()\n",
        "    EntradaEQ = str(eq[0]).rstrip()\n",
        "    file2.write(str(count) + \"|sub Y \" + IntegralEQ + \"\\t\" + EntradaEQ[7:] + \"\\n\")\n",
        "    number += 1\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "R8c6ZwB5dxjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('prim_fwd.valid', 'r')\n",
        "file2 = open('deriv_fwd.valid', 'w')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "number = 0\n",
        "for line in Lines:\n",
        "    if(number > 2000): #2,000 max\n",
        "      break\n",
        "    firstSplit = line.split(\"|\")\n",
        "    count = firstSplit[0]\n",
        "    eq = firstSplit[1].split(\"\\t\")\n",
        "    IntegralEQ = str(eq[1]).rstrip()\n",
        "    EntradaEQ = str(eq[0]).rstrip()\n",
        "    file2.write(str(count) + \"|sub Y \" + IntegralEQ + \"\\t\" + EntradaEQ[7:] + \"\\n\")\n",
        "    number += 1\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "50RxMAhslnAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('prim_fwd.train', 'r')\n",
        "file2 = open('deriv_fwd.train', 'w')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "number = 0\n",
        "for line in Lines:\n",
        "    if(number > 100000): #100,000 max\n",
        "      break\n",
        "    firstSplit = line.split(\"|\")\n",
        "    count = firstSplit[0]\n",
        "    eq = firstSplit[1].split(\"\\t\")\n",
        "    IntegralEQ = str(eq[1]).rstrip()\n",
        "    EntradaEQ = str(eq[0]).rstrip()\n",
        "    file2.write(str(count) + \"|sub Y \" + IntegralEQ + \"\\t\" + EntradaEQ[7:] + \"\\n\")\n",
        "    number += 1\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "vspWo2M7lg8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento y evaluación del modelo, primer experimento\n",
        "Se toman 100,000 fórmulas para realizar el entrenamiento, 2,000 para el conjunto de validación y 2,000 para el conjunto de prueba."
      ],
      "metadata": {
        "id": "NqZ1CzEGm6GC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjCXWJRag2at",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a529e3b-5a07-4dd6-a572-5f02b106c80d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLURM job: False\n",
            "0 - Number of nodes: 1\n",
            "0 - Node ID        : 0\n",
            "0 - Local rank     : 0\n",
            "0 - Global rank    : 0\n",
            "0 - World size     : 1\n",
            "0 - GPUs per node  : 1\n",
            "0 - Master         : True\n",
            "0 - Multi-node     : False\n",
            "0 - Multi-GPU      : False\n",
            "0 - Hostname       : e1165a85fa0b\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - accumulate_gradients: 1\n",
            "                                     amp: 2\n",
            "                                     attention_dropout: 0\n",
            "                                     balanced: False\n",
            "                                     batch_size: 128\n",
            "                                     beam_early_stopping: True\n",
            "                                     beam_eval: False\n",
            "                                     beam_length_penalty: 1\n",
            "                                     beam_size: 1\n",
            "                                     clean_prefix_expr: True\n",
            "                                     clip_grad_norm: 5\n",
            "                                     command: python ./SymbolicMathematics/main.py --exp_name derivfwd_modelbwd --fp16 true --amp 2 --tasks prim_bwd --reload_model 'bwd.pth' --reload_data 'prim_bwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_fwd_acc --exp_id \"3snai9209s\"\n",
            "                                     cpu: False\n",
            "                                     debug: False\n",
            "                                     debug_slurm: False\n",
            "                                     dropout: 0\n",
            "                                     dump_path: ./dumped/derivfwd_modelbwd/3snai9209s\n",
            "                                     emb_dim: 1024\n",
            "                                     env_base_seed: 0\n",
            "                                     env_name: char_sp\n",
            "                                     epoch_size: 100000\n",
            "                                     eval_only: False\n",
            "                                     eval_verbose: 0\n",
            "                                     eval_verbose_print: False\n",
            "                                     exp_id: 3snai9209s\n",
            "                                     exp_name: derivfwd_modelbwd\n",
            "                                     export_data: False\n",
            "                                     fp16: True\n",
            "                                     global_rank: 0\n",
            "                                     int_base: 10\n",
            "                                     is_master: True\n",
            "                                     is_slurm_job: False\n",
            "                                     leaf_probs: 0.75,0,0.25,0\n",
            "                                     local_rank: 0\n",
            "                                     master_port: -1\n",
            "                                     max_epoch: 100000\n",
            "                                     max_int: 10000\n",
            "                                     max_len: 512\n",
            "                                     max_ops: 10\n",
            "                                     max_ops_G: 4\n",
            "                                     multi_gpu: False\n",
            "                                     multi_node: False\n",
            "                                     n_coefficients: 0\n",
            "                                     n_dec_layers: 6\n",
            "                                     n_enc_layers: 6\n",
            "                                     n_gpu_per_node: 1\n",
            "                                     n_heads: 8\n",
            "                                     n_nodes: 1\n",
            "                                     n_variables: 1\n",
            "                                     node_id: 0\n",
            "                                     num_workers: 10\n",
            "                                     operators: add:2,sub:1\n",
            "                                     optimizer: adam,lr=0.0001\n",
            "                                     positive: False\n",
            "                                     precision: 10\n",
            "                                     reload_checkpoint: \n",
            "                                     reload_data: prim_bwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test\n",
            "                                     reload_model: bwd.pth\n",
            "                                     reload_size: 100000\n",
            "                                     rewrite_functions: \n",
            "                                     same_nb_ops_per_batch: False\n",
            "                                     save_periodic: 0\n",
            "                                     share_inout_emb: True\n",
            "                                     sinusoidal_embeddings: False\n",
            "                                     stopping_criterion: \n",
            "                                     tasks: prim_bwd\n",
            "                                     validation_metrics: valid_prim_fwd_acc\n",
            "                                     world_size: 1\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - The experiment will be stored in ./dumped/derivfwd_modelbwd/3snai9209s\n",
            "                                     \n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - Running command: python ./SymbolicMathematics/main.py --exp_name derivfwd_modelbwd --fp16 true --amp 2 --tasks prim_bwd --reload_model 'bwd.pth' --reload_data 'prim_bwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_fwd_acc\n",
            "\n",
            "WARNING - 12/05/22 05:03:39 - 0:00:00 - Signal handler installed.\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - Unary operators: []\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - Binary operators: ['add', 'sub']\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - words: {'<s>': 0, '</s>': 1, '<pad>': 2, '(': 3, ')': 4, '<SPECIAL_5>': 5, '<SPECIAL_6>': 6, '<SPECIAL_7>': 7, '<SPECIAL_8>': 8, '<SPECIAL_9>': 9, 'pi': 10, 'E': 11, 'x': 12, 'y': 13, 'z': 14, 't': 15, 'a0': 16, 'a1': 17, 'a2': 18, 'a3': 19, 'a4': 20, 'a5': 21, 'a6': 22, 'a7': 23, 'a8': 24, 'a9': 25, 'abs': 26, 'acos': 27, 'acosh': 28, 'acot': 29, 'acoth': 30, 'acsc': 31, 'acsch': 32, 'add': 33, 'asec': 34, 'asech': 35, 'asin': 36, 'asinh': 37, 'atan': 38, 'atanh': 39, 'cos': 40, 'cosh': 41, 'cot': 42, 'coth': 43, 'csc': 44, 'csch': 45, 'derivative': 46, 'div': 47, 'exp': 48, 'f': 49, 'g': 50, 'h': 51, 'inv': 52, 'ln': 53, 'mul': 54, 'pow': 55, 'pow2': 56, 'pow3': 57, 'pow4': 58, 'pow5': 59, 'rac': 60, 'sec': 61, 'sech': 62, 'sign': 63, 'sin': 64, 'sinh': 65, 'sqrt': 66, 'sub': 67, 'tan': 68, 'tanh': 69, 'I': 70, 'INT+': 71, 'INT-': 72, 'INT': 73, 'FLOAT': 74, '-': 75, '.': 76, '10^': 77, 'Y': 78, \"Y'\": 79, \"Y''\": 80, '0': 81, '1': 82, '2': 83, '3': 84, '4': 85, '5': 86, '6': 87, '7': 88, '8': 89, '9': 90}\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - 20001 possible leaves.\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - Checking expressions in [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 2.1, 3.1, -0.01, -0.1, -0.3, -0.5, -0.7, -0.9, -1.1, -2.1, -3.1]\n",
            "INFO - 12/05/22 05:03:39 - 0:00:00 - Training tasks: prim_bwd\n",
            "INFO - 12/05/22 05:03:41 - 0:00:02 - Reloading modules from bwd.pth ...\n",
            "INFO - 12/05/22 05:03:45 - 0:00:06 - Number of parameters (encoder): 79866880\n",
            "INFO - 12/05/22 05:03:45 - 0:00:06 - Number of parameters (decoder): 105069659\n",
            "INFO - 12/05/22 05:03:46 - 0:00:06 - Found 261 parameters in model.\n",
            "INFO - 12/05/22 05:03:46 - 0:00:06 - Optimizers: model\n",
            "/usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "INFO - 12/05/22 05:03:46 - 0:00:06 - Creating train iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:03:46 - 0:00:06 - Loading data from deriv_fwd.train ...\n",
            "INFO - 12/05/22 05:03:46 - 0:00:07 - Loaded 100000 equations from the disk.\n",
            "INFO - 12/05/22 05:03:46 - 0:00:07 - ============ Starting epoch 0 ... ============\n",
            "INFO - 12/05/22 05:03:46 - 0:00:07 - Initialized random generator for worker 0, with seed [0, 0, 0] (base seed=0).\n",
            "/usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/content/SymbolicMathematics/src/optim.py:72: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "INFO - 12/05/22 05:03:54 - 0:00:15 -      20 -  309.79 equations/s - 12854.56 words/s - PRIM-BWD:  0.9861 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:03:59 - 0:00:19 -      40 -  551.53 equations/s - 22987.17 words/s - PRIM-BWD:  0.2501 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:03 - 0:00:24 -      60 -  571.10 equations/s - 23609.57 words/s - PRIM-BWD:  0.1496 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:09 - 0:00:29 -      80 -  474.13 equations/s - 19939.33 words/s - PRIM-BWD:  0.1152 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:13 - 0:00:34 -     100 -  532.07 equations/s - 22245.03 words/s - PRIM-BWD:  0.0945 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:18 - 0:00:39 -     120 -  504.80 equations/s - 21427.71 words/s - PRIM-BWD:  0.0850 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:23 - 0:00:44 -     140 -  578.74 equations/s - 23819.95 words/s - PRIM-BWD:  0.0790 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:28 - 0:00:48 -     160 -  545.21 equations/s - 23166.45 words/s - PRIM-BWD:  0.0717 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:32 - 0:00:53 -     180 -  569.93 equations/s - 23738.82 words/s - PRIM-BWD:  0.0711 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:37 - 0:00:58 -     200 -  476.46 equations/s - 20026.30 words/s - PRIM-BWD:  0.0689 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:41 - 0:01:02 -     220 -  689.92 equations/s - 28151.54 words/s - PRIM-BWD:  0.0618 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:47 - 0:01:07 -     240 -  460.92 equations/s - 19452.26 words/s - PRIM-BWD:  0.0581 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:51 - 0:01:12 -     260 -  547.68 equations/s - 22765.18 words/s - PRIM-BWD:  0.0566 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:04:56 - 0:01:17 -     280 -  529.16 equations/s - 22002.87 words/s - PRIM-BWD:  0.0548 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:00 - 0:01:21 -     300 -  616.07 equations/s - 25294.45 words/s - PRIM-BWD:  0.0528 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:05 - 0:01:26 -     320 -  547.39 equations/s - 22639.70 words/s - PRIM-BWD:  0.0510 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:10 - 0:01:30 -     340 -  553.74 equations/s - 23079.81 words/s - PRIM-BWD:  0.0542 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:15 - 0:01:36 -     360 -  475.21 equations/s - 19994.25 words/s - PRIM-BWD:  0.0528 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:20 - 0:01:40 -     380 -  562.68 equations/s - 23588.06 words/s - PRIM-BWD:  0.0532 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:24 - 0:01:45 -     400 -  564.96 equations/s - 23461.89 words/s - PRIM-BWD:  0.0534 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:29 - 0:01:50 -     420 -  533.20 equations/s - 21984.69 words/s - PRIM-BWD:  0.0488 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:33 - 0:01:54 -     440 -  598.89 equations/s - 24855.07 words/s - PRIM-BWD:  0.0515 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:39 - 0:02:00 -     460 -  427.72 equations/s - 17795.65 words/s - PRIM-BWD:  0.0479 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:45 - 0:02:05 -     480 -  473.65 equations/s - 19663.70 words/s - PRIM-BWD:  0.0513 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:49 - 0:02:10 -     500 -  543.55 equations/s - 22676.63 words/s - PRIM-BWD:  0.0488 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:54 - 0:02:15 -     520 -  537.04 equations/s - 22406.44 words/s - PRIM-BWD:  0.0498 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:05:59 - 0:02:20 -     540 -  500.32 equations/s - 20918.20 words/s - PRIM-BWD:  0.0493 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:04 - 0:02:25 -     560 -  532.78 equations/s - 22363.12 words/s - PRIM-BWD:  0.0484 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:09 - 0:02:29 -     580 -  553.14 equations/s - 22876.84 words/s - PRIM-BWD:  0.0460 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:14 - 0:02:34 -     600 -  516.54 equations/s - 21446.27 words/s - PRIM-BWD:  0.0454 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:19 - 0:02:39 -     620 -  504.37 equations/s - 21499.21 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:23 - 0:02:44 -     640 -  545.04 equations/s - 22973.57 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:27 - 0:02:48 -     660 -  614.38 equations/s - 25437.24 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:32 - 0:02:53 -     680 -  509.33 equations/s - 21105.20 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:37 - 0:02:58 -     700 -  562.06 equations/s - 23527.67 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:42 - 0:03:02 -     720 -  561.89 equations/s - 23473.21 words/s - PRIM-BWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:46 - 0:03:06 -     740 -  634.55 equations/s - 26019.42 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:50 - 0:03:11 -     760 -  537.26 equations/s - 22242.30 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:55 - 0:03:16 -     780 -  535.62 equations/s - 22105.15 words/s - PRIM-BWD:  0.0442 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - ============ End of epoch 0 ============\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - 0/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - 128/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:16 - 256/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 384/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 512/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 640/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 768/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 896/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 1024/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 1152/2001\n",
            "INFO - 12/05/22 05:06:56 - 0:03:17 - 1280/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:17 - 1408/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:17 - 1536/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:17 - 1664/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 1792/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 1920/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 1324/2001 (66.16691654172914%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 0/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 128/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 256/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 384/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 512/2001\n",
            "INFO - 12/05/22 05:06:57 - 0:03:18 - 640/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:18 - 768/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:18 - 896/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:18 - 1024/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1152/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1280/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1408/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1536/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1664/2001\n",
            "INFO - 12/05/22 05:06:58 - 0:03:19 - 1792/2001\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - 1920/2001\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - 1312/2001 (65.5672163918041%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - epoch -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_xe_loss -> 1.693983\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc -> 66.166917\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_5 -> 70.422535\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_6 -> 73.484848\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_7 -> 69.536424\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_8 -> 68.240343\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_9 -> 70.588235\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_10 -> 59.006211\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_11 -> 70.992366\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_12 -> 56.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_13 -> 67.368421\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_14 -> 66.981132\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_15 -> 68.055556\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_16 -> 64.864865\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_17 -> 67.441860\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_18 -> 63.043478\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_19 -> 61.363636\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_20 -> 68.750000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_21 -> 78.260870\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_22 -> 71.428571\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_23 -> 77.272727\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_24 -> 45.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_25 -> 44.444444\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_29 -> 45.454545\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_44 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_45 -> 33.333333\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_62 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_68 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_101 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_xe_loss -> 1.837624\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc -> 65.567216\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_4 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_5 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_6 -> 74.814815\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_8 -> 71.296296\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_9 -> 67.701863\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_10 -> 72.560976\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_11 -> 53.956835\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_12 -> 59.854015\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_13 -> 69.230769\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_14 -> 61.445783\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_15 -> 67.187500\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_16 -> 55.714286\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_17 -> 61.363636\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_18 -> 64.285714\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_19 -> 63.414634\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_20 -> 70.212766\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_21 -> 48.148148\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_22 -> 72.413793\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_23 -> 75.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_24 -> 47.619048\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_25 -> 65.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_27 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_29 -> 58.823529\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_37 -> 60.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - __log__:{\"epoch\": 0, \"valid_prim_bwd_xe_loss\": 1.6939833153491615, \"valid_prim_bwd_acc\": 66.16691654172914, \"valid_prim_bwd_acc_2\": 50.0, \"valid_prim_bwd_acc_3\": 80.0, \"valid_prim_bwd_acc_4\": 74.4186046511628, \"valid_prim_bwd_acc_5\": 70.4225352112676, \"valid_prim_bwd_acc_6\": 73.48484848484848, \"valid_prim_bwd_acc_7\": 69.5364238410596, \"valid_prim_bwd_acc_8\": 68.24034334763948, \"valid_prim_bwd_acc_9\": 70.58823529411765, \"valid_prim_bwd_acc_10\": 59.006211180124225, \"valid_prim_bwd_acc_11\": 70.99236641221374, \"valid_prim_bwd_acc_12\": 56.0, \"valid_prim_bwd_acc_13\": 67.36842105263158, \"valid_prim_bwd_acc_14\": 66.98113207547169, \"valid_prim_bwd_acc_15\": 68.05555555555556, \"valid_prim_bwd_acc_16\": 64.86486486486487, \"valid_prim_bwd_acc_17\": 67.44186046511628, \"valid_prim_bwd_acc_18\": 63.04347826086956, \"valid_prim_bwd_acc_19\": 61.36363636363637, \"valid_prim_bwd_acc_20\": 68.75, \"valid_prim_bwd_acc_21\": 78.26086956521739, \"valid_prim_bwd_acc_22\": 71.42857142857143, \"valid_prim_bwd_acc_23\": 77.27272727272727, \"valid_prim_bwd_acc_24\": 45.0, \"valid_prim_bwd_acc_25\": 44.44444444444444, \"valid_prim_bwd_acc_26\": 60.0, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 53.84615384615385, \"valid_prim_bwd_acc_29\": 45.45454545454545, \"valid_prim_bwd_acc_30\": 66.66666666666667, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 50.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 100.0, \"valid_prim_bwd_acc_45\": 33.333333333333336, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 0.0, \"valid_prim_bwd_acc_64\": 0.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 100.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 0.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 100.0, \"valid_prim_bwd_acc_103\": 0.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 1.837623546625125, \"test_prim_bwd_acc\": 65.5672163918041, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 66.66666666666667, \"test_prim_bwd_acc_5\": 66.66666666666667, \"test_prim_bwd_acc_6\": 74.81481481481481, \"test_prim_bwd_acc_7\": 71.19565217391305, \"test_prim_bwd_acc_8\": 71.29629629629629, \"test_prim_bwd_acc_9\": 67.70186335403727, \"test_prim_bwd_acc_10\": 72.5609756097561, \"test_prim_bwd_acc_11\": 53.9568345323741, \"test_prim_bwd_acc_12\": 59.85401459854015, \"test_prim_bwd_acc_13\": 69.23076923076923, \"test_prim_bwd_acc_14\": 61.44578313253012, \"test_prim_bwd_acc_15\": 67.1875, \"test_prim_bwd_acc_16\": 55.714285714285715, \"test_prim_bwd_acc_17\": 61.36363636363637, \"test_prim_bwd_acc_18\": 64.28571428571429, \"test_prim_bwd_acc_19\": 63.41463414634146, \"test_prim_bwd_acc_20\": 70.2127659574468, \"test_prim_bwd_acc_21\": 48.148148148148145, \"test_prim_bwd_acc_22\": 72.41379310344827, \"test_prim_bwd_acc_23\": 75.0, \"test_prim_bwd_acc_24\": 47.61904761904762, \"test_prim_bwd_acc_25\": 65.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 50.0, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 58.8235294117647, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 60.0, \"test_prim_bwd_acc_38\": 20.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 0.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 50.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 100.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:06:59 - 0:03:19 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:06:59 - 0:03:19 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:06:59 - 0:03:19 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:06:59 - 0:03:19 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:06:59 - 0:03:19 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:07:03 - 0:03:24 - ============ Starting epoch 1 ... ============\n",
            "INFO - 12/05/22 05:07:08 - 0:03:28 -     800 -  206.22 equations/s -  8625.06 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:12 - 0:03:32 -     820 -  630.27 equations/s - 26319.98 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:16 - 0:03:37 -     840 -  555.09 equations/s - 22949.37 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:22 - 0:03:42 -     860 -  464.54 equations/s - 19420.62 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:27 - 0:03:47 -     880 -  521.19 equations/s - 21661.66 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:32 - 0:03:53 -     900 -  458.87 equations/s - 19419.45 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:37 - 0:03:57 -     920 -  568.31 equations/s - 23540.40 words/s - PRIM-BWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:42 - 0:04:03 -     940 -  462.71 equations/s - 19541.01 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:47 - 0:04:08 -     960 -  563.05 equations/s - 23612.75 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:51 - 0:04:12 -     980 -  631.22 equations/s - 26088.31 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:07:56 - 0:04:17 -    1000 -  481.31 equations/s - 19987.35 words/s - PRIM-BWD:  0.0379 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:01 - 0:04:21 -    1020 -  572.63 equations/s - 23789.38 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:06 - 0:04:27 -    1040 -  493.84 equations/s - 20932.24 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:11 - 0:04:31 -    1060 -  528.14 equations/s - 21917.46 words/s - PRIM-BWD:  0.0381 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:15 - 0:04:36 -    1080 -  539.72 equations/s - 22663.64 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:20 - 0:04:41 -    1100 -  586.98 equations/s - 24227.98 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:24 - 0:04:45 -    1120 -  590.98 equations/s - 24531.62 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:29 - 0:04:50 -    1140 -  530.45 equations/s - 22122.43 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:34 - 0:04:55 -    1160 -  507.98 equations/s - 21211.85 words/s - PRIM-BWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:39 - 0:04:59 -    1180 -  537.91 equations/s - 22257.83 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:43 - 0:05:04 -    1200 -  574.61 equations/s - 24048.43 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:48 - 0:05:09 -    1220 -  551.78 equations/s - 23052.62 words/s - PRIM-BWD:  0.0372 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:53 - 0:05:13 -    1240 -  532.96 equations/s - 22344.67 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:08:58 - 0:05:19 -    1260 -  483.33 equations/s - 20596.63 words/s - PRIM-BWD:  0.0375 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:03 - 0:05:23 -    1280 -  566.55 equations/s - 23471.24 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:07 - 0:05:28 -    1300 -  582.70 equations/s - 24410.40 words/s - PRIM-BWD:  0.0347 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:11 - 0:05:32 -    1320 -  635.65 equations/s - 26533.31 words/s - PRIM-BWD:  0.0371 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:15 - 0:05:36 -    1340 -  618.64 equations/s - 25664.12 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:20 - 0:05:40 -    1360 -  564.63 equations/s - 23698.14 words/s - PRIM-BWD:  0.0358 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:24 - 0:05:45 -    1380 -  601.80 equations/s - 24989.66 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:29 - 0:05:49 -    1400 -  522.68 equations/s - 21991.48 words/s - PRIM-BWD:  0.0360 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:34 - 0:05:54 -    1420 -  539.81 equations/s - 22944.56 words/s - PRIM-BWD:  0.0364 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:38 - 0:05:58 -    1440 -  603.78 equations/s - 24896.37 words/s - PRIM-BWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:42 - 0:06:03 -    1460 -  585.87 equations/s - 24345.53 words/s - PRIM-BWD:  0.0357 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:47 - 0:06:07 -    1480 -  581.57 equations/s - 24197.32 words/s - PRIM-BWD:  0.0370 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:51 - 0:06:12 -    1500 -  556.16 equations/s - 22944.09 words/s - PRIM-BWD:  0.0373 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:09:56 - 0:06:16 -    1520 -  564.75 equations/s - 23441.30 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:01 - 0:06:22 -    1540 -  484.79 equations/s - 20468.00 words/s - PRIM-BWD:  0.0371 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:05 - 0:06:26 -    1560 -  647.13 equations/s - 26812.49 words/s - PRIM-BWD:  0.0330 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - ============ End of epoch 1 ============\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - 0/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:26 - 128/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 256/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 384/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 512/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 640/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 768/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 896/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 1024/2001\n",
            "INFO - 12/05/22 05:10:06 - 0:06:27 - 1152/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:27 - 1280/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:27 - 1408/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:27 - 1536/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 1664/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 1792/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 1920/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 1360/2001 (67.96601699150425%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 0/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 128/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 256/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 384/2001\n",
            "INFO - 12/05/22 05:10:07 - 0:06:28 - 512/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:28 - 640/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:28 - 768/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:28 - 896/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1024/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1152/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1280/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1408/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1536/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1664/2001\n",
            "INFO - 12/05/22 05:10:08 - 0:06:29 - 1792/2001\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - 1920/2001\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - 1333/2001 (66.61669165417291%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - epoch -> 1.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_xe_loss -> 1.849754\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc -> 67.966017\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_6 -> 71.969697\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_7 -> 74.834437\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_8 -> 71.244635\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_9 -> 71.176471\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_10 -> 64.596273\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_11 -> 69.465649\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_12 -> 63.333333\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_13 -> 69.473684\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_14 -> 66.037736\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_15 -> 65.277778\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_16 -> 68.918919\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_17 -> 65.116279\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_18 -> 65.217391\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_19 -> 61.363636\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_20 -> 59.375000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_21 -> 60.869565\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_22 -> 61.904762\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_23 -> 81.818182\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_24 -> 45.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_25 -> 61.111111\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_26 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_27 -> 84.615385\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_28 -> 69.230769\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_29 -> 72.727273\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_32 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_44 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_56 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_101 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_xe_loss -> 1.999818\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc -> 66.616692\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_3 -> 77.777778\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_4 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_5 -> 64.912281\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_6 -> 76.296296\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_8 -> 71.296296\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_9 -> 67.080745\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_10 -> 73.170732\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_11 -> 56.115108\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_12 -> 56.934307\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_13 -> 72.115385\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_14 -> 62.650602\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_15 -> 65.625000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_16 -> 60.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_17 -> 63.636364\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_18 -> 61.904762\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_19 -> 73.170732\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_20 -> 74.468085\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_21 -> 62.962963\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_22 -> 68.965517\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_23 -> 75.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_24 -> 38.095238\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_25 -> 75.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_27 -> 71.428571\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_29 -> 58.823529\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - __log__:{\"epoch\": 1, \"valid_prim_bwd_xe_loss\": 1.849754444067029, \"valid_prim_bwd_acc\": 67.96601699150425, \"valid_prim_bwd_acc_2\": 50.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 74.4186046511628, \"valid_prim_bwd_acc_5\": 66.19718309859155, \"valid_prim_bwd_acc_6\": 71.96969696969697, \"valid_prim_bwd_acc_7\": 74.83443708609272, \"valid_prim_bwd_acc_8\": 71.24463519313305, \"valid_prim_bwd_acc_9\": 71.17647058823529, \"valid_prim_bwd_acc_10\": 64.59627329192547, \"valid_prim_bwd_acc_11\": 69.46564885496183, \"valid_prim_bwd_acc_12\": 63.333333333333336, \"valid_prim_bwd_acc_13\": 69.47368421052632, \"valid_prim_bwd_acc_14\": 66.0377358490566, \"valid_prim_bwd_acc_15\": 65.27777777777777, \"valid_prim_bwd_acc_16\": 68.91891891891892, \"valid_prim_bwd_acc_17\": 65.11627906976744, \"valid_prim_bwd_acc_18\": 65.21739130434783, \"valid_prim_bwd_acc_19\": 61.36363636363637, \"valid_prim_bwd_acc_20\": 59.375, \"valid_prim_bwd_acc_21\": 60.869565217391305, \"valid_prim_bwd_acc_22\": 61.904761904761905, \"valid_prim_bwd_acc_23\": 81.81818181818181, \"valid_prim_bwd_acc_24\": 45.0, \"valid_prim_bwd_acc_25\": 61.111111111111114, \"valid_prim_bwd_acc_26\": 66.66666666666667, \"valid_prim_bwd_acc_27\": 84.61538461538461, \"valid_prim_bwd_acc_28\": 69.23076923076923, \"valid_prim_bwd_acc_29\": 72.72727272727273, \"valid_prim_bwd_acc_30\": 55.55555555555556, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 66.66666666666667, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 100.0, \"valid_prim_bwd_acc_45\": 100.0, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 100.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 0.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 0.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 0.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 100.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 0.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 1.9998177333452414, \"test_prim_bwd_acc\": 66.61669165417291, \"test_prim_bwd_acc_3\": 77.77777777777777, \"test_prim_bwd_acc_4\": 66.66666666666667, \"test_prim_bwd_acc_5\": 64.91228070175438, \"test_prim_bwd_acc_6\": 76.29629629629629, \"test_prim_bwd_acc_7\": 71.19565217391305, \"test_prim_bwd_acc_8\": 71.29629629629629, \"test_prim_bwd_acc_9\": 67.0807453416149, \"test_prim_bwd_acc_10\": 73.17073170731707, \"test_prim_bwd_acc_11\": 56.115107913669064, \"test_prim_bwd_acc_12\": 56.934306569343065, \"test_prim_bwd_acc_13\": 72.11538461538461, \"test_prim_bwd_acc_14\": 62.65060240963855, \"test_prim_bwd_acc_15\": 65.625, \"test_prim_bwd_acc_16\": 60.0, \"test_prim_bwd_acc_17\": 63.63636363636363, \"test_prim_bwd_acc_18\": 61.904761904761905, \"test_prim_bwd_acc_19\": 73.17073170731707, \"test_prim_bwd_acc_20\": 74.46808510638297, \"test_prim_bwd_acc_21\": 62.96296296296296, \"test_prim_bwd_acc_22\": 68.96551724137932, \"test_prim_bwd_acc_23\": 75.0, \"test_prim_bwd_acc_24\": 38.095238095238095, \"test_prim_bwd_acc_25\": 75.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 71.42857142857143, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 58.8235294117647, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 66.66666666666667, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 100.0, \"test_prim_bwd_acc_37\": 80.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 0.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 50.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:10:09 - 0:06:29 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:10:09 - 0:06:29 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:10:09 - 0:06:29 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:10:09 - 0:06:29 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:10:09 - 0:06:29 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:10:14 - 0:06:35 - ============ Starting epoch 2 ... ============\n",
            "INFO - 12/05/22 05:10:18 - 0:06:38 -    1580 -  200.62 equations/s -  8332.25 words/s - PRIM-BWD:  0.0355 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:22 - 0:06:43 -    1600 -  565.76 equations/s - 23709.29 words/s - PRIM-BWD:  0.0356 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:27 - 0:06:47 -    1620 -  560.91 equations/s - 23464.68 words/s - PRIM-BWD:  0.0382 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:31 - 0:06:52 -    1640 -  558.32 equations/s - 23181.23 words/s - PRIM-BWD:  0.0367 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:36 - 0:06:57 -    1660 -  557.31 equations/s - 22992.90 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:40 - 0:07:00 -    1680 -  665.67 equations/s - 26989.59 words/s - PRIM-BWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:45 - 0:07:06 -    1700 -  479.74 equations/s - 20414.60 words/s - PRIM-BWD:  0.0364 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:50 - 0:07:11 -    1720 -  542.12 equations/s - 22653.17 words/s - PRIM-BWD:  0.0363 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:54 - 0:07:15 -    1740 -  575.81 equations/s - 24072.28 words/s - PRIM-BWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:10:59 - 0:07:19 -    1760 -  586.96 equations/s - 24146.17 words/s - PRIM-BWD:  0.0354 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:03 - 0:07:24 -    1780 -  537.29 equations/s - 22363.74 words/s - PRIM-BWD:  0.0360 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:08 - 0:07:28 -    1800 -  612.77 equations/s - 25573.37 words/s - PRIM-BWD:  0.0350 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:12 - 0:07:33 -    1820 -  542.03 equations/s - 22679.10 words/s - PRIM-BWD:  0.0343 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:17 - 0:07:38 -    1840 -  557.97 equations/s - 23248.80 words/s - PRIM-BWD:  0.0351 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:21 - 0:07:42 -    1860 -  624.67 equations/s - 25666.00 words/s - PRIM-BWD:  0.0362 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:25 - 0:07:46 -    1880 -  592.92 equations/s - 24869.35 words/s - PRIM-BWD:  0.0341 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:30 - 0:07:50 -    1900 -  587.68 equations/s - 24113.38 words/s - PRIM-BWD:  0.0367 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:35 - 0:07:56 -    1920 -  493.28 equations/s - 20721.27 words/s - PRIM-BWD:  0.0342 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:40 - 0:08:00 -    1940 -  544.66 equations/s - 22359.13 words/s - PRIM-BWD:  0.0349 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:44 - 0:08:05 -    1960 -  527.32 equations/s - 21972.45 words/s - PRIM-BWD:  0.0370 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:49 - 0:08:10 -    1980 -  513.45 equations/s - 21505.11 words/s - PRIM-BWD:  0.0350 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:54 - 0:08:14 -    2000 -  581.47 equations/s - 24389.09 words/s - PRIM-BWD:  0.0359 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:11:58 - 0:08:19 -    2020 -  574.50 equations/s - 23867.93 words/s - PRIM-BWD:  0.0361 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:03 - 0:08:24 -    2040 -  489.92 equations/s - 20541.63 words/s - PRIM-BWD:  0.0343 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:08 - 0:08:28 -    2060 -  601.16 equations/s - 25034.96 words/s - PRIM-BWD:  0.0368 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:12 - 0:08:33 -    2080 -  577.58 equations/s - 23569.19 words/s - PRIM-BWD:  0.0340 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:17 - 0:08:38 -    2100 -  500.79 equations/s - 20895.66 words/s - PRIM-BWD:  0.0334 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:22 - 0:08:43 -    2120 -  558.39 equations/s - 23635.30 words/s - PRIM-BWD:  0.0353 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:27 - 0:08:48 -    2140 -  510.78 equations/s - 21365.61 words/s - PRIM-BWD:  0.0336 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:32 - 0:08:52 -    2160 -  546.35 equations/s - 22640.19 words/s - PRIM-BWD:  0.0344 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:36 - 0:08:57 -    2180 -  556.67 equations/s - 23218.07 words/s - PRIM-BWD:  0.0335 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:41 - 0:09:02 -    2200 -  537.21 equations/s - 22361.84 words/s - PRIM-BWD:  0.0345 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:45 - 0:09:06 -    2220 -  606.46 equations/s - 25176.05 words/s - PRIM-BWD:  0.0327 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:50 - 0:09:11 -    2240 -  545.72 equations/s - 22983.54 words/s - PRIM-BWD:  0.0372 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:54 - 0:09:15 -    2260 -  573.06 equations/s - 23878.59 words/s - PRIM-BWD:  0.0379 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:12:59 - 0:09:19 -    2280 -  602.60 equations/s - 24753.82 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:03 - 0:09:23 -    2300 -  626.40 equations/s - 25876.59 words/s - PRIM-BWD:  0.0349 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:08 - 0:09:28 -    2320 -  521.51 equations/s - 21795.24 words/s - PRIM-BWD:  0.0356 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:11 - 0:09:32 -    2340 -  698.12 equations/s - 28356.48 words/s - PRIM-BWD:  0.0306 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - ============ End of epoch 2 ============\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 0/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 128/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 256/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 384/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 512/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 640/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 768/2001\n",
            "INFO - 12/05/22 05:13:13 - 0:09:34 - 896/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:34 - 1024/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:34 - 1152/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:34 - 1280/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1408/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1536/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1664/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1792/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1920/2001\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 1342/2001 (67.06646676661668%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:13:14 - 0:09:35 - 0/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:35 - 128/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:35 - 256/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:35 - 384/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:35 - 512/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:35 - 640/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 768/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 896/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 1024/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 1152/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 1280/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 1408/2001\n",
            "INFO - 12/05/22 05:13:15 - 0:09:36 - 1536/2001\n",
            "INFO - 12/05/22 05:13:16 - 0:09:36 - 1664/2001\n",
            "INFO - 12/05/22 05:13:16 - 0:09:36 - 1792/2001\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - 1920/2001\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - 1316/2001 (65.76711644177911%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - epoch -> 2.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_xe_loss -> 1.883830\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc -> 67.066467\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_4 -> 76.744186\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_5 -> 69.014085\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_6 -> 70.454545\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_7 -> 73.509934\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_8 -> 70.386266\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_9 -> 67.058824\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_10 -> 64.596273\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_11 -> 67.938931\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_12 -> 61.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_13 -> 69.473684\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_14 -> 66.981132\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_15 -> 70.833333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_16 -> 64.864865\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_17 -> 62.790698\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_18 -> 69.565217\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_19 -> 56.818182\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_20 -> 62.500000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_21 -> 73.913043\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_22 -> 71.428571\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_23 -> 77.272727\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_24 -> 35.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_25 -> 44.444444\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_26 -> 80.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_28 -> 61.538462\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_29 -> 54.545455\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_30 -> 44.444444\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_39 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_101 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_xe_loss -> 2.047909\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc -> 65.767116\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_4 -> 64.444444\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_5 -> 71.929825\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_6 -> 75.555556\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_8 -> 72.685185\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_9 -> 65.838509\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_10 -> 66.463415\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_11 -> 55.395683\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_12 -> 56.934307\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_13 -> 65.384615\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_14 -> 63.855422\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_15 -> 64.062500\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_16 -> 60.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_17 -> 72.727273\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_18 -> 69.047619\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_19 -> 73.170732\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_20 -> 70.212766\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_21 -> 48.148148\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_22 -> 68.965517\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_23 -> 80.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_24 -> 33.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_25 -> 70.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_26 -> 85.714286\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_27 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_29 -> 64.705882\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_32 -> 83.333333\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_35 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - __log__:{\"epoch\": 2, \"valid_prim_bwd_xe_loss\": 1.8838299661204554, \"valid_prim_bwd_acc\": 67.06646676661668, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 76.74418604651163, \"valid_prim_bwd_acc_5\": 69.01408450704226, \"valid_prim_bwd_acc_6\": 70.45454545454545, \"valid_prim_bwd_acc_7\": 73.50993377483444, \"valid_prim_bwd_acc_8\": 70.3862660944206, \"valid_prim_bwd_acc_9\": 67.05882352941177, \"valid_prim_bwd_acc_10\": 64.59627329192547, \"valid_prim_bwd_acc_11\": 67.93893129770993, \"valid_prim_bwd_acc_12\": 61.333333333333336, \"valid_prim_bwd_acc_13\": 69.47368421052632, \"valid_prim_bwd_acc_14\": 66.98113207547169, \"valid_prim_bwd_acc_15\": 70.83333333333333, \"valid_prim_bwd_acc_16\": 64.86486486486487, \"valid_prim_bwd_acc_17\": 62.7906976744186, \"valid_prim_bwd_acc_18\": 69.56521739130434, \"valid_prim_bwd_acc_19\": 56.81818181818182, \"valid_prim_bwd_acc_20\": 62.5, \"valid_prim_bwd_acc_21\": 73.91304347826087, \"valid_prim_bwd_acc_22\": 71.42857142857143, \"valid_prim_bwd_acc_23\": 77.27272727272727, \"valid_prim_bwd_acc_24\": 35.0, \"valid_prim_bwd_acc_25\": 44.44444444444444, \"valid_prim_bwd_acc_26\": 80.0, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 61.53846153846154, \"valid_prim_bwd_acc_29\": 54.54545454545455, \"valid_prim_bwd_acc_30\": 44.44444444444444, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 100.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 100.0, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 0.0, \"valid_prim_bwd_acc_49\": 0.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 50.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 0.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 100.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.0479091203909765, \"test_prim_bwd_acc\": 65.76711644177911, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 64.44444444444444, \"test_prim_bwd_acc_5\": 71.9298245614035, \"test_prim_bwd_acc_6\": 75.55555555555556, \"test_prim_bwd_acc_7\": 71.19565217391305, \"test_prim_bwd_acc_8\": 72.68518518518519, \"test_prim_bwd_acc_9\": 65.83850931677019, \"test_prim_bwd_acc_10\": 66.46341463414635, \"test_prim_bwd_acc_11\": 55.39568345323741, \"test_prim_bwd_acc_12\": 56.934306569343065, \"test_prim_bwd_acc_13\": 65.38461538461539, \"test_prim_bwd_acc_14\": 63.855421686746986, \"test_prim_bwd_acc_15\": 64.0625, \"test_prim_bwd_acc_16\": 60.0, \"test_prim_bwd_acc_17\": 72.72727272727273, \"test_prim_bwd_acc_18\": 69.04761904761905, \"test_prim_bwd_acc_19\": 73.17073170731707, \"test_prim_bwd_acc_20\": 70.2127659574468, \"test_prim_bwd_acc_21\": 48.148148148148145, \"test_prim_bwd_acc_22\": 68.96551724137932, \"test_prim_bwd_acc_23\": 80.0, \"test_prim_bwd_acc_24\": 33.333333333333336, \"test_prim_bwd_acc_25\": 70.0, \"test_prim_bwd_acc_26\": 85.71428571428571, \"test_prim_bwd_acc_27\": 50.0, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 64.70588235294117, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 83.33333333333333, \"test_prim_bwd_acc_33\": 66.66666666666667, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 100.0, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 80.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 100.0, \"test_prim_bwd_acc_46\": 0.0, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:13:16 - 0:09:37 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:13:16 - 0:09:37 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:13:16 - 0:09:37 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:13:16 - 0:09:37 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:13:16 - 0:09:37 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:13:23 - 0:09:44 - ============ Starting epoch 3 ... ============\n",
            "INFO - 12/05/22 05:13:27 - 0:09:48 -    2360 -  162.17 equations/s -  6774.87 words/s - PRIM-BWD:  0.0332 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:31 - 0:09:52 -    2380 -  572.40 equations/s - 24171.09 words/s - PRIM-BWD:  0.0350 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:36 - 0:09:56 -    2400 -  588.86 equations/s - 24411.85 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:41 - 0:10:02 -    2420 -  474.45 equations/s - 19915.57 words/s - PRIM-BWD:  0.0341 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:45 - 0:10:06 -    2440 -  653.75 equations/s - 27006.61 words/s - PRIM-BWD:  0.0349 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:49 - 0:10:10 -    2460 -  609.59 equations/s - 25334.49 words/s - PRIM-BWD:  0.0339 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:54 - 0:10:15 -    2480 -  501.02 equations/s - 21226.23 words/s - PRIM-BWD:  0.0346 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:13:59 - 0:10:20 -    2500 -  557.44 equations/s - 22976.71 words/s - PRIM-BWD:  0.0345 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:04 - 0:10:25 -    2520 -  498.72 equations/s - 20947.29 words/s - PRIM-BWD:  0.0339 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:09 - 0:10:30 -    2540 -  509.47 equations/s - 21583.41 words/s - PRIM-BWD:  0.0331 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:13 - 0:10:34 -    2560 -  616.06 equations/s - 25320.43 words/s - PRIM-BWD:  0.0354 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:18 - 0:10:39 -    2580 -  552.14 equations/s - 23021.68 words/s - PRIM-BWD:  0.0334 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:23 - 0:10:44 -    2600 -  500.09 equations/s - 20798.04 words/s - PRIM-BWD:  0.0360 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:28 - 0:10:48 -    2620 -  548.51 equations/s - 22823.41 words/s - PRIM-BWD:  0.0348 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:32 - 0:10:53 -    2640 -  589.65 equations/s - 24689.54 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:37 - 0:10:57 -    2660 -  577.25 equations/s - 23772.57 words/s - PRIM-BWD:  0.0327 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:42 - 0:11:03 -    2680 -  467.20 equations/s - 19413.68 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:47 - 0:11:08 -    2700 -  508.62 equations/s - 21476.44 words/s - PRIM-BWD:  0.0321 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:51 - 0:11:12 -    2720 -  583.02 equations/s - 24037.67 words/s - PRIM-BWD:  0.0344 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:14:57 - 0:11:18 -    2740 -  466.71 equations/s - 19492.62 words/s - PRIM-BWD:  0.0327 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:02 - 0:11:23 -    2760 -  514.38 equations/s - 21470.24 words/s - PRIM-BWD:  0.0349 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:07 - 0:11:28 -    2780 -  491.57 equations/s - 20498.14 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:11 - 0:11:32 -    2800 -  618.83 equations/s - 25786.02 words/s - PRIM-BWD:  0.0358 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:16 - 0:11:37 -    2820 -  499.18 equations/s - 21077.52 words/s - PRIM-BWD:  0.0348 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:21 - 0:11:42 -    2840 -  534.59 equations/s - 22406.88 words/s - PRIM-BWD:  0.0340 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:25 - 0:11:46 -    2860 -  602.63 equations/s - 24845.73 words/s - PRIM-BWD:  0.0353 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:30 - 0:11:51 -    2880 -  551.34 equations/s - 22841.84 words/s - PRIM-BWD:  0.0337 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:34 - 0:11:55 -    2900 -  630.51 equations/s - 26184.96 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:39 - 0:12:00 -    2920 -  517.02 equations/s - 21675.70 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:44 - 0:12:05 -    2940 -  529.99 equations/s - 22113.75 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:48 - 0:12:09 -    2960 -  610.29 equations/s - 25292.61 words/s - PRIM-BWD:  0.0335 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:53 - 0:12:14 -    2980 -  503.25 equations/s - 21287.25 words/s - PRIM-BWD:  0.0350 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:15:59 - 0:12:19 -    3000 -  471.85 equations/s - 19881.30 words/s - PRIM-BWD:  0.0335 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:04 - 0:12:24 -    3020 -  512.12 equations/s - 21688.08 words/s - PRIM-BWD:  0.0337 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:09 - 0:12:29 -    3040 -  509.93 equations/s - 21577.25 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:14 - 0:12:35 -    3060 -  474.99 equations/s - 20173.20 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:18 - 0:12:39 -    3080 -  597.05 equations/s - 24518.62 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:23 - 0:12:44 -    3100 -  543.27 equations/s - 22595.61 words/s - PRIM-BWD:  0.0318 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:27 - 0:12:48 -    3120 -  588.31 equations/s - 24366.58 words/s - PRIM-BWD:  0.0320 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - ============ End of epoch 3 ============\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 0/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 128/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 256/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 384/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 512/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 640/2001\n",
            "INFO - 12/05/22 05:16:29 - 0:12:50 - 768/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:50 - 896/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:50 - 1024/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:50 - 1152/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:50 - 1280/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1408/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1536/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1664/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1792/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1920/2001\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 1350/2001 (67.46626686656671%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:16:30 - 0:12:51 - 0/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:51 - 128/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:51 - 256/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:51 - 384/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:51 - 512/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 640/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 768/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 896/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 1024/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 1152/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 1280/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 1408/2001\n",
            "INFO - 12/05/22 05:16:31 - 0:12:52 - 1536/2001\n",
            "INFO - 12/05/22 05:16:32 - 0:12:52 - 1664/2001\n",
            "INFO - 12/05/22 05:16:32 - 0:12:52 - 1792/2001\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - 1920/2001\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - 1335/2001 (66.71664167916042%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - epoch -> 3.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_xe_loss -> 2.081458\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc -> 67.466267\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_4 -> 76.744186\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_5 -> 67.605634\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_6 -> 72.727273\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_7 -> 69.536424\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_8 -> 70.815451\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_9 -> 70.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_10 -> 63.975155\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_11 -> 67.938931\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_12 -> 62.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_13 -> 68.421053\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_14 -> 65.094340\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_15 -> 69.444444\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_16 -> 66.216216\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_17 -> 65.116279\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_18 -> 71.739130\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_19 -> 65.909091\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_20 -> 65.625000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_21 -> 78.260870\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_22 -> 61.904762\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_23 -> 72.727273\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_24 -> 55.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_25 -> 61.111111\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_26 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_27 -> 84.615385\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_29 -> 63.636364\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_39 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_62 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_xe_loss -> 2.278560\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc -> 66.716642\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_3 -> 55.555556\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_4 -> 60.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_5 -> 71.929825\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_6 -> 74.814815\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_8 -> 70.370370\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_9 -> 68.322981\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_10 -> 74.390244\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_11 -> 57.553957\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_12 -> 58.394161\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_13 -> 67.307692\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_14 -> 61.445783\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_15 -> 65.625000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_16 -> 58.571429\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_17 -> 65.909091\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_18 -> 64.285714\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_19 -> 73.170732\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_20 -> 76.595745\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_21 -> 59.259259\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_22 -> 62.068966\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_23 -> 80.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_24 -> 52.380952\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_25 -> 65.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_27 -> 71.428571\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_29 -> 58.823529\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - __log__:{\"epoch\": 3, \"valid_prim_bwd_xe_loss\": 2.0814580960252296, \"valid_prim_bwd_acc\": 67.46626686656671, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 80.0, \"valid_prim_bwd_acc_4\": 76.74418604651163, \"valid_prim_bwd_acc_5\": 67.6056338028169, \"valid_prim_bwd_acc_6\": 72.72727272727273, \"valid_prim_bwd_acc_7\": 69.5364238410596, \"valid_prim_bwd_acc_8\": 70.81545064377683, \"valid_prim_bwd_acc_9\": 70.0, \"valid_prim_bwd_acc_10\": 63.975155279503106, \"valid_prim_bwd_acc_11\": 67.93893129770993, \"valid_prim_bwd_acc_12\": 62.666666666666664, \"valid_prim_bwd_acc_13\": 68.42105263157895, \"valid_prim_bwd_acc_14\": 65.09433962264151, \"valid_prim_bwd_acc_15\": 69.44444444444444, \"valid_prim_bwd_acc_16\": 66.21621621621621, \"valid_prim_bwd_acc_17\": 65.11627906976744, \"valid_prim_bwd_acc_18\": 71.73913043478261, \"valid_prim_bwd_acc_19\": 65.9090909090909, \"valid_prim_bwd_acc_20\": 65.625, \"valid_prim_bwd_acc_21\": 78.26086956521739, \"valid_prim_bwd_acc_22\": 61.904761904761905, \"valid_prim_bwd_acc_23\": 72.72727272727273, \"valid_prim_bwd_acc_24\": 55.0, \"valid_prim_bwd_acc_25\": 61.111111111111114, \"valid_prim_bwd_acc_26\": 66.66666666666667, \"valid_prim_bwd_acc_27\": 84.61538461538461, \"valid_prim_bwd_acc_28\": 53.84615384615385, \"valid_prim_bwd_acc_29\": 63.63636363636363, \"valid_prim_bwd_acc_30\": 66.66666666666667, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 100.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 50.0, \"valid_prim_bwd_acc_44\": 0.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 0.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 0.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 0.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 0.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.278559908278283, \"test_prim_bwd_acc\": 66.71664167916042, \"test_prim_bwd_acc_3\": 55.55555555555556, \"test_prim_bwd_acc_4\": 60.0, \"test_prim_bwd_acc_5\": 71.9298245614035, \"test_prim_bwd_acc_6\": 74.81481481481481, \"test_prim_bwd_acc_7\": 71.19565217391305, \"test_prim_bwd_acc_8\": 70.37037037037037, \"test_prim_bwd_acc_9\": 68.32298136645963, \"test_prim_bwd_acc_10\": 74.39024390243902, \"test_prim_bwd_acc_11\": 57.55395683453237, \"test_prim_bwd_acc_12\": 58.394160583941606, \"test_prim_bwd_acc_13\": 67.3076923076923, \"test_prim_bwd_acc_14\": 61.44578313253012, \"test_prim_bwd_acc_15\": 65.625, \"test_prim_bwd_acc_16\": 58.57142857142857, \"test_prim_bwd_acc_17\": 65.9090909090909, \"test_prim_bwd_acc_18\": 64.28571428571429, \"test_prim_bwd_acc_19\": 73.17073170731707, \"test_prim_bwd_acc_20\": 76.59574468085107, \"test_prim_bwd_acc_21\": 59.25925925925926, \"test_prim_bwd_acc_22\": 62.06896551724138, \"test_prim_bwd_acc_23\": 80.0, \"test_prim_bwd_acc_24\": 52.38095238095238, \"test_prim_bwd_acc_25\": 65.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 71.42857142857143, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 58.8235294117647, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 100.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 100.0, \"test_prim_bwd_acc_37\": 80.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 0.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 66.66666666666667, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 100.0, \"test_prim_bwd_acc_65\": 100.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:16:32 - 0:12:53 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:16:32 - 0:12:53 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:16:32 - 0:12:53 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:16:32 - 0:12:53 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:16:32 - 0:12:53 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:16:37 - 0:12:58 - ============ Starting epoch 4 ... ============\n",
            "INFO - 12/05/22 05:16:39 - 0:13:00 -    3140 -  211.79 equations/s -  8641.10 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:44 - 0:13:05 -    3160 -  582.48 equations/s - 23963.33 words/s - PRIM-BWD:  0.0361 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:48 - 0:13:09 -    3180 -  613.10 equations/s - 25546.63 words/s - PRIM-BWD:  0.0323 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:52 - 0:13:13 -    3200 -  582.39 equations/s - 24182.84 words/s - PRIM-BWD:  0.0333 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:16:57 - 0:13:18 -    3220 -  578.09 equations/s - 23972.52 words/s - PRIM-BWD:  0.0348 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:01 - 0:13:22 -    3240 -  574.36 equations/s - 23719.28 words/s - PRIM-BWD:  0.0357 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:07 - 0:13:27 -    3260 -  469.43 equations/s - 20000.03 words/s - PRIM-BWD:  0.0352 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:11 - 0:13:32 -    3280 -  554.07 equations/s - 23101.01 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:17 - 0:13:37 -    3300 -  488.85 equations/s - 20325.01 words/s - PRIM-BWD:  0.0310 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:21 - 0:13:42 -    3320 -  592.00 equations/s - 24605.07 words/s - PRIM-BWD:  0.0335 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:26 - 0:13:47 -    3340 -  485.12 equations/s - 20124.45 words/s - PRIM-BWD:  0.0316 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:31 - 0:13:52 -    3360 -  540.68 equations/s - 22483.92 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:36 - 0:13:56 -    3380 -  540.03 equations/s - 22237.32 words/s - PRIM-BWD:  0.0338 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:40 - 0:14:01 -    3400 -  607.97 equations/s - 24961.09 words/s - PRIM-BWD:  0.0345 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:44 - 0:14:05 -    3420 -  603.01 equations/s - 24731.56 words/s - PRIM-BWD:  0.0340 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:49 - 0:14:09 -    3440 -  564.15 equations/s - 23496.35 words/s - PRIM-BWD:  0.0333 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:53 - 0:14:14 -    3460 -  614.74 equations/s - 25506.88 words/s - PRIM-BWD:  0.0344 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:17:57 - 0:14:18 -    3480 -  604.83 equations/s - 24728.44 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:02 - 0:14:22 -    3500 -  559.24 equations/s - 23406.60 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:06 - 0:14:27 -    3520 -  572.95 equations/s - 23893.16 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:10 - 0:14:31 -    3540 -  614.74 equations/s - 25439.19 words/s - PRIM-BWD:  0.0336 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:15 - 0:14:36 -    3560 -  562.53 equations/s - 23667.07 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:20 - 0:14:41 -    3580 -  481.88 equations/s - 20256.84 words/s - PRIM-BWD:  0.0341 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:25 - 0:14:46 -    3600 -  486.81 equations/s - 20397.69 words/s - PRIM-BWD:  0.0306 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:30 - 0:14:50 -    3620 -  598.04 equations/s - 25248.33 words/s - PRIM-BWD:  0.0335 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:35 - 0:14:55 -    3640 -  532.39 equations/s - 22440.39 words/s - PRIM-BWD:  0.0310 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:40 - 0:15:01 -    3660 -  457.53 equations/s - 19367.67 words/s - PRIM-BWD:  0.0329 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:45 - 0:15:06 -    3680 -  492.06 equations/s - 20518.39 words/s - PRIM-BWD:  0.0354 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:49 - 0:15:10 -    3700 -  630.25 equations/s - 25937.76 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:54 - 0:15:15 -    3720 -  557.59 equations/s - 22758.33 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:18:58 - 0:15:19 -    3740 -  579.99 equations/s - 23986.61 words/s - PRIM-BWD:  0.0308 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:03 - 0:15:24 -    3760 -  566.36 equations/s - 23611.69 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:07 - 0:15:28 -    3780 -  595.04 equations/s - 24607.88 words/s - PRIM-BWD:  0.0327 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:12 - 0:15:32 -    3800 -  592.22 equations/s - 24408.53 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:16 - 0:15:37 -    3820 -  558.75 equations/s - 23166.02 words/s - PRIM-BWD:  0.0316 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:21 - 0:15:41 -    3840 -  567.72 equations/s - 23514.10 words/s - PRIM-BWD:  0.0295 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:26 - 0:15:46 -    3860 -  495.36 equations/s - 20719.78 words/s - PRIM-BWD:  0.0339 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:31 - 0:15:52 -    3880 -  502.64 equations/s - 20883.59 words/s - PRIM-BWD:  0.0323 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:35 - 0:15:56 -    3900 -  603.75 equations/s - 24793.83 words/s - PRIM-BWD:  0.0302 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:37 - 0:15:58 - ============ End of epoch 4 ============\n",
            "INFO - 12/05/22 05:19:37 - 0:15:58 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:19:37 - 0:15:58 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:19:37 - 0:15:58 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:19:37 - 0:15:58 - 0/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:58 - 128/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:58 - 256/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:58 - 384/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:58 - 512/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 640/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 768/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 896/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 1024/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 1152/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 1280/2001\n",
            "INFO - 12/05/22 05:19:38 - 0:15:59 - 1408/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:15:59 - 1536/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:15:59 - 1664/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:15:59 - 1792/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 1920/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 1338/2001 (66.86656671664169%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 0/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 128/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 256/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 384/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 512/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 640/2001\n",
            "INFO - 12/05/22 05:19:39 - 0:16:00 - 768/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:00 - 896/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:00 - 1024/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:00 - 1152/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:00 - 1280/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:01 - 1408/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:01 - 1536/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:01 - 1664/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:01 - 1792/2001\n",
            "INFO - 12/05/22 05:19:40 - 0:16:01 - 1920/2001\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - 1326/2001 (66.26686656671664%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - epoch -> 4.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_xe_loss -> 2.093049\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc -> 66.866567\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_5 -> 60.563380\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_6 -> 69.696970\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_7 -> 73.509934\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_8 -> 69.098712\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_9 -> 69.411765\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_10 -> 62.732919\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_11 -> 68.702290\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_12 -> 64.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_13 -> 67.368421\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_14 -> 69.811321\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_15 -> 70.833333\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_16 -> 64.864865\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_17 -> 72.093023\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_18 -> 63.043478\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_19 -> 59.090909\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_20 -> 62.500000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_21 -> 82.608696\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_22 -> 71.428571\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_23 -> 63.636364\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_24 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_25 -> 44.444444\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_26 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_29 -> 63.636364\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_32 -> 16.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_xe_loss -> 2.184875\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc -> 66.266867\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_4 -> 62.222222\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_5 -> 70.175439\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_6 -> 72.592593\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_7 -> 72.282609\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_8 -> 68.981481\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_9 -> 68.944099\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_10 -> 70.121951\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_11 -> 59.712230\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_12 -> 58.394161\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_13 -> 69.230769\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_14 -> 62.650602\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_15 -> 65.625000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_16 -> 54.285714\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_17 -> 61.363636\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_18 -> 61.904762\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_19 -> 78.048780\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_20 -> 74.468085\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_21 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_22 -> 68.965517\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_23 -> 85.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_24 -> 47.619048\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_25 -> 85.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_26 -> 71.428571\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_27 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_28 -> 44.444444\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_29 -> 70.588235\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - __log__:{\"epoch\": 4, \"valid_prim_bwd_xe_loss\": 2.0930488430101355, \"valid_prim_bwd_acc\": 66.86656671664169, \"valid_prim_bwd_acc_2\": 50.0, \"valid_prim_bwd_acc_3\": 80.0, \"valid_prim_bwd_acc_4\": 74.4186046511628, \"valid_prim_bwd_acc_5\": 60.563380281690144, \"valid_prim_bwd_acc_6\": 69.6969696969697, \"valid_prim_bwd_acc_7\": 73.50993377483444, \"valid_prim_bwd_acc_8\": 69.09871244635193, \"valid_prim_bwd_acc_9\": 69.41176470588235, \"valid_prim_bwd_acc_10\": 62.732919254658384, \"valid_prim_bwd_acc_11\": 68.70229007633588, \"valid_prim_bwd_acc_12\": 64.0, \"valid_prim_bwd_acc_13\": 67.36842105263158, \"valid_prim_bwd_acc_14\": 69.81132075471699, \"valid_prim_bwd_acc_15\": 70.83333333333333, \"valid_prim_bwd_acc_16\": 64.86486486486487, \"valid_prim_bwd_acc_17\": 72.09302325581395, \"valid_prim_bwd_acc_18\": 63.04347826086956, \"valid_prim_bwd_acc_19\": 59.09090909090909, \"valid_prim_bwd_acc_20\": 62.5, \"valid_prim_bwd_acc_21\": 82.6086956521739, \"valid_prim_bwd_acc_22\": 71.42857142857143, \"valid_prim_bwd_acc_23\": 63.63636363636363, \"valid_prim_bwd_acc_24\": 50.0, \"valid_prim_bwd_acc_25\": 44.44444444444444, \"valid_prim_bwd_acc_26\": 66.66666666666667, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 53.84615384615385, \"valid_prim_bwd_acc_29\": 63.63636363636363, \"valid_prim_bwd_acc_30\": 55.55555555555556, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 16.666666666666668, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 0.0, \"valid_prim_bwd_acc_104\": 0.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.1848751985091437, \"test_prim_bwd_acc\": 66.26686656671664, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 62.22222222222222, \"test_prim_bwd_acc_5\": 70.17543859649123, \"test_prim_bwd_acc_6\": 72.5925925925926, \"test_prim_bwd_acc_7\": 72.28260869565217, \"test_prim_bwd_acc_8\": 68.98148148148148, \"test_prim_bwd_acc_9\": 68.94409937888199, \"test_prim_bwd_acc_10\": 70.1219512195122, \"test_prim_bwd_acc_11\": 59.71223021582734, \"test_prim_bwd_acc_12\": 58.394160583941606, \"test_prim_bwd_acc_13\": 69.23076923076923, \"test_prim_bwd_acc_14\": 62.65060240963855, \"test_prim_bwd_acc_15\": 65.625, \"test_prim_bwd_acc_16\": 54.285714285714285, \"test_prim_bwd_acc_17\": 61.36363636363637, \"test_prim_bwd_acc_18\": 61.904761904761905, \"test_prim_bwd_acc_19\": 78.04878048780488, \"test_prim_bwd_acc_20\": 74.46808510638297, \"test_prim_bwd_acc_21\": 66.66666666666667, \"test_prim_bwd_acc_22\": 68.96551724137932, \"test_prim_bwd_acc_23\": 85.0, \"test_prim_bwd_acc_24\": 47.61904761904762, \"test_prim_bwd_acc_25\": 85.0, \"test_prim_bwd_acc_26\": 71.42857142857143, \"test_prim_bwd_acc_27\": 50.0, \"test_prim_bwd_acc_28\": 44.44444444444444, \"test_prim_bwd_acc_29\": 70.58823529411765, \"test_prim_bwd_acc_30\": 66.66666666666667, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 66.66666666666667, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 100.0, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 0.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:19:41 - 0:16:01 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:19:41 - 0:16:01 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:19:41 - 0:16:01 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:19:41 - 0:16:01 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:19:41 - 0:16:01 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:19:46 - 0:16:06 - ============ Starting epoch 5 ... ============\n",
            "INFO - 12/05/22 05:19:48 - 0:16:09 -    3920 -  195.05 equations/s -  8247.36 words/s - PRIM-BWD:  0.0320 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:53 - 0:16:14 -    3940 -  547.28 equations/s - 22948.32 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:19:58 - 0:16:18 -    3960 -  539.10 equations/s - 22559.94 words/s - PRIM-BWD:  0.0310 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:02 - 0:16:23 -    3980 -  573.35 equations/s - 24009.83 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:07 - 0:16:27 -    4000 -  552.95 equations/s - 23025.49 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 05:20:11 - 0:16:32 -    4020 -  566.63 equations/s - 23425.02 words/s - PRIM-BWD:  0.0309 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:16 - 0:16:37 -    4040 -  517.82 equations/s - 21557.13 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:20 - 0:16:41 -    4060 -  698.25 equations/s - 28695.01 words/s - PRIM-BWD:  0.0333 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:25 - 0:16:45 -    4080 -  525.77 equations/s - 21937.04 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:30 - 0:16:50 -    4100 -  512.69 equations/s - 21588.78 words/s - PRIM-BWD:  0.0301 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:34 - 0:16:55 -    4120 -  550.37 equations/s - 22901.89 words/s - PRIM-BWD:  0.0352 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:39 - 0:17:00 -    4140 -  515.24 equations/s - 21741.10 words/s - PRIM-BWD:  0.0323 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:44 - 0:17:04 -    4160 -  596.25 equations/s - 24735.21 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:48 - 0:17:09 -    4180 -  548.53 equations/s - 22822.24 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:53 - 0:17:13 -    4200 -  575.81 equations/s - 24141.40 words/s - PRIM-BWD:  0.0306 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:20:58 - 0:17:18 -    4220 -  535.53 equations/s - 22743.09 words/s - PRIM-BWD:  0.0313 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:02 - 0:17:23 -    4240 -  559.09 equations/s - 23634.40 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:07 - 0:17:28 -    4260 -  542.16 equations/s - 22672.40 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:13 - 0:17:33 -    4280 -  437.96 equations/s - 18778.30 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:18 - 0:17:38 -    4300 -  507.71 equations/s - 21157.23 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:22 - 0:17:43 -    4320 -  585.45 equations/s - 24490.70 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:26 - 0:17:47 -    4340 -  616.91 equations/s - 25785.46 words/s - PRIM-BWD:  0.0309 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:31 - 0:17:51 -    4360 -  585.64 equations/s - 24587.71 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:36 - 0:17:56 -    4380 -  519.58 equations/s - 21915.25 words/s - PRIM-BWD:  0.0318 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:40 - 0:18:01 -    4400 -  531.40 equations/s - 22081.21 words/s - PRIM-BWD:  0.0291 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:45 - 0:18:05 -    4420 -  591.36 equations/s - 24634.80 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:49 - 0:18:10 -    4440 -  599.64 equations/s - 24918.09 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:54 - 0:18:15 -    4460 -  485.38 equations/s - 20358.51 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:21:59 - 0:18:20 -    4480 -  542.23 equations/s - 22688.15 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:04 - 0:18:24 -    4500 -  542.79 equations/s - 22633.48 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:08 - 0:18:29 -    4520 -  569.10 equations/s - 23531.24 words/s - PRIM-BWD:  0.0308 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:13 - 0:18:33 -    4540 -  590.74 equations/s - 24388.31 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:17 - 0:18:37 -    4560 -  617.41 equations/s - 25604.13 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:21 - 0:18:42 -    4580 -  543.89 equations/s - 22356.88 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:26 - 0:18:47 -    4600 -  516.99 equations/s - 21440.91 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:30 - 0:18:51 -    4620 -  612.69 equations/s - 25402.34 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:35 - 0:18:56 -    4640 -  586.55 equations/s - 24511.89 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:40 - 0:19:00 -    4660 -  545.62 equations/s - 22687.23 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:45 - 0:19:06 -    4680 -  458.45 equations/s - 19361.64 words/s - PRIM-BWD:  0.0332 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - ============ End of epoch 5 ============\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 0/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 128/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 256/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 384/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 512/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 640/2001\n",
            "INFO - 12/05/22 05:22:48 - 0:19:09 - 768/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:09 - 896/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:09 - 1024/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:09 - 1152/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1280/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1408/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1536/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1664/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1792/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1920/2001\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - 1315/2001 (65.71714142928536%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:22:49 - 0:19:10 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:22:50 - 0:19:10 - 0/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:10 - 128/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:10 - 256/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:10 - 384/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:10 - 512/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 640/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 768/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 896/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 1024/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 1152/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 1280/2001\n",
            "INFO - 12/05/22 05:22:50 - 0:19:11 - 1408/2001\n",
            "INFO - 12/05/22 05:22:51 - 0:19:11 - 1536/2001\n",
            "INFO - 12/05/22 05:22:51 - 0:19:11 - 1664/2001\n",
            "INFO - 12/05/22 05:22:51 - 0:19:11 - 1792/2001\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - 1920/2001\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - 1288/2001 (64.36781609195403%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - epoch -> 5.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_xe_loss -> 2.237320\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc -> 65.717141\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_4 -> 76.744186\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_5 -> 64.788732\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_6 -> 70.454545\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_7 -> 72.185430\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_8 -> 68.669528\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_9 -> 69.411765\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_10 -> 63.354037\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_11 -> 70.229008\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_12 -> 61.333333\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_13 -> 60.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_14 -> 66.037736\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_15 -> 63.888889\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_16 -> 58.108108\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_17 -> 65.116279\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_18 -> 65.217391\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_19 -> 59.090909\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_20 -> 53.125000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_21 -> 78.260870\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_22 -> 76.190476\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_23 -> 72.727273\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_24 -> 45.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_25 -> 44.444444\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_26 -> 73.333333\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_27 -> 76.923077\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_28 -> 61.538462\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_29 -> 63.636364\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_30 -> 44.444444\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_45 -> 33.333333\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_xe_loss -> 2.337371\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc -> 64.367816\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_4 -> 64.444444\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_5 -> 70.175439\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_6 -> 72.592593\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_7 -> 69.565217\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_8 -> 67.129630\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_9 -> 67.080745\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_10 -> 68.902439\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_11 -> 53.956835\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_12 -> 55.474453\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_13 -> 63.461538\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_14 -> 62.650602\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_15 -> 62.500000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_16 -> 55.714286\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_17 -> 61.363636\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_18 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_19 -> 73.170732\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_20 -> 72.340426\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_21 -> 59.259259\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_22 -> 72.413793\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_23 -> 70.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_24 -> 47.619048\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_25 -> 65.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_27 -> 57.142857\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_28 -> 77.777778\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_29 -> 64.705882\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - __log__:{\"epoch\": 5, \"valid_prim_bwd_xe_loss\": 2.237319923412317, \"valid_prim_bwd_acc\": 65.71714142928536, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 76.74418604651163, \"valid_prim_bwd_acc_5\": 64.78873239436619, \"valid_prim_bwd_acc_6\": 70.45454545454545, \"valid_prim_bwd_acc_7\": 72.18543046357615, \"valid_prim_bwd_acc_8\": 68.6695278969957, \"valid_prim_bwd_acc_9\": 69.41176470588235, \"valid_prim_bwd_acc_10\": 63.35403726708075, \"valid_prim_bwd_acc_11\": 70.22900763358778, \"valid_prim_bwd_acc_12\": 61.333333333333336, \"valid_prim_bwd_acc_13\": 60.0, \"valid_prim_bwd_acc_14\": 66.0377358490566, \"valid_prim_bwd_acc_15\": 63.888888888888886, \"valid_prim_bwd_acc_16\": 58.108108108108105, \"valid_prim_bwd_acc_17\": 65.11627906976744, \"valid_prim_bwd_acc_18\": 65.21739130434783, \"valid_prim_bwd_acc_19\": 59.09090909090909, \"valid_prim_bwd_acc_20\": 53.125, \"valid_prim_bwd_acc_21\": 78.26086956521739, \"valid_prim_bwd_acc_22\": 76.19047619047619, \"valid_prim_bwd_acc_23\": 72.72727272727273, \"valid_prim_bwd_acc_24\": 45.0, \"valid_prim_bwd_acc_25\": 44.44444444444444, \"valid_prim_bwd_acc_26\": 73.33333333333333, \"valid_prim_bwd_acc_27\": 76.92307692307692, \"valid_prim_bwd_acc_28\": 61.53846153846154, \"valid_prim_bwd_acc_29\": 63.63636363636363, \"valid_prim_bwd_acc_30\": 44.44444444444444, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 33.333333333333336, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 0.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 50.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.33737132288348, \"test_prim_bwd_acc\": 64.36781609195403, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 64.44444444444444, \"test_prim_bwd_acc_5\": 70.17543859649123, \"test_prim_bwd_acc_6\": 72.5925925925926, \"test_prim_bwd_acc_7\": 69.56521739130434, \"test_prim_bwd_acc_8\": 67.12962962962963, \"test_prim_bwd_acc_9\": 67.0807453416149, \"test_prim_bwd_acc_10\": 68.90243902439025, \"test_prim_bwd_acc_11\": 53.9568345323741, \"test_prim_bwd_acc_12\": 55.47445255474452, \"test_prim_bwd_acc_13\": 63.46153846153846, \"test_prim_bwd_acc_14\": 62.65060240963855, \"test_prim_bwd_acc_15\": 62.5, \"test_prim_bwd_acc_16\": 55.714285714285715, \"test_prim_bwd_acc_17\": 61.36363636363637, \"test_prim_bwd_acc_18\": 66.66666666666667, \"test_prim_bwd_acc_19\": 73.17073170731707, \"test_prim_bwd_acc_20\": 72.34042553191489, \"test_prim_bwd_acc_21\": 59.25925925925926, \"test_prim_bwd_acc_22\": 72.41379310344827, \"test_prim_bwd_acc_23\": 70.0, \"test_prim_bwd_acc_24\": 47.61904761904762, \"test_prim_bwd_acc_25\": 65.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 57.142857142857146, \"test_prim_bwd_acc_28\": 77.77777777777777, \"test_prim_bwd_acc_29\": 64.70588235294117, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 66.66666666666667, \"test_prim_bwd_acc_34\": 100.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 50.0, \"test_prim_bwd_acc_43\": 66.66666666666667, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 0.0, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:22:51 - 0:19:12 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:22:51 - 0:19:12 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:22:51 - 0:19:12 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:22:51 - 0:19:12 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:22:51 - 0:19:12 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:22:58 - 0:19:19 - ============ Starting epoch 6 ... ============\n",
            "INFO - 12/05/22 05:23:00 - 0:19:21 -    4700 -  170.61 equations/s -  7173.42 words/s - PRIM-BWD:  0.0345 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:05 - 0:19:26 -    4720 -  499.79 equations/s - 21030.61 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:10 - 0:19:30 -    4740 -  572.83 equations/s - 23347.17 words/s - PRIM-BWD:  0.0309 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:15 - 0:19:35 -    4760 -  529.19 equations/s - 22427.10 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:20 - 0:19:41 -    4780 -  472.53 equations/s - 19817.65 words/s - PRIM-BWD:  0.0322 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:25 - 0:19:45 -    4800 -  560.35 equations/s - 23606.84 words/s - PRIM-BWD:  0.0316 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:29 - 0:19:50 -    4820 -  536.09 equations/s - 22617.03 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:34 - 0:19:54 -    4840 -  596.29 equations/s - 24829.84 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:38 - 0:19:59 -    4860 -  581.19 equations/s - 24011.59 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:43 - 0:20:03 -    4880 -  563.89 equations/s - 23349.73 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:47 - 0:20:08 -    4900 -  573.16 equations/s - 24299.24 words/s - PRIM-BWD:  0.0323 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:52 - 0:20:12 -    4920 -  572.93 equations/s - 23806.41 words/s - PRIM-BWD:  0.0324 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:23:56 - 0:20:17 -    4940 -  572.73 equations/s - 23458.70 words/s - PRIM-BWD:  0.0313 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:01 - 0:20:21 -    4960 -  560.05 equations/s - 23248.47 words/s - PRIM-BWD:  0.0321 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:05 - 0:20:26 -    4980 -  548.12 equations/s - 22943.89 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:09 - 0:20:30 -    5000 -  698.99 equations/s - 28927.02 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:13 - 0:20:34 -    5020 -  568.59 equations/s - 23473.48 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:18 - 0:20:38 -    5040 -  593.99 equations/s - 24597.00 words/s - PRIM-BWD:  0.0330 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:22 - 0:20:43 -    5060 -  553.14 equations/s - 23076.64 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:28 - 0:20:48 -    5080 -  491.87 equations/s - 20877.83 words/s - PRIM-BWD:  0.0283 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:32 - 0:20:52 -    5100 -  600.03 equations/s - 24984.07 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:37 - 0:20:57 -    5120 -  533.04 equations/s - 22273.84 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:41 - 0:21:02 -    5140 -  550.40 equations/s - 22652.03 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:46 - 0:21:06 -    5160 -  574.34 equations/s - 23891.79 words/s - PRIM-BWD:  0.0276 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:51 - 0:21:12 -    5180 -  483.24 equations/s - 20204.21 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:24:56 - 0:21:16 -    5200 -  558.51 equations/s - 23273.43 words/s - PRIM-BWD:  0.0317 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:00 - 0:21:21 -    5220 -  542.07 equations/s - 22664.57 words/s - PRIM-BWD:  0.0313 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:05 - 0:21:26 -    5240 -  558.37 equations/s - 23425.87 words/s - PRIM-BWD:  0.0282 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:09 - 0:21:30 -    5260 -  576.00 equations/s - 23995.28 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:13 - 0:21:34 -    5280 -  649.50 equations/s - 26750.18 words/s - PRIM-BWD:  0.0287 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:18 - 0:21:39 -    5300 -  549.19 equations/s - 22807.46 words/s - PRIM-BWD:  0.0325 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:22 - 0:21:43 -    5320 -  592.26 equations/s - 24630.72 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:27 - 0:21:48 -    5340 -  505.35 equations/s - 21085.21 words/s - PRIM-BWD:  0.0311 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:32 - 0:21:53 -    5360 -  557.36 equations/s - 23153.09 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:37 - 0:21:57 -    5380 -  544.35 equations/s - 22778.06 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:41 - 0:22:01 -    5400 -  633.60 equations/s - 26356.85 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:45 - 0:22:06 -    5420 -  529.83 equations/s - 22325.43 words/s - PRIM-BWD:  0.0304 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:50 - 0:22:11 -    5440 -  512.64 equations/s - 21073.80 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:55 - 0:22:16 -    5460 -  554.78 equations/s - 23196.73 words/s - PRIM-BWD:  0.0316 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:25:58 - 0:22:19 - ============ End of epoch 6 ============\n",
            "INFO - 12/05/22 05:25:58 - 0:22:19 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:25:58 - 0:22:19 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:25:58 - 0:22:19 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:25:59 - 0:22:19 - 0/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:19 - 128/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:19 - 256/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:19 - 384/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 512/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 640/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 768/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 896/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 1024/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 1152/2001\n",
            "INFO - 12/05/22 05:25:59 - 0:22:20 - 1280/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:20 - 1408/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:20 - 1536/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:20 - 1664/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:20 - 1792/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 1920/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 1326/2001 (66.26686656671664%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 0/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 128/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 256/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 384/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 512/2001\n",
            "INFO - 12/05/22 05:26:00 - 0:22:21 - 640/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:21 - 768/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:21 - 896/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:21 - 1024/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:21 - 1152/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:22 - 1280/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:22 - 1408/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:22 - 1536/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:22 - 1664/2001\n",
            "INFO - 12/05/22 05:26:01 - 0:22:22 - 1792/2001\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - 1920/2001\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - 1317/2001 (65.81709145427287%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - epoch -> 6.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_xe_loss -> 2.439226\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc -> 66.266867\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_3 -> 60.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_4 -> 72.093023\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_6 -> 72.727273\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_7 -> 72.847682\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_8 -> 69.527897\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_9 -> 68.823529\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_10 -> 62.732919\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_11 -> 70.229008\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_12 -> 62.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_13 -> 65.263158\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_14 -> 71.698113\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_15 -> 62.500000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_16 -> 68.918919\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_17 -> 62.790698\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_18 -> 65.217391\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_19 -> 61.363636\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_20 -> 59.375000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_21 -> 73.913043\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_22 -> 57.142857\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_23 -> 68.181818\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_24 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_25 -> 38.888889\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_26 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_28 -> 46.153846\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_29 -> 45.454545\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_30 -> 77.777778\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_xe_loss -> 2.605070\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc -> 65.817091\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_3 -> 55.555556\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_4 -> 64.444444\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_5 -> 71.929825\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_6 -> 74.074074\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_8 -> 68.981481\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_9 -> 65.217391\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_10 -> 73.170732\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_11 -> 58.273381\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_12 -> 59.854015\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_13 -> 67.307692\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_14 -> 60.240964\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_15 -> 65.625000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_16 -> 55.714286\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_17 -> 63.636364\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_18 -> 59.523810\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_19 -> 65.853659\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_20 -> 76.595745\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_21 -> 59.259259\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_22 -> 72.413793\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_23 -> 70.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_24 -> 57.142857\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_25 -> 75.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_26 -> 64.285714\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_27 -> 64.285714\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_28 -> 77.777778\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_29 -> 58.823529\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_31 -> 75.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - __log__:{\"epoch\": 6, \"valid_prim_bwd_xe_loss\": 2.4392260046734444, \"valid_prim_bwd_acc\": 66.26686656671664, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 60.0, \"valid_prim_bwd_acc_4\": 72.09302325581395, \"valid_prim_bwd_acc_5\": 66.19718309859155, \"valid_prim_bwd_acc_6\": 72.72727272727273, \"valid_prim_bwd_acc_7\": 72.8476821192053, \"valid_prim_bwd_acc_8\": 69.52789699570816, \"valid_prim_bwd_acc_9\": 68.82352941176471, \"valid_prim_bwd_acc_10\": 62.732919254658384, \"valid_prim_bwd_acc_11\": 70.22900763358778, \"valid_prim_bwd_acc_12\": 62.0, \"valid_prim_bwd_acc_13\": 65.26315789473684, \"valid_prim_bwd_acc_14\": 71.69811320754717, \"valid_prim_bwd_acc_15\": 62.5, \"valid_prim_bwd_acc_16\": 68.91891891891892, \"valid_prim_bwd_acc_17\": 62.7906976744186, \"valid_prim_bwd_acc_18\": 65.21739130434783, \"valid_prim_bwd_acc_19\": 61.36363636363637, \"valid_prim_bwd_acc_20\": 59.375, \"valid_prim_bwd_acc_21\": 73.91304347826087, \"valid_prim_bwd_acc_22\": 57.142857142857146, \"valid_prim_bwd_acc_23\": 68.18181818181819, \"valid_prim_bwd_acc_24\": 50.0, \"valid_prim_bwd_acc_25\": 38.888888888888886, \"valid_prim_bwd_acc_26\": 66.66666666666667, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 46.15384615384615, \"valid_prim_bwd_acc_29\": 45.45454545454545, \"valid_prim_bwd_acc_30\": 77.77777777777777, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 0.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 0.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 0.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 50.0, \"valid_prim_bwd_acc_103\": 0.0, \"valid_prim_bwd_acc_104\": 0.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.6050698405679973, \"test_prim_bwd_acc\": 65.81709145427287, \"test_prim_bwd_acc_3\": 55.55555555555556, \"test_prim_bwd_acc_4\": 64.44444444444444, \"test_prim_bwd_acc_5\": 71.9298245614035, \"test_prim_bwd_acc_6\": 74.07407407407408, \"test_prim_bwd_acc_7\": 71.19565217391305, \"test_prim_bwd_acc_8\": 68.98148148148148, \"test_prim_bwd_acc_9\": 65.21739130434783, \"test_prim_bwd_acc_10\": 73.17073170731707, \"test_prim_bwd_acc_11\": 58.273381294964025, \"test_prim_bwd_acc_12\": 59.85401459854015, \"test_prim_bwd_acc_13\": 67.3076923076923, \"test_prim_bwd_acc_14\": 60.24096385542169, \"test_prim_bwd_acc_15\": 65.625, \"test_prim_bwd_acc_16\": 55.714285714285715, \"test_prim_bwd_acc_17\": 63.63636363636363, \"test_prim_bwd_acc_18\": 59.523809523809526, \"test_prim_bwd_acc_19\": 65.85365853658537, \"test_prim_bwd_acc_20\": 76.59574468085107, \"test_prim_bwd_acc_21\": 59.25925925925926, \"test_prim_bwd_acc_22\": 72.41379310344827, \"test_prim_bwd_acc_23\": 70.0, \"test_prim_bwd_acc_24\": 57.142857142857146, \"test_prim_bwd_acc_25\": 75.0, \"test_prim_bwd_acc_26\": 64.28571428571429, \"test_prim_bwd_acc_27\": 64.28571428571429, \"test_prim_bwd_acc_28\": 77.77777777777777, \"test_prim_bwd_acc_29\": 58.8235294117647, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 75.0, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 80.0, \"test_prim_bwd_acc_38\": 20.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 66.66666666666667, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 0.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 100.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:26:02 - 0:22:22 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:26:02 - 0:22:22 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:26:02 - 0:22:22 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:26:02 - 0:22:22 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:26:02 - 0:22:22 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:26:09 - 0:22:29 - ============ Starting epoch 7 ... ============\n",
            "INFO - 12/05/22 05:26:10 - 0:22:31 -    5480 -  173.29 equations/s -  7228.44 words/s - PRIM-BWD:  0.0310 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:15 - 0:22:36 -    5500 -  501.47 equations/s - 21338.54 words/s - PRIM-BWD:  0.0294 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:19 - 0:22:39 -    5520 -  702.54 equations/s - 28770.15 words/s - PRIM-BWD:  0.0318 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:23 - 0:22:44 -    5540 -  554.79 equations/s - 22855.70 words/s - PRIM-BWD:  0.0293 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:27 - 0:22:48 -    5560 -  606.05 equations/s - 25143.34 words/s - PRIM-BWD:  0.0309 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:32 - 0:22:53 -    5580 -  582.96 equations/s - 24266.32 words/s - PRIM-BWD:  0.0297 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:36 - 0:22:57 -    5600 -  639.93 equations/s - 26399.37 words/s - PRIM-BWD:  0.0298 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:40 - 0:23:01 -    5620 -  585.77 equations/s - 24430.77 words/s - PRIM-BWD:  0.0305 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:45 - 0:23:06 -    5640 -  537.99 equations/s - 22369.94 words/s - PRIM-BWD:  0.0302 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:49 - 0:23:10 -    5660 -  583.23 equations/s - 24182.80 words/s - PRIM-BWD:  0.0296 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:26:55 - 0:23:16 -    5680 -  448.01 equations/s - 18751.62 words/s - PRIM-BWD:  0.0292 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:01 - 0:23:22 -    5700 -  441.68 equations/s - 18786.09 words/s - PRIM-BWD:  0.0292 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:05 - 0:23:26 -    5720 -  593.88 equations/s - 24528.51 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:10 - 0:23:30 -    5740 -  555.50 equations/s - 23310.29 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:14 - 0:23:35 -    5760 -  560.84 equations/s - 23740.49 words/s - PRIM-BWD:  0.0289 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:19 - 0:23:40 -    5780 -  508.58 equations/s - 21561.44 words/s - PRIM-BWD:  0.0301 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:24 - 0:23:45 -    5800 -  560.32 equations/s - 23366.77 words/s - PRIM-BWD:  0.0293 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:28 - 0:23:49 -    5820 -  593.25 equations/s - 24475.40 words/s - PRIM-BWD:  0.0302 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:33 - 0:23:53 -    5840 -  600.86 equations/s - 24587.20 words/s - PRIM-BWD:  0.0282 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:37 - 0:23:58 -    5860 -  597.19 equations/s - 24753.65 words/s - PRIM-BWD:  0.0284 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:41 - 0:24:02 -    5880 -  554.77 equations/s - 22917.63 words/s - PRIM-BWD:  0.0295 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:46 - 0:24:07 -    5900 -  546.38 equations/s - 23086.61 words/s - PRIM-BWD:  0.0308 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:51 - 0:24:11 -    5920 -  555.74 equations/s - 23315.85 words/s - PRIM-BWD:  0.0297 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:27:55 - 0:24:16 -    5940 -  601.39 equations/s - 24819.35 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:00 - 0:24:21 -    5960 -  518.76 equations/s - 21459.78 words/s - PRIM-BWD:  0.0326 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:04 - 0:24:25 -    5980 -  604.39 equations/s - 25092.89 words/s - PRIM-BWD:  0.0334 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:09 - 0:24:30 -    6000 -  506.52 equations/s - 21276.80 words/s - PRIM-BWD:  0.0296 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 05:28:15 - 0:24:36 -    6020 -  448.20 equations/s - 18802.06 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:20 - 0:24:41 -    6040 -  492.24 equations/s - 20451.99 words/s - PRIM-BWD:  0.0299 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:26 - 0:24:46 -    6060 -  472.33 equations/s - 19541.28 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:30 - 0:24:51 -    6080 -  530.60 equations/s - 22205.30 words/s - PRIM-BWD:  0.0301 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:35 - 0:24:56 -    6100 -  539.86 equations/s - 22475.22 words/s - PRIM-BWD:  0.0277 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:39 - 0:25:00 -    6120 -  617.69 equations/s - 25735.79 words/s - PRIM-BWD:  0.0288 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:44 - 0:25:04 -    6140 -  578.62 equations/s - 24232.51 words/s - PRIM-BWD:  0.0305 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:48 - 0:25:08 -    6160 -  665.24 equations/s - 27274.75 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:52 - 0:25:13 -    6180 -  592.19 equations/s - 24416.59 words/s - PRIM-BWD:  0.0281 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:28:56 - 0:25:17 -    6200 -  598.30 equations/s - 24621.18 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:01 - 0:25:21 -    6220 -  583.77 equations/s - 24685.97 words/s - PRIM-BWD:  0.0292 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:05 - 0:25:25 -    6240 -  605.17 equations/s - 25037.72 words/s - PRIM-BWD:  0.0272 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - ============ End of epoch 7 ============\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 0/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 128/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 256/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 384/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 512/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 640/2001\n",
            "INFO - 12/05/22 05:29:09 - 0:25:30 - 768/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:30 - 896/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:30 - 1024/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:30 - 1152/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1280/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1408/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1536/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1664/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1792/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1920/2001\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - 1316/2001 (65.76711644177911%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:29:10 - 0:25:31 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:29:11 - 0:25:31 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:29:11 - 0:25:31 - 0/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:31 - 128/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:31 - 256/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:31 - 384/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 512/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 640/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 768/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 896/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 1024/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 1152/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 1280/2001\n",
            "INFO - 12/05/22 05:29:11 - 0:25:32 - 1408/2001\n",
            "INFO - 12/05/22 05:29:12 - 0:25:32 - 1536/2001\n",
            "INFO - 12/05/22 05:29:12 - 0:25:32 - 1664/2001\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - 1792/2001\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - 1920/2001\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - 1319/2001 (65.91704147926038%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - epoch -> 7.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_xe_loss -> 2.494810\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc -> 65.767116\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_6 -> 68.939394\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_7 -> 70.860927\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_8 -> 67.811159\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_9 -> 65.294118\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_10 -> 63.975155\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_11 -> 70.229008\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_12 -> 61.333333\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_13 -> 62.105263\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_14 -> 70.754717\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_15 -> 63.888889\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_16 -> 63.513514\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_17 -> 65.116279\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_18 -> 60.869565\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_19 -> 56.818182\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_20 -> 56.250000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_21 -> 78.260870\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_22 -> 57.142857\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_23 -> 77.272727\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_24 -> 55.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_25 -> 72.222222\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_28 -> 69.230769\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_29 -> 54.545455\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_44 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_61 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_xe_loss -> 2.649347\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc -> 65.917041\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_4 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_5 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_6 -> 77.777778\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_7 -> 72.282609\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_8 -> 68.981481\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_9 -> 68.944099\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_10 -> 71.341463\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_11 -> 58.992806\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_12 -> 56.934307\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_13 -> 65.384615\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_14 -> 60.240964\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_15 -> 65.625000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_16 -> 55.714286\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_17 -> 61.363636\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_18 -> 59.523810\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_19 -> 68.292683\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_20 -> 74.468085\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_21 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_22 -> 75.862069\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_23 -> 65.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_24 -> 52.380952\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_25 -> 75.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_27 -> 57.142857\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_29 -> 52.941176\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - __log__:{\"epoch\": 7, \"valid_prim_bwd_xe_loss\": 2.4948097240442277, \"valid_prim_bwd_acc\": 65.76711644177911, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 74.4186046511628, \"valid_prim_bwd_acc_5\": 66.19718309859155, \"valid_prim_bwd_acc_6\": 68.93939393939394, \"valid_prim_bwd_acc_7\": 70.86092715231788, \"valid_prim_bwd_acc_8\": 67.81115879828326, \"valid_prim_bwd_acc_9\": 65.29411764705883, \"valid_prim_bwd_acc_10\": 63.975155279503106, \"valid_prim_bwd_acc_11\": 70.22900763358778, \"valid_prim_bwd_acc_12\": 61.333333333333336, \"valid_prim_bwd_acc_13\": 62.10526315789474, \"valid_prim_bwd_acc_14\": 70.75471698113208, \"valid_prim_bwd_acc_15\": 63.888888888888886, \"valid_prim_bwd_acc_16\": 63.513513513513516, \"valid_prim_bwd_acc_17\": 65.11627906976744, \"valid_prim_bwd_acc_18\": 60.869565217391305, \"valid_prim_bwd_acc_19\": 56.81818181818182, \"valid_prim_bwd_acc_20\": 56.25, \"valid_prim_bwd_acc_21\": 78.26086956521739, \"valid_prim_bwd_acc_22\": 57.142857142857146, \"valid_prim_bwd_acc_23\": 77.27272727272727, \"valid_prim_bwd_acc_24\": 55.0, \"valid_prim_bwd_acc_25\": 72.22222222222223, \"valid_prim_bwd_acc_26\": 60.0, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 69.23076923076923, \"valid_prim_bwd_acc_29\": 54.54545454545455, \"valid_prim_bwd_acc_30\": 66.66666666666667, \"valid_prim_bwd_acc_31\": 66.66666666666667, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 100.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 0.0, \"valid_prim_bwd_acc_49\": 0.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 100.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.6493470793899925, \"test_prim_bwd_acc\": 65.91704147926038, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 66.66666666666667, \"test_prim_bwd_acc_5\": 66.66666666666667, \"test_prim_bwd_acc_6\": 77.77777777777777, \"test_prim_bwd_acc_7\": 72.28260869565217, \"test_prim_bwd_acc_8\": 68.98148148148148, \"test_prim_bwd_acc_9\": 68.94409937888199, \"test_prim_bwd_acc_10\": 71.34146341463415, \"test_prim_bwd_acc_11\": 58.992805755395686, \"test_prim_bwd_acc_12\": 56.934306569343065, \"test_prim_bwd_acc_13\": 65.38461538461539, \"test_prim_bwd_acc_14\": 60.24096385542169, \"test_prim_bwd_acc_15\": 65.625, \"test_prim_bwd_acc_16\": 55.714285714285715, \"test_prim_bwd_acc_17\": 61.36363636363637, \"test_prim_bwd_acc_18\": 59.523809523809526, \"test_prim_bwd_acc_19\": 68.29268292682927, \"test_prim_bwd_acc_20\": 74.46808510638297, \"test_prim_bwd_acc_21\": 66.66666666666667, \"test_prim_bwd_acc_22\": 75.86206896551724, \"test_prim_bwd_acc_23\": 65.0, \"test_prim_bwd_acc_24\": 52.38095238095238, \"test_prim_bwd_acc_25\": 75.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 57.142857142857146, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 52.94117647058823, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 100.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 20.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 0.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 66.66666666666667, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 0.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 0.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:29:12 - 0:25:33 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:29:12 - 0:25:33 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:29:12 - 0:25:33 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:29:12 - 0:25:33 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:29:12 - 0:25:33 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:29:19 - 0:25:39 - ============ Starting epoch 8 ... ============\n",
            "INFO - 12/05/22 05:29:20 - 0:25:40 -    6260 -  171.95 equations/s -  7137.70 words/s - PRIM-BWD:  0.0286 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:25 - 0:25:45 -    6280 -  513.46 equations/s - 21385.89 words/s - PRIM-BWD:  0.0310 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:29 - 0:25:50 -    6300 -  584.52 equations/s - 24256.35 words/s - PRIM-BWD:  0.0328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:34 - 0:25:54 -    6320 -  565.62 equations/s - 23436.13 words/s - PRIM-BWD:  0.0283 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:39 - 0:25:59 -    6340 -  498.65 equations/s - 20860.04 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:43 - 0:26:04 -    6360 -  559.61 equations/s - 23326.27 words/s - PRIM-BWD:  0.0312 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:48 - 0:26:09 -    6380 -  515.69 equations/s - 21477.81 words/s - PRIM-BWD:  0.0298 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:52 - 0:26:13 -    6400 -  633.46 equations/s - 26279.07 words/s - PRIM-BWD:  0.0319 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:29:58 - 0:26:18 -    6420 -  481.55 equations/s - 20118.13 words/s - PRIM-BWD:  0.0306 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:03 - 0:26:24 -    6440 -  470.36 equations/s - 19880.48 words/s - PRIM-BWD:  0.0308 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:07 - 0:26:28 -    6460 -  577.02 equations/s - 23944.93 words/s - PRIM-BWD:  0.0305 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:12 - 0:26:33 -    6480 -  553.34 equations/s - 23112.13 words/s - PRIM-BWD:  0.0320 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:16 - 0:26:37 -    6500 -  610.05 equations/s - 25250.44 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:21 - 0:26:42 -    6520 -  546.29 equations/s - 22673.84 words/s - PRIM-BWD:  0.0290 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:26 - 0:26:47 -    6540 -  517.24 equations/s - 21476.04 words/s - PRIM-BWD:  0.0278 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:31 - 0:26:52 -    6560 -  479.79 equations/s - 20447.95 words/s - PRIM-BWD:  0.0279 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:36 - 0:26:57 -    6580 -  518.19 equations/s - 21502.76 words/s - PRIM-BWD:  0.0314 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:41 - 0:27:02 -    6600 -  534.96 equations/s - 22190.97 words/s - PRIM-BWD:  0.0303 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:46 - 0:27:06 -    6620 -  551.61 equations/s - 23124.18 words/s - PRIM-BWD:  0.0322 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:50 - 0:27:11 -    6640 -  567.68 equations/s - 24007.72 words/s - PRIM-BWD:  0.0277 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:54 - 0:27:15 -    6660 -  606.00 equations/s - 24781.47 words/s - PRIM-BWD:  0.0297 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:30:59 - 0:27:19 -    6680 -  597.83 equations/s - 24430.74 words/s - PRIM-BWD:  0.0311 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:03 - 0:27:24 -    6700 -  563.27 equations/s - 23294.98 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:08 - 0:27:28 -    6720 -  565.20 equations/s - 23293.78 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:12 - 0:27:33 -    6740 -  600.23 equations/s - 24920.22 words/s - PRIM-BWD:  0.0302 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:17 - 0:27:37 -    6760 -  532.00 equations/s - 22397.68 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:22 - 0:27:43 -    6780 -  471.22 equations/s - 19994.25 words/s - PRIM-BWD:  0.0305 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:27 - 0:27:48 -    6800 -  537.05 equations/s - 22440.61 words/s - PRIM-BWD:  0.0305 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:31 - 0:27:52 -    6820 -  591.48 equations/s - 24518.29 words/s - PRIM-BWD:  0.0291 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:36 - 0:27:57 -    6840 -  537.23 equations/s - 22404.91 words/s - PRIM-BWD:  0.0329 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:41 - 0:28:01 -    6860 -  563.50 equations/s - 23282.60 words/s - PRIM-BWD:  0.0289 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:45 - 0:28:06 -    6880 -  587.56 equations/s - 24055.27 words/s - PRIM-BWD:  0.0296 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:50 - 0:28:10 -    6900 -  562.89 equations/s - 23529.40 words/s - PRIM-BWD:  0.0275 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:54 - 0:28:14 -    6920 -  606.09 equations/s - 25055.24 words/s - PRIM-BWD:  0.0309 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:31:59 - 0:28:20 -    6940 -  498.50 equations/s - 20593.34 words/s - PRIM-BWD:  0.0307 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:32:03 - 0:28:24 -    6960 -  587.09 equations/s - 24683.81 words/s - PRIM-BWD:  0.0315 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:32:08 - 0:28:29 -    6980 -  528.51 equations/s - 22207.57 words/s - PRIM-BWD:  0.0275 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:32:13 - 0:28:33 -    7000 -  559.94 equations/s - 23110.94 words/s - PRIM-BWD:  0.0285 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:32:17 - 0:28:38 -    7020 -  561.62 equations/s - 23524.80 words/s - PRIM-BWD:  0.0292 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - ============ End of epoch 8 ============\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - 0/2001\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - 128/2001\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - 256/2001\n",
            "INFO - 12/05/22 05:32:21 - 0:28:42 - 384/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:42 - 512/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:42 - 640/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:42 - 768/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:42 - 896/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:42 - 1024/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1152/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1280/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1408/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1536/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1664/2001\n",
            "INFO - 12/05/22 05:32:22 - 0:28:43 - 1792/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - 1920/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - 1286/2001 (64.26786606696652%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:32:23 - 0:28:43 - 0/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 128/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 256/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 384/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 512/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 640/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 768/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 896/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 1024/2001\n",
            "INFO - 12/05/22 05:32:23 - 0:28:44 - 1152/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:44 - 1280/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:44 - 1408/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:44 - 1536/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - 1664/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - 1792/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - 1920/2001\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - 1271/2001 (63.51824087956022%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - epoch -> 8.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_xe_loss -> 2.640875\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc -> 64.267866\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_4 -> 76.744186\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_6 -> 68.181818\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_7 -> 72.847682\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_8 -> 66.952790\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_9 -> 66.470588\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_10 -> 60.248447\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_11 -> 70.992366\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_12 -> 58.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_13 -> 58.947368\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_14 -> 66.981132\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_15 -> 61.111111\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_16 -> 63.513514\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_17 -> 55.813953\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_18 -> 65.217391\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_19 -> 56.818182\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_20 -> 56.250000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_21 -> 73.913043\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_22 -> 61.904762\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_23 -> 68.181818\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_24 -> 35.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_25 -> 33.333333\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_27 -> 76.923077\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_29 -> 63.636364\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_xe_loss -> 2.744810\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc -> 63.518241\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_4 -> 57.777778\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_5 -> 68.421053\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_6 -> 75.555556\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_7 -> 72.826087\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_8 -> 64.814815\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_9 -> 68.944099\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_10 -> 69.512195\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_11 -> 55.395683\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_12 -> 54.014599\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_13 -> 63.461538\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_14 -> 57.831325\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_15 -> 64.062500\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_16 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_17 -> 59.090909\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_18 -> 54.761905\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_19 -> 65.853659\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_20 -> 74.468085\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_21 -> 51.851852\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_22 -> 68.965517\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_23 -> 70.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_24 -> 42.857143\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_25 -> 65.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_26 -> 71.428571\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_27 -> 71.428571\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_28 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_29 -> 47.058824\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_34 -> 60.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_46 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_54 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - test_prim_bwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - __log__:{\"epoch\": 8, \"valid_prim_bwd_xe_loss\": 2.640875332597373, \"valid_prim_bwd_acc\": 64.26786606696652, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 80.0, \"valid_prim_bwd_acc_4\": 76.74418604651163, \"valid_prim_bwd_acc_5\": 66.19718309859155, \"valid_prim_bwd_acc_6\": 68.18181818181819, \"valid_prim_bwd_acc_7\": 72.8476821192053, \"valid_prim_bwd_acc_8\": 66.95278969957081, \"valid_prim_bwd_acc_9\": 66.47058823529412, \"valid_prim_bwd_acc_10\": 60.24844720496895, \"valid_prim_bwd_acc_11\": 70.99236641221374, \"valid_prim_bwd_acc_12\": 58.0, \"valid_prim_bwd_acc_13\": 58.94736842105263, \"valid_prim_bwd_acc_14\": 66.98113207547169, \"valid_prim_bwd_acc_15\": 61.111111111111114, \"valid_prim_bwd_acc_16\": 63.513513513513516, \"valid_prim_bwd_acc_17\": 55.81395348837209, \"valid_prim_bwd_acc_18\": 65.21739130434783, \"valid_prim_bwd_acc_19\": 56.81818181818182, \"valid_prim_bwd_acc_20\": 56.25, \"valid_prim_bwd_acc_21\": 73.91304347826087, \"valid_prim_bwd_acc_22\": 61.904761904761905, \"valid_prim_bwd_acc_23\": 68.18181818181819, \"valid_prim_bwd_acc_24\": 35.0, \"valid_prim_bwd_acc_25\": 33.333333333333336, \"valid_prim_bwd_acc_26\": 60.0, \"valid_prim_bwd_acc_27\": 76.92307692307692, \"valid_prim_bwd_acc_28\": 53.84615384615385, \"valid_prim_bwd_acc_29\": 63.63636363636363, \"valid_prim_bwd_acc_30\": 55.55555555555556, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 33.333333333333336, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 0.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 0.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 2.7448098460535357, \"test_prim_bwd_acc\": 63.51824087956022, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 57.77777777777778, \"test_prim_bwd_acc_5\": 68.42105263157895, \"test_prim_bwd_acc_6\": 75.55555555555556, \"test_prim_bwd_acc_7\": 72.82608695652173, \"test_prim_bwd_acc_8\": 64.81481481481481, \"test_prim_bwd_acc_9\": 68.94409937888199, \"test_prim_bwd_acc_10\": 69.51219512195122, \"test_prim_bwd_acc_11\": 55.39568345323741, \"test_prim_bwd_acc_12\": 54.01459854014598, \"test_prim_bwd_acc_13\": 63.46153846153846, \"test_prim_bwd_acc_14\": 57.83132530120482, \"test_prim_bwd_acc_15\": 64.0625, \"test_prim_bwd_acc_16\": 50.0, \"test_prim_bwd_acc_17\": 59.09090909090909, \"test_prim_bwd_acc_18\": 54.76190476190476, \"test_prim_bwd_acc_19\": 65.85365853658537, \"test_prim_bwd_acc_20\": 74.46808510638297, \"test_prim_bwd_acc_21\": 51.851851851851855, \"test_prim_bwd_acc_22\": 68.96551724137932, \"test_prim_bwd_acc_23\": 70.0, \"test_prim_bwd_acc_24\": 42.857142857142854, \"test_prim_bwd_acc_25\": 65.0, \"test_prim_bwd_acc_26\": 71.42857142857143, \"test_prim_bwd_acc_27\": 71.42857142857143, \"test_prim_bwd_acc_28\": 66.66666666666667, \"test_prim_bwd_acc_29\": 47.05882352941177, \"test_prim_bwd_acc_30\": 66.66666666666667, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 66.66666666666667, \"test_prim_bwd_acc_34\": 60.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 80.0, \"test_prim_bwd_acc_38\": 20.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 66.66666666666667, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 66.66666666666667, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 100.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 0.0, \"test_prim_bwd_acc_57\": 0.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 0.0, \"test_prim_bwd_acc_144\": 0.0}\n",
            "WARNING - 12/05/22 05:32:24 - 0:28:45 - Metric \"valid_prim_fwd_acc\" not found in scores!\n",
            "INFO - 12/05/22 05:32:24 - 0:28:45 - Saving checkpoint to ./dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:32:24 - 0:28:45 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:32:24 - 0:28:45 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:32:24 - 0:28:45 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:32:30 - 0:28:51 - ============ Starting epoch 9 ... ============\n",
            "INFO - 12/05/22 05:32:31 - 0:28:51 -    7040 -  191.77 equations/s -  7888.33 words/s - PRIM-BWD:  0.0300 - model LR: 1.0000e-04\n",
            "\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 ./SymbolicMathematics/main.py --exp_name derivfwd_modelbwd --fp16 true --amp 2 --tasks \"prim_bwd\" --reload_model \"bwd.pth\" --reload_data \"prim_bwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test\" --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer \"adam,lr=0.0001\" --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_bwd_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento y evaluación del modelo, segundo experimento\n",
        "\n",
        "Ahora se tomarán 500,000 fórmulas para realizar el entrenamiento, 2,000 para el conjunto de validación y 2,000 para el conjunto de prueba."
      ],
      "metadata": {
        "id": "LXVKCKp8w28z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = open('prim_fwd.train', 'r')\n",
        "file2 = open('deriv2_fwd.train', 'w')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "number = 0\n",
        "for line in Lines:\n",
        "    if(number > 500000): #500,000 max\n",
        "      break\n",
        "    firstSplit = line.split(\"|\")\n",
        "    count = firstSplit[0]\n",
        "    eq = firstSplit[1].split(\"\\t\")\n",
        "    IntegralEQ = str(eq[1]).rstrip()\n",
        "    EntradaEQ = str(eq[0]).rstrip()\n",
        "    file2.write(str(count) + \"|sub Y \" + IntegralEQ + \"\\t\" + EntradaEQ[7:] + \"\\n\")\n",
        "    number += 1\n",
        "\n",
        "file1.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "id": "eg2yIIYaxOV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./SymbolicMathematics/main.py --exp_name deriv2fwd_modelbwd --fp16 true --amp 2 --tasks \"prim_bwd\" --reload_model \"bwd.pth\" --reload_data \"prim_bwd,deriv2_fwd.train,deriv_fwd.valid,deriv_fwd.test\" --reload_size 500000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer \"adam,lr=0.0001\" --batch_size 128 --epoch_size 500000 --validation_metrics valid_prim_bwd_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi3szJxvxssr",
        "outputId": "45dfe0d6-b5af-4d23-8e49-14aaacae7a1c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLURM job: False\n",
            "0 - Number of nodes: 1\n",
            "0 - Node ID        : 0\n",
            "0 - Local rank     : 0\n",
            "0 - Global rank    : 0\n",
            "0 - World size     : 1\n",
            "0 - GPUs per node  : 1\n",
            "0 - Master         : True\n",
            "0 - Multi-node     : False\n",
            "0 - Multi-GPU      : False\n",
            "0 - Hostname       : e1165a85fa0b\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - accumulate_gradients: 1\n",
            "                                     amp: 2\n",
            "                                     attention_dropout: 0\n",
            "                                     balanced: False\n",
            "                                     batch_size: 128\n",
            "                                     beam_early_stopping: True\n",
            "                                     beam_eval: False\n",
            "                                     beam_length_penalty: 1\n",
            "                                     beam_size: 1\n",
            "                                     clean_prefix_expr: True\n",
            "                                     clip_grad_norm: 5\n",
            "                                     command: python ./SymbolicMathematics/main.py --exp_name deriv2fwd_modelbwd --fp16 true --amp 2 --tasks prim_bwd --reload_model 'bwd.pth' --reload_data 'prim_bwd,deriv2_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 500000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 500000 --validation_metrics valid_prim_bwd_acc --exp_id \"6gswznivsf\"\n",
            "                                     cpu: False\n",
            "                                     debug: False\n",
            "                                     debug_slurm: False\n",
            "                                     dropout: 0\n",
            "                                     dump_path: ./dumped/deriv2fwd_modelbwd/6gswznivsf\n",
            "                                     emb_dim: 1024\n",
            "                                     env_base_seed: 0\n",
            "                                     env_name: char_sp\n",
            "                                     epoch_size: 500000\n",
            "                                     eval_only: False\n",
            "                                     eval_verbose: 0\n",
            "                                     eval_verbose_print: False\n",
            "                                     exp_id: 6gswznivsf\n",
            "                                     exp_name: deriv2fwd_modelbwd\n",
            "                                     export_data: False\n",
            "                                     fp16: True\n",
            "                                     global_rank: 0\n",
            "                                     int_base: 10\n",
            "                                     is_master: True\n",
            "                                     is_slurm_job: False\n",
            "                                     leaf_probs: 0.75,0,0.25,0\n",
            "                                     local_rank: 0\n",
            "                                     master_port: -1\n",
            "                                     max_epoch: 100000\n",
            "                                     max_int: 10000\n",
            "                                     max_len: 512\n",
            "                                     max_ops: 10\n",
            "                                     max_ops_G: 4\n",
            "                                     multi_gpu: False\n",
            "                                     multi_node: False\n",
            "                                     n_coefficients: 0\n",
            "                                     n_dec_layers: 6\n",
            "                                     n_enc_layers: 6\n",
            "                                     n_gpu_per_node: 1\n",
            "                                     n_heads: 8\n",
            "                                     n_nodes: 1\n",
            "                                     n_variables: 1\n",
            "                                     node_id: 0\n",
            "                                     num_workers: 10\n",
            "                                     operators: add:2,sub:1\n",
            "                                     optimizer: adam,lr=0.0001\n",
            "                                     positive: False\n",
            "                                     precision: 10\n",
            "                                     reload_checkpoint: \n",
            "                                     reload_data: prim_bwd,deriv2_fwd.train,deriv_fwd.valid,deriv_fwd.test\n",
            "                                     reload_model: bwd.pth\n",
            "                                     reload_size: 500000\n",
            "                                     rewrite_functions: \n",
            "                                     same_nb_ops_per_batch: False\n",
            "                                     save_periodic: 0\n",
            "                                     share_inout_emb: True\n",
            "                                     sinusoidal_embeddings: False\n",
            "                                     stopping_criterion: \n",
            "                                     tasks: prim_bwd\n",
            "                                     validation_metrics: valid_prim_bwd_acc\n",
            "                                     world_size: 1\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - The experiment will be stored in ./dumped/deriv2fwd_modelbwd/6gswznivsf\n",
            "                                     \n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - Running command: python ./SymbolicMathematics/main.py --exp_name deriv2fwd_modelbwd --fp16 true --amp 2 --tasks prim_bwd --reload_model 'bwd.pth' --reload_data 'prim_bwd,deriv2_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 500000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 500000 --validation_metrics valid_prim_bwd_acc\n",
            "\n",
            "WARNING - 12/05/22 05:40:57 - 0:00:00 - Signal handler installed.\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - Unary operators: []\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - Binary operators: ['add', 'sub']\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - words: {'<s>': 0, '</s>': 1, '<pad>': 2, '(': 3, ')': 4, '<SPECIAL_5>': 5, '<SPECIAL_6>': 6, '<SPECIAL_7>': 7, '<SPECIAL_8>': 8, '<SPECIAL_9>': 9, 'pi': 10, 'E': 11, 'x': 12, 'y': 13, 'z': 14, 't': 15, 'a0': 16, 'a1': 17, 'a2': 18, 'a3': 19, 'a4': 20, 'a5': 21, 'a6': 22, 'a7': 23, 'a8': 24, 'a9': 25, 'abs': 26, 'acos': 27, 'acosh': 28, 'acot': 29, 'acoth': 30, 'acsc': 31, 'acsch': 32, 'add': 33, 'asec': 34, 'asech': 35, 'asin': 36, 'asinh': 37, 'atan': 38, 'atanh': 39, 'cos': 40, 'cosh': 41, 'cot': 42, 'coth': 43, 'csc': 44, 'csch': 45, 'derivative': 46, 'div': 47, 'exp': 48, 'f': 49, 'g': 50, 'h': 51, 'inv': 52, 'ln': 53, 'mul': 54, 'pow': 55, 'pow2': 56, 'pow3': 57, 'pow4': 58, 'pow5': 59, 'rac': 60, 'sec': 61, 'sech': 62, 'sign': 63, 'sin': 64, 'sinh': 65, 'sqrt': 66, 'sub': 67, 'tan': 68, 'tanh': 69, 'I': 70, 'INT+': 71, 'INT-': 72, 'INT': 73, 'FLOAT': 74, '-': 75, '.': 76, '10^': 77, 'Y': 78, \"Y'\": 79, \"Y''\": 80, '0': 81, '1': 82, '2': 83, '3': 84, '4': 85, '5': 86, '6': 87, '7': 88, '8': 89, '9': 90}\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - 20001 possible leaves.\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - Checking expressions in [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 2.1, 3.1, -0.01, -0.1, -0.3, -0.5, -0.7, -0.9, -1.1, -2.1, -3.1]\n",
            "INFO - 12/05/22 05:40:57 - 0:00:00 - Training tasks: prim_bwd\n",
            "INFO - 12/05/22 05:40:58 - 0:00:02 - Reloading modules from bwd.pth ...\n",
            "INFO - 12/05/22 05:41:01 - 0:00:04 - Number of parameters (encoder): 79866880\n",
            "INFO - 12/05/22 05:41:01 - 0:00:04 - Number of parameters (decoder): 105069659\n",
            "INFO - 12/05/22 05:41:01 - 0:00:05 - Found 261 parameters in model.\n",
            "INFO - 12/05/22 05:41:01 - 0:00:05 - Optimizers: model\n",
            "/usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "INFO - 12/05/22 05:41:01 - 0:00:05 - Creating train iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:41:01 - 0:00:05 - Loading data from deriv2_fwd.train ...\n",
            "INFO - 12/05/22 05:41:03 - 0:00:06 - Loaded 500000 equations from the disk.\n",
            "INFO - 12/05/22 05:41:03 - 0:00:06 - ============ Starting epoch 0 ... ============\n",
            "INFO - 12/05/22 05:41:03 - 0:00:06 - Initialized random generator for worker 0, with seed [0, 0, 0] (base seed=0).\n",
            "/usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/content/SymbolicMathematics/src/optim.py:72: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "INFO - 12/05/22 05:41:10 - 0:00:13 -      20 -  297.37 equations/s - 14097.92 words/s - PRIM-BWD:  1.0997 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:15 - 0:00:19 -      40 -  464.98 equations/s - 21753.59 words/s - PRIM-BWD:  0.2954 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:21 - 0:00:24 -      60 -  467.52 equations/s - 21877.16 words/s - PRIM-BWD:  0.1828 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:27 - 0:00:30 -      80 -  440.34 equations/s - 20531.55 words/s - PRIM-BWD:  0.1405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:33 - 0:00:37 -     100 -  387.97 equations/s - 18621.29 words/s - PRIM-BWD:  0.1242 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:39 - 0:00:43 -     120 -  418.85 equations/s - 19883.86 words/s - PRIM-BWD:  0.1116 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:45 - 0:00:48 -     140 -  484.18 equations/s - 22910.13 words/s - PRIM-BWD:  0.1038 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:50 - 0:00:54 -     160 -  456.19 equations/s - 21396.24 words/s - PRIM-BWD:  0.0922 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:41:56 - 0:01:00 -     180 -  437.61 equations/s - 20500.94 words/s - PRIM-BWD:  0.0928 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:01 - 0:01:05 -     200 -  478.68 equations/s - 22315.84 words/s - PRIM-BWD:  0.0852 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:07 - 0:01:10 -     220 -  477.03 equations/s - 22091.00 words/s - PRIM-BWD:  0.0817 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:13 - 0:01:16 -     240 -  425.11 equations/s - 20218.45 words/s - PRIM-BWD:  0.0774 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:19 - 0:01:22 -     260 -  451.16 equations/s - 21049.00 words/s - PRIM-BWD:  0.0810 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:25 - 0:01:28 -     280 -  415.88 equations/s - 19297.83 words/s - PRIM-BWD:  0.0755 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:31 - 0:01:35 -     300 -  398.11 equations/s - 18559.22 words/s - PRIM-BWD:  0.0740 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:37 - 0:01:41 -     320 -  416.30 equations/s - 19741.74 words/s - PRIM-BWD:  0.0755 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:43 - 0:01:46 -     340 -  454.48 equations/s - 21730.58 words/s - PRIM-BWD:  0.0724 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:48 - 0:01:52 -     360 -  478.02 equations/s - 22789.15 words/s - PRIM-BWD:  0.0717 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:53 - 0:01:57 -     380 -  519.68 equations/s - 24228.64 words/s - PRIM-BWD:  0.0728 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:42:59 - 0:02:03 -     400 -  426.98 equations/s - 19758.99 words/s - PRIM-BWD:  0.0695 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:06 - 0:02:09 -     420 -  398.15 equations/s - 18956.21 words/s - PRIM-BWD:  0.0703 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:12 - 0:02:15 -     440 -  429.52 equations/s - 20319.15 words/s - PRIM-BWD:  0.0686 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:17 - 0:02:20 -     460 -  489.86 equations/s - 23263.81 words/s - PRIM-BWD:  0.0658 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:23 - 0:02:27 -     480 -  389.60 equations/s - 18515.71 words/s - PRIM-BWD:  0.0659 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:29 - 0:02:32 -     500 -  463.48 equations/s - 22007.44 words/s - PRIM-BWD:  0.0633 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:36 - 0:02:39 -     520 -  383.81 equations/s - 17879.44 words/s - PRIM-BWD:  0.0629 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:41 - 0:02:45 -     540 -  446.16 equations/s - 21007.80 words/s - PRIM-BWD:  0.0646 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:48 - 0:02:51 -     560 -  412.90 equations/s - 19693.62 words/s - PRIM-BWD:  0.0670 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:54 - 0:02:58 -     580 -  391.61 equations/s - 18770.91 words/s - PRIM-BWD:  0.0674 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:43:59 - 0:03:03 -     600 -  483.97 equations/s - 22380.33 words/s - PRIM-BWD:  0.0634 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:05 - 0:03:08 -     620 -  458.90 equations/s - 21445.49 words/s - PRIM-BWD:  0.0659 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:10 - 0:03:13 -     640 -  506.59 equations/s - 23548.69 words/s - PRIM-BWD:  0.0629 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:16 - 0:03:20 -     660 -  421.79 equations/s - 20015.78 words/s - PRIM-BWD:  0.0623 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:22 - 0:03:25 -     680 -  441.85 equations/s - 20746.76 words/s - PRIM-BWD:  0.0635 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:27 - 0:03:30 -     700 -  506.45 equations/s - 23675.54 words/s - PRIM-BWD:  0.0608 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:33 - 0:03:36 -     720 -  440.53 equations/s - 20608.75 words/s - PRIM-BWD:  0.0578 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:40 - 0:03:43 -     740 -  359.40 equations/s - 17088.37 words/s - PRIM-BWD:  0.0575 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:45 - 0:03:48 -     760 -  501.83 equations/s - 23194.94 words/s - PRIM-BWD:  0.0595 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:51 - 0:03:55 -     780 -  393.99 equations/s - 18600.34 words/s - PRIM-BWD:  0.0587 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:44:57 - 0:04:00 -     800 -  475.60 equations/s - 22324.05 words/s - PRIM-BWD:  0.0581 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:02 - 0:04:06 -     820 -  459.63 equations/s - 21932.61 words/s - PRIM-BWD:  0.0609 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:08 - 0:04:11 -     840 -  474.76 equations/s - 22310.76 words/s - PRIM-BWD:  0.0592 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:13 - 0:04:17 -     860 -  448.58 equations/s - 21059.59 words/s - PRIM-BWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:19 - 0:04:23 -     880 -  437.98 equations/s - 20750.31 words/s - PRIM-BWD:  0.0591 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:24 - 0:04:28 -     900 -  515.86 equations/s - 23766.11 words/s - PRIM-BWD:  0.0628 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:30 - 0:04:33 -     920 -  479.62 equations/s - 22535.17 words/s - PRIM-BWD:  0.0588 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:36 - 0:04:40 -     940 -  389.14 equations/s - 18314.85 words/s - PRIM-BWD:  0.0540 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:41 - 0:04:45 -     960 -  522.33 equations/s - 24095.72 words/s - PRIM-BWD:  0.0566 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:47 - 0:04:50 -     980 -  441.63 equations/s - 20731.91 words/s - PRIM-BWD:  0.0556 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:52 - 0:04:56 -    1000 -  467.20 equations/s - 21921.99 words/s - PRIM-BWD:  0.0563 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:45:59 - 0:05:02 -    1020 -  415.79 equations/s - 19318.09 words/s - PRIM-BWD:  0.0569 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:04 - 0:05:08 -    1040 -  448.55 equations/s - 21285.28 words/s - PRIM-BWD:  0.0567 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:09 - 0:05:12 -    1060 -  542.91 equations/s - 25337.73 words/s - PRIM-BWD:  0.0552 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:14 - 0:05:17 -    1080 -  518.90 equations/s - 23820.25 words/s - PRIM-BWD:  0.0565 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:19 - 0:05:23 -    1100 -  466.07 equations/s - 21840.30 words/s - PRIM-BWD:  0.0540 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:25 - 0:05:29 -    1120 -  444.75 equations/s - 21005.94 words/s - PRIM-BWD:  0.0561 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:31 - 0:05:34 -    1140 -  466.31 equations/s - 21523.64 words/s - PRIM-BWD:  0.0554 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:36 - 0:05:39 -    1160 -  481.05 equations/s - 22566.52 words/s - PRIM-BWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:41 - 0:05:45 -    1180 -  487.10 equations/s - 22759.77 words/s - PRIM-BWD:  0.0588 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:47 - 0:05:51 -    1200 -  419.16 equations/s - 19860.77 words/s - PRIM-BWD:  0.0514 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:53 - 0:05:56 -    1220 -  470.97 equations/s - 21941.35 words/s - PRIM-BWD:  0.0541 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:46:59 - 0:06:02 -    1240 -  437.98 equations/s - 20683.67 words/s - PRIM-BWD:  0.0529 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:04 - 0:06:08 -    1260 -  437.68 equations/s - 20533.37 words/s - PRIM-BWD:  0.0551 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:10 - 0:06:13 -    1280 -  504.89 equations/s - 23166.98 words/s - PRIM-BWD:  0.0525 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:15 - 0:06:19 -    1300 -  444.87 equations/s - 20907.69 words/s - PRIM-BWD:  0.0542 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:21 - 0:06:25 -    1320 -  433.51 equations/s - 20121.02 words/s - PRIM-BWD:  0.0521 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:28 - 0:06:31 -    1340 -  393.79 equations/s - 18929.46 words/s - PRIM-BWD:  0.0541 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:33 - 0:06:37 -    1360 -  469.64 equations/s - 21948.90 words/s - PRIM-BWD:  0.0506 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:39 - 0:06:42 -    1380 -  463.15 equations/s - 21532.13 words/s - PRIM-BWD:  0.0533 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:45 - 0:06:49 -    1400 -  378.34 equations/s - 17866.67 words/s - PRIM-BWD:  0.0547 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:51 - 0:06:54 -    1420 -  457.93 equations/s - 21440.94 words/s - PRIM-BWD:  0.0555 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:47:57 - 0:07:00 -    1440 -  453.89 equations/s - 21099.82 words/s - PRIM-BWD:  0.0556 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:02 - 0:07:05 -    1460 -  487.93 equations/s - 22577.38 words/s - PRIM-BWD:  0.0524 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:08 - 0:07:11 -    1480 -  453.73 equations/s - 21505.84 words/s - PRIM-BWD:  0.0517 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:13 - 0:07:17 -    1500 -  461.27 equations/s - 21815.42 words/s - PRIM-BWD:  0.0569 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:19 - 0:07:23 -    1520 -  409.87 equations/s - 19330.98 words/s - PRIM-BWD:  0.0518 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:25 - 0:07:28 -    1540 -  455.76 equations/s - 21249.97 words/s - PRIM-BWD:  0.0538 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:31 - 0:07:34 -    1560 -  449.86 equations/s - 21349.35 words/s - PRIM-BWD:  0.0561 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:38 - 0:07:41 -    1580 -  369.73 equations/s - 17639.67 words/s - PRIM-BWD:  0.0516 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:42 - 0:07:46 -    1600 -  566.70 equations/s - 26339.62 words/s - PRIM-BWD:  0.0502 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:48 - 0:07:52 -    1620 -  412.58 equations/s - 19681.91 words/s - PRIM-BWD:  0.0547 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:48:54 - 0:07:57 -    1640 -  470.15 equations/s - 22219.68 words/s - PRIM-BWD:  0.0518 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:00 - 0:08:03 -    1660 -  439.99 equations/s - 21059.17 words/s - PRIM-BWD:  0.0528 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:06 - 0:08:09 -    1680 -  416.39 equations/s - 19441.02 words/s - PRIM-BWD:  0.0538 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:11 - 0:08:14 -    1700 -  483.27 equations/s - 22711.21 words/s - PRIM-BWD:  0.0529 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:16 - 0:08:20 -    1720 -  475.85 equations/s - 22104.58 words/s - PRIM-BWD:  0.0533 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:22 - 0:08:25 -    1740 -  472.48 equations/s - 22212.82 words/s - PRIM-BWD:  0.0512 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:27 - 0:08:31 -    1760 -  480.20 equations/s - 22548.14 words/s - PRIM-BWD:  0.0502 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:33 - 0:08:36 -    1780 -  469.60 equations/s - 22251.43 words/s - PRIM-BWD:  0.0562 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:39 - 0:08:42 -    1800 -  405.00 equations/s - 19022.01 words/s - PRIM-BWD:  0.0517 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:44 - 0:08:48 -    1820 -  475.00 equations/s - 22099.01 words/s - PRIM-BWD:  0.0527 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:50 - 0:08:53 -    1840 -  449.55 equations/s - 21369.26 words/s - PRIM-BWD:  0.0507 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:49:55 - 0:08:58 -    1860 -  537.59 equations/s - 24505.58 words/s - PRIM-BWD:  0.0513 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:01 - 0:09:04 -    1880 -  443.76 equations/s - 21184.31 words/s - PRIM-BWD:  0.0516 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:06 - 0:09:10 -    1900 -  444.66 equations/s - 20769.61 words/s - PRIM-BWD:  0.0492 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:12 - 0:09:15 -    1920 -  462.56 equations/s - 21979.56 words/s - PRIM-BWD:  0.0523 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:18 - 0:09:22 -    1940 -  400.61 equations/s - 18964.21 words/s - PRIM-BWD:  0.0520 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:23 - 0:09:27 -    1960 -  494.08 equations/s - 22942.33 words/s - PRIM-BWD:  0.0509 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:29 - 0:09:32 -    1980 -  470.64 equations/s - 22344.04 words/s - PRIM-BWD:  0.0485 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:34 - 0:09:37 -    2000 -  505.45 equations/s - 24003.57 words/s - PRIM-BWD:  0.0490 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:39 - 0:09:42 -    2020 -  499.99 equations/s - 23606.37 words/s - PRIM-BWD:  0.0511 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:45 - 0:09:48 -    2040 -  438.11 equations/s - 20295.99 words/s - PRIM-BWD:  0.0537 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:50 - 0:09:54 -    2060 -  483.76 equations/s - 22864.90 words/s - PRIM-BWD:  0.0495 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:50:55 - 0:09:59 -    2080 -  497.46 equations/s - 22881.78 words/s - PRIM-BWD:  0.0510 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:00 - 0:10:04 -    2100 -  506.96 equations/s - 23642.50 words/s - PRIM-BWD:  0.0501 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:05 - 0:10:09 -    2120 -  506.87 equations/s - 23510.65 words/s - PRIM-BWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:10 - 0:10:14 -    2140 -  530.75 equations/s - 24917.78 words/s - PRIM-BWD:  0.0516 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:16 - 0:10:19 -    2160 -  461.64 equations/s - 21844.50 words/s - PRIM-BWD:  0.0504 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:22 - 0:10:25 -    2180 -  442.13 equations/s - 20598.22 words/s - PRIM-BWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:27 - 0:10:30 -    2200 -  466.63 equations/s - 21981.85 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:33 - 0:10:36 -    2220 -  443.21 equations/s - 20974.75 words/s - PRIM-BWD:  0.0517 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:38 - 0:10:42 -    2240 -  468.49 equations/s - 22106.48 words/s - PRIM-BWD:  0.0502 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:43 - 0:10:46 -    2260 -  542.35 equations/s - 25274.70 words/s - PRIM-BWD:  0.0494 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:48 - 0:10:52 -    2280 -  475.62 equations/s - 22523.60 words/s - PRIM-BWD:  0.0511 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:51:55 - 0:10:58 -    2300 -  405.79 equations/s - 19352.87 words/s - PRIM-BWD:  0.0495 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:01 - 0:11:04 -    2320 -  429.03 equations/s - 20242.39 words/s - PRIM-BWD:  0.0518 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:06 - 0:11:09 -    2340 -  511.38 equations/s - 23796.41 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:11 - 0:11:15 -    2360 -  455.93 equations/s - 21562.87 words/s - PRIM-BWD:  0.0497 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:17 - 0:11:20 -    2380 -  447.28 equations/s - 20923.52 words/s - PRIM-BWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:23 - 0:11:26 -    2400 -  435.64 equations/s - 20443.09 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:29 - 0:11:32 -    2420 -  447.07 equations/s - 21023.50 words/s - PRIM-BWD:  0.0517 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:34 - 0:11:38 -    2440 -  447.85 equations/s - 21062.40 words/s - PRIM-BWD:  0.0499 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:40 - 0:11:44 -    2460 -  427.24 equations/s - 20487.93 words/s - PRIM-BWD:  0.0521 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:45 - 0:11:49 -    2480 -  496.60 equations/s - 23455.85 words/s - PRIM-BWD:  0.0515 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:51 - 0:11:55 -    2500 -  424.49 equations/s - 19848.84 words/s - PRIM-BWD:  0.0499 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:52:57 - 0:12:00 -    2520 -  480.93 equations/s - 22608.84 words/s - PRIM-BWD:  0.0490 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:03 - 0:12:06 -    2540 -  441.70 equations/s - 20677.03 words/s - PRIM-BWD:  0.0500 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:09 - 0:12:12 -    2560 -  401.30 equations/s - 19112.92 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:14 - 0:12:18 -    2580 -  478.71 equations/s - 22560.98 words/s - PRIM-BWD:  0.0476 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:19 - 0:12:23 -    2600 -  496.85 equations/s - 23400.90 words/s - PRIM-BWD:  0.0497 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:26 - 0:12:30 -    2620 -  375.91 equations/s - 17762.37 words/s - PRIM-BWD:  0.0500 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:31 - 0:12:35 -    2640 -  509.90 equations/s - 23796.49 words/s - PRIM-BWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:38 - 0:12:41 -    2660 -  385.62 equations/s - 18390.93 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:44 - 0:12:48 -    2680 -  409.92 equations/s - 19433.42 words/s - PRIM-BWD:  0.0521 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:50 - 0:12:53 -    2700 -  463.83 equations/s - 22119.62 words/s - PRIM-BWD:  0.0495 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:53:55 - 0:12:58 -    2720 -  481.80 equations/s - 22470.89 words/s - PRIM-BWD:  0.0508 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:00 - 0:13:04 -    2740 -  504.22 equations/s - 23650.10 words/s - PRIM-BWD:  0.0489 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:06 - 0:13:10 -    2760 -  403.16 equations/s - 19080.13 words/s - PRIM-BWD:  0.0484 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:11 - 0:13:15 -    2780 -  532.07 equations/s - 25026.84 words/s - PRIM-BWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:17 - 0:13:20 -    2800 -  456.61 equations/s - 21700.41 words/s - PRIM-BWD:  0.0479 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:22 - 0:13:25 -    2820 -  525.44 equations/s - 24406.25 words/s - PRIM-BWD:  0.0504 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:27 - 0:13:30 -    2840 -  521.41 equations/s - 24245.69 words/s - PRIM-BWD:  0.0488 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:32 - 0:13:35 -    2860 -  490.84 equations/s - 22870.93 words/s - PRIM-BWD:  0.0522 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:38 - 0:13:42 -    2880 -  399.16 equations/s - 18907.26 words/s - PRIM-BWD:  0.0498 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:44 - 0:13:48 -    2900 -  420.87 equations/s - 19834.96 words/s - PRIM-BWD:  0.0522 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:51 - 0:13:54 -    2920 -  393.41 equations/s - 18575.16 words/s - PRIM-BWD:  0.0511 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:54:57 - 0:14:00 -    2940 -  418.40 equations/s - 19551.09 words/s - PRIM-BWD:  0.0472 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:02 - 0:14:06 -    2960 -  487.79 equations/s - 22992.37 words/s - PRIM-BWD:  0.0484 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:08 - 0:14:12 -    2980 -  429.39 equations/s - 20515.76 words/s - PRIM-BWD:  0.0499 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:13 - 0:14:17 -    3000 -  491.61 equations/s - 23278.10 words/s - PRIM-BWD:  0.0463 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:18 - 0:14:22 -    3020 -  519.32 equations/s - 24163.43 words/s - PRIM-BWD:  0.0481 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:23 - 0:14:27 -    3040 -  497.62 equations/s - 23310.54 words/s - PRIM-BWD:  0.0481 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:29 - 0:14:33 -    3060 -  449.46 equations/s - 20993.52 words/s - PRIM-BWD:  0.0458 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:35 - 0:14:39 -    3080 -  430.77 equations/s - 20246.37 words/s - PRIM-BWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:41 - 0:14:44 -    3100 -  440.52 equations/s - 21069.88 words/s - PRIM-BWD:  0.0473 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:46 - 0:14:49 -    3120 -  520.08 equations/s - 23876.12 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:50 - 0:14:54 -    3140 -  591.96 equations/s - 27197.66 words/s - PRIM-BWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:55:57 - 0:15:00 -    3160 -  388.96 equations/s - 18287.26 words/s - PRIM-BWD:  0.0472 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:02 - 0:15:05 -    3180 -  520.75 equations/s - 24162.63 words/s - PRIM-BWD:  0.0523 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:07 - 0:15:11 -    3200 -  456.51 equations/s - 21692.12 words/s - PRIM-BWD:  0.0492 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:13 - 0:15:16 -    3220 -  469.38 equations/s - 22060.87 words/s - PRIM-BWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:20 - 0:15:23 -    3240 -  363.31 equations/s - 17349.17 words/s - PRIM-BWD:  0.0488 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:26 - 0:15:30 -    3260 -  403.11 equations/s - 19321.66 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:32 - 0:15:35 -    3280 -  446.16 equations/s - 20737.14 words/s - PRIM-BWD:  0.0505 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:39 - 0:15:42 -    3300 -  357.79 equations/s - 17174.09 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:45 - 0:15:48 -    3320 -  428.65 equations/s - 19987.86 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:51 - 0:15:55 -    3340 -  404.00 equations/s - 19055.25 words/s - PRIM-BWD:  0.0467 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:56:57 - 0:16:00 -    3360 -  450.62 equations/s - 21319.60 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:03 - 0:16:06 -    3380 -  435.99 equations/s - 20518.80 words/s - PRIM-BWD:  0.0472 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:08 - 0:16:12 -    3400 -  469.74 equations/s - 21879.40 words/s - PRIM-BWD:  0.0494 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:14 - 0:16:18 -    3420 -  440.27 equations/s - 20590.04 words/s - PRIM-BWD:  0.0454 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:19 - 0:16:23 -    3440 -  501.28 equations/s - 23372.75 words/s - PRIM-BWD:  0.0463 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:27 - 0:16:30 -    3460 -  350.53 equations/s - 16820.07 words/s - PRIM-BWD:  0.0499 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:33 - 0:16:37 -    3480 -  384.08 equations/s - 18320.41 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:38 - 0:16:41 -    3500 -  541.30 equations/s - 25169.21 words/s - PRIM-BWD:  0.0479 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:43 - 0:16:47 -    3520 -  482.18 equations/s - 22453.67 words/s - PRIM-BWD:  0.0497 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:48 - 0:16:52 -    3540 -  532.53 equations/s - 24227.14 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:54 - 0:16:57 -    3560 -  465.52 equations/s - 21930.66 words/s - PRIM-BWD:  0.0490 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:57:58 - 0:17:02 -    3580 -  567.37 equations/s - 26108.00 words/s - PRIM-BWD:  0.0494 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:04 - 0:17:07 -    3600 -  461.99 equations/s - 21607.08 words/s - PRIM-BWD:  0.0479 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:09 - 0:17:12 -    3620 -  482.15 equations/s - 22759.90 words/s - PRIM-BWD:  0.0450 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:14 - 0:17:17 -    3640 -  548.37 equations/s - 25404.42 words/s - PRIM-BWD:  0.0471 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:19 - 0:17:22 -    3660 -  482.75 equations/s - 22600.50 words/s - PRIM-BWD:  0.0473 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:25 - 0:17:28 -    3680 -  426.48 equations/s - 20197.95 words/s - PRIM-BWD:  0.0471 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:31 - 0:17:34 -    3700 -  429.64 equations/s - 20498.60 words/s - PRIM-BWD:  0.0470 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:37 - 0:17:41 -    3720 -  397.37 equations/s - 18970.40 words/s - PRIM-BWD:  0.0474 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:42 - 0:17:46 -    3740 -  528.02 equations/s - 24244.51 words/s - PRIM-BWD:  0.0473 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:48 - 0:17:51 -    3760 -  476.24 equations/s - 22253.20 words/s - PRIM-BWD:  0.0492 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:58:53 - 0:17:57 -    3780 -  437.71 equations/s - 20596.19 words/s - PRIM-BWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:00 - 0:18:03 -    3800 -  417.96 equations/s - 19855.61 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:05 - 0:18:08 -    3820 -  506.13 equations/s - 23761.94 words/s - PRIM-BWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:10 - 0:18:13 -    3840 -  487.30 equations/s - 22809.50 words/s - PRIM-BWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:16 - 0:18:19 -    3860 -  445.55 equations/s - 20919.59 words/s - PRIM-BWD:  0.0471 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:22 - 0:18:25 -    3880 -  404.09 equations/s - 19205.61 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:28 - 0:18:31 -    3900 -  426.31 equations/s - 20225.13 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - ============ End of epoch 0 ============\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - 0/2001\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - 128/2001\n",
            "INFO - 12/05/22 05:59:30 - 0:18:34 - 256/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 384/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 512/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 640/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 768/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 896/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 1024/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:34 - 1152/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:35 - 1280/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:35 - 1408/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:35 - 1536/2001\n",
            "INFO - 12/05/22 05:59:31 - 0:18:35 - 1664/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 1792/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 1920/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 1409/2001 (70.41479260369815%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 0/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 128/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:35 - 256/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 384/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 512/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 640/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 768/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 896/2001\n",
            "INFO - 12/05/22 05:59:32 - 0:18:36 - 1024/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:36 - 1152/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:36 - 1280/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:36 - 1408/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:36 - 1536/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:36 - 1664/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - 1792/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - 1920/2001\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - 1406/2001 (70.26486756621689%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - epoch -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_xe_loss -> 1.182626\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc -> 70.414793\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_4 -> 76.744186\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_5 -> 70.422535\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_6 -> 72.727273\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_7 -> 72.847682\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_8 -> 69.957082\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_9 -> 69.411765\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_10 -> 63.975155\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_11 -> 71.755725\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_12 -> 65.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_13 -> 78.947368\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_14 -> 68.867925\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_15 -> 70.833333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_16 -> 71.621622\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_17 -> 62.790698\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_18 -> 78.260870\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_19 -> 65.909091\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_20 -> 62.500000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_21 -> 82.608696\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_22 -> 71.428571\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_23 -> 81.818182\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_24 -> 65.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_25 -> 77.777778\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_26 -> 73.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_27 -> 76.923077\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_28 -> 69.230769\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_29 -> 72.727273\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_30 -> 44.444444\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_32 -> 83.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_37 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_39 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_44 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_56 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_68 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_97 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_xe_loss -> 1.286207\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc -> 70.264868\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_3 -> 55.555556\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_4 -> 66.666667\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_5 -> 70.175439\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_6 -> 76.296296\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_8 -> 71.759259\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_9 -> 71.428571\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_10 -> 71.951220\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_11 -> 60.431655\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_12 -> 63.503650\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_13 -> 74.038462\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_14 -> 69.879518\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_15 -> 67.187500\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_16 -> 64.285714\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_17 -> 77.272727\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_18 -> 69.047619\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_19 -> 75.609756\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_20 -> 82.978723\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_21 -> 59.259259\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_22 -> 75.862069\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_23 -> 85.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_24 -> 52.380952\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_25 -> 75.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_27 -> 85.714286\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_28 -> 88.888889\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_29 -> 70.588235\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_32 -> 66.666667\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_38 -> 60.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_55 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_81 -> 50.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_127 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - test_prim_bwd_acc_144 -> 100.000000\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - __log__:{\"epoch\": 0, \"valid_prim_bwd_xe_loss\": 1.1826257129003857, \"valid_prim_bwd_acc\": 70.41479260369815, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 80.0, \"valid_prim_bwd_acc_4\": 76.74418604651163, \"valid_prim_bwd_acc_5\": 70.4225352112676, \"valid_prim_bwd_acc_6\": 72.72727272727273, \"valid_prim_bwd_acc_7\": 72.8476821192053, \"valid_prim_bwd_acc_8\": 69.95708154506438, \"valid_prim_bwd_acc_9\": 69.41176470588235, \"valid_prim_bwd_acc_10\": 63.975155279503106, \"valid_prim_bwd_acc_11\": 71.7557251908397, \"valid_prim_bwd_acc_12\": 65.33333333333333, \"valid_prim_bwd_acc_13\": 78.94736842105263, \"valid_prim_bwd_acc_14\": 68.86792452830188, \"valid_prim_bwd_acc_15\": 70.83333333333333, \"valid_prim_bwd_acc_16\": 71.62162162162163, \"valid_prim_bwd_acc_17\": 62.7906976744186, \"valid_prim_bwd_acc_18\": 78.26086956521739, \"valid_prim_bwd_acc_19\": 65.9090909090909, \"valid_prim_bwd_acc_20\": 62.5, \"valid_prim_bwd_acc_21\": 82.6086956521739, \"valid_prim_bwd_acc_22\": 71.42857142857143, \"valid_prim_bwd_acc_23\": 81.81818181818181, \"valid_prim_bwd_acc_24\": 65.0, \"valid_prim_bwd_acc_25\": 77.77777777777777, \"valid_prim_bwd_acc_26\": 73.33333333333333, \"valid_prim_bwd_acc_27\": 76.92307692307692, \"valid_prim_bwd_acc_28\": 69.23076923076923, \"valid_prim_bwd_acc_29\": 72.72727272727273, \"valid_prim_bwd_acc_30\": 44.44444444444444, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 83.33333333333333, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 50.0, \"valid_prim_bwd_acc_39\": 100.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 50.0, \"valid_prim_bwd_acc_44\": 100.0, \"valid_prim_bwd_acc_45\": 100.0, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 100.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 100.0, \"valid_prim_bwd_acc_71\": 100.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 100.0, \"valid_prim_bwd_acc_101\": 0.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 1.2862067653917182, \"test_prim_bwd_acc\": 70.26486756621689, \"test_prim_bwd_acc_3\": 55.55555555555556, \"test_prim_bwd_acc_4\": 66.66666666666667, \"test_prim_bwd_acc_5\": 70.17543859649123, \"test_prim_bwd_acc_6\": 76.29629629629629, \"test_prim_bwd_acc_7\": 71.73913043478261, \"test_prim_bwd_acc_8\": 71.75925925925925, \"test_prim_bwd_acc_9\": 71.42857142857143, \"test_prim_bwd_acc_10\": 71.95121951219512, \"test_prim_bwd_acc_11\": 60.431654676258994, \"test_prim_bwd_acc_12\": 63.503649635036496, \"test_prim_bwd_acc_13\": 74.03846153846153, \"test_prim_bwd_acc_14\": 69.87951807228916, \"test_prim_bwd_acc_15\": 67.1875, \"test_prim_bwd_acc_16\": 64.28571428571429, \"test_prim_bwd_acc_17\": 77.27272727272727, \"test_prim_bwd_acc_18\": 69.04761904761905, \"test_prim_bwd_acc_19\": 75.60975609756098, \"test_prim_bwd_acc_20\": 82.97872340425532, \"test_prim_bwd_acc_21\": 59.25925925925926, \"test_prim_bwd_acc_22\": 75.86206896551724, \"test_prim_bwd_acc_23\": 85.0, \"test_prim_bwd_acc_24\": 52.38095238095238, \"test_prim_bwd_acc_25\": 75.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 85.71428571428571, \"test_prim_bwd_acc_28\": 88.88888888888889, \"test_prim_bwd_acc_29\": 70.58823529411765, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 66.66666666666667, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 60.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 100.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 0.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 0.0, \"test_prim_bwd_acc_53\": 100.0, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 100.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 100.0, \"test_prim_bwd_acc_65\": 100.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 0.0, \"test_prim_bwd_acc_81\": 50.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 0.0, \"test_prim_bwd_acc_127\": 100.0, \"test_prim_bwd_acc_144\": 100.0}\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - New best score for valid_prim_bwd_acc: 70.414793\n",
            "INFO - 12/05/22 05:59:33 - 0:18:37 - Saving best-valid_prim_bwd_acc to ./dumped/deriv2fwd_modelbwd/6gswznivsf/best-valid_prim_bwd_acc.pth ...\n",
            "WARNING - 12/05/22 05:59:33 - 0:18:37 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:59:33 - 0:18:37 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:59:33 - 0:18:37 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:59:38 - 0:18:41 - Saving checkpoint to ./dumped/deriv2fwd_modelbwd/6gswznivsf/checkpoint.pth ...\n",
            "WARNING - 12/05/22 05:59:38 - 0:18:41 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 05:59:38 - 0:18:41 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 05:59:38 - 0:18:41 - Saving model optimizer ...\n",
            "INFO - 12/05/22 05:59:44 - 0:18:47 - ============ Starting epoch 1 ... ============\n",
            "INFO - 12/05/22 05:59:47 - 0:18:51 -    3920 -  133.64 equations/s -  6202.92 words/s - PRIM-BWD:  0.0487 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:52 - 0:18:56 -    3940 -  508.09 equations/s - 23884.86 words/s - PRIM-BWD:  0.0491 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 05:59:58 - 0:19:02 -    3960 -  426.15 equations/s - 20134.62 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:03 - 0:19:07 -    3980 -  487.45 equations/s - 22995.02 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:08 - 0:19:12 -    4000 -  529.03 equations/s - 24290.56 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:14 - 0:19:18 -    4020 -  410.04 equations/s - 19297.85 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:20 - 0:19:23 -    4040 -  490.90 equations/s - 22915.00 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:25 - 0:19:28 -    4060 -  504.18 equations/s - 23600.67 words/s - PRIM-BWD:  0.0457 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:30 - 0:19:34 -    4080 -  468.94 equations/s - 22013.76 words/s - PRIM-BWD:  0.0458 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:36 - 0:19:39 -    4100 -  463.38 equations/s - 22118.81 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:41 - 0:19:44 -    4120 -  504.54 equations/s - 23348.29 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:46 - 0:19:49 -    4140 -  527.03 equations/s - 24747.40 words/s - PRIM-BWD:  0.0477 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:52 - 0:19:56 -    4160 -  395.14 equations/s - 18550.97 words/s - PRIM-BWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:00:58 - 0:20:01 -    4180 -  439.28 equations/s - 20413.25 words/s - PRIM-BWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:04 - 0:20:07 -    4200 -  447.36 equations/s - 21213.88 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:09 - 0:20:13 -    4220 -  442.61 equations/s - 20892.76 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:14 - 0:20:18 -    4240 -  523.73 equations/s - 24558.78 words/s - PRIM-BWD:  0.0460 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:20 - 0:20:24 -    4260 -  425.24 equations/s - 20026.67 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:25 - 0:20:28 -    4280 -  595.27 equations/s - 26963.31 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:30 - 0:20:33 -    4300 -  505.80 equations/s - 23849.43 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:36 - 0:20:39 -    4320 -  426.85 equations/s - 20313.94 words/s - PRIM-BWD:  0.0479 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:40 - 0:20:44 -    4340 -  545.09 equations/s - 25080.75 words/s - PRIM-BWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:46 - 0:20:49 -    4360 -  464.62 equations/s - 21609.42 words/s - PRIM-BWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:52 - 0:20:55 -    4380 -  442.64 equations/s - 20600.26 words/s - PRIM-BWD:  0.0476 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:01:57 - 0:21:01 -    4400 -  460.30 equations/s - 21883.16 words/s - PRIM-BWD:  0.0499 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:02 - 0:21:06 -    4420 -  528.20 equations/s - 24819.71 words/s - PRIM-BWD:  0.0477 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:07 - 0:21:11 -    4440 -  507.04 equations/s - 23643.60 words/s - PRIM-BWD:  0.0447 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:13 - 0:21:17 -    4460 -  423.66 equations/s - 20192.60 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:18 - 0:21:21 -    4480 -  553.18 equations/s - 25416.81 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:24 - 0:21:27 -    4500 -  445.47 equations/s - 21058.35 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:29 - 0:21:32 -    4520 -  510.29 equations/s - 24113.07 words/s - PRIM-BWD:  0.0456 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:35 - 0:21:39 -    4540 -  390.05 equations/s - 18472.85 words/s - PRIM-BWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:40 - 0:21:44 -    4560 -  486.26 equations/s - 22965.31 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:48 - 0:21:52 -    4580 -  331.25 equations/s - 16021.89 words/s - PRIM-BWD:  0.0457 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:54 - 0:21:58 -    4600 -  435.67 equations/s - 20580.88 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:02:59 - 0:22:03 -    4620 -  501.53 equations/s - 23493.73 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:04 - 0:22:07 -    4640 -  545.95 equations/s - 25478.85 words/s - PRIM-BWD:  0.0467 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:09 - 0:22:13 -    4660 -  493.14 equations/s - 23315.50 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:15 - 0:22:19 -    4680 -  426.19 equations/s - 20096.26 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:20 - 0:22:24 -    4700 -  469.59 equations/s - 21886.48 words/s - PRIM-BWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:26 - 0:22:30 -    4720 -  449.73 equations/s - 20943.76 words/s - PRIM-BWD:  0.0461 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:33 - 0:22:36 -    4740 -  389.87 equations/s - 18524.14 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:38 - 0:22:41 -    4760 -  487.82 equations/s - 22753.60 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:43 - 0:22:47 -    4780 -  489.60 equations/s - 22880.52 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:49 - 0:22:53 -    4800 -  424.28 equations/s - 20059.98 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:03:55 - 0:22:58 -    4820 -  455.18 equations/s - 21071.24 words/s - PRIM-BWD:  0.0454 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:00 - 0:23:04 -    4840 -  477.20 equations/s - 22020.01 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:05 - 0:23:09 -    4860 -  501.30 equations/s - 23334.11 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:12 - 0:23:15 -    4880 -  387.95 equations/s - 18208.88 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:17 - 0:23:21 -    4900 -  464.92 equations/s - 21814.62 words/s - PRIM-BWD:  0.0464 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:23 - 0:23:27 -    4920 -  435.19 equations/s - 20516.61 words/s - PRIM-BWD:  0.0473 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:29 - 0:23:33 -    4940 -  421.58 equations/s - 19660.66 words/s - PRIM-BWD:  0.0447 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:35 - 0:23:38 -    4960 -  481.93 equations/s - 22445.55 words/s - PRIM-BWD:  0.0464 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:40 - 0:23:44 -    4980 -  449.20 equations/s - 20962.97 words/s - PRIM-BWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:45 - 0:23:49 -    5000 -  517.18 equations/s - 23878.43 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:52 - 0:23:55 -    5020 -  414.74 equations/s - 19848.22 words/s - PRIM-BWD:  0.0468 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:04:57 - 0:24:00 -    5040 -  499.96 equations/s - 23401.08 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:02 - 0:24:05 -    5060 -  492.24 equations/s - 23018.57 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:08 - 0:24:11 -    5080 -  426.48 equations/s - 20419.08 words/s - PRIM-BWD:  0.0470 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:13 - 0:24:16 -    5100 -  523.30 equations/s - 24567.19 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:18 - 0:24:21 -    5120 -  498.50 equations/s - 23255.94 words/s - PRIM-BWD:  0.0460 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:24 - 0:24:27 -    5140 -  446.74 equations/s - 20996.40 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:29 - 0:24:33 -    5160 -  455.56 equations/s - 21408.29 words/s - PRIM-BWD:  0.0448 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:35 - 0:24:39 -    5180 -  428.01 equations/s - 20207.74 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:41 - 0:24:44 -    5200 -  466.40 equations/s - 21879.77 words/s - PRIM-BWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:47 - 0:24:51 -    5220 -  386.77 equations/s - 18514.92 words/s - PRIM-BWD:  0.0483 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:53 - 0:24:57 -    5240 -  427.59 equations/s - 20185.49 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:05:59 - 0:25:02 -    5260 -  460.90 equations/s - 21613.11 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:04 - 0:25:08 -    5280 -  463.12 equations/s - 21856.55 words/s - PRIM-BWD:  0.0470 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:11 - 0:25:14 -    5300 -  396.58 equations/s - 18772.64 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:17 - 0:25:21 -    5320 -  406.82 equations/s - 19299.14 words/s - PRIM-BWD:  0.0463 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:22 - 0:25:26 -    5340 -  478.72 equations/s - 22785.73 words/s - PRIM-BWD:  0.0482 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:28 - 0:25:32 -    5360 -  442.73 equations/s - 20954.75 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:33 - 0:25:37 -    5380 -  534.38 equations/s - 24905.94 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:39 - 0:25:43 -    5400 -  405.00 equations/s - 19213.12 words/s - PRIM-BWD:  0.0472 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:46 - 0:25:49 -    5420 -  405.30 equations/s - 19023.44 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:52 - 0:25:55 -    5440 -  415.68 equations/s - 19543.93 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:06:57 - 0:26:00 -    5460 -  547.65 equations/s - 25469.94 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:02 - 0:26:05 -    5480 -  477.31 equations/s - 22294.30 words/s - PRIM-BWD:  0.0463 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:07 - 0:26:11 -    5500 -  477.02 equations/s - 22536.17 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:12 - 0:26:16 -    5520 -  509.29 equations/s - 23503.12 words/s - PRIM-BWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:17 - 0:26:21 -    5540 -  494.30 equations/s - 23272.41 words/s - PRIM-BWD:  0.0448 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:23 - 0:26:26 -    5560 -  475.08 equations/s - 22230.62 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:28 - 0:26:32 -    5580 -  456.36 equations/s - 21349.40 words/s - PRIM-BWD:  0.0471 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:34 - 0:26:37 -    5600 -  492.95 equations/s - 23075.49 words/s - PRIM-BWD:  0.0450 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:40 - 0:26:43 -    5620 -  435.35 equations/s - 20880.14 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:45 - 0:26:49 -    5640 -  432.11 equations/s - 20209.15 words/s - PRIM-BWD:  0.0442 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:50 - 0:26:54 -    5660 -  547.80 equations/s - 25185.53 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:07:56 - 0:26:59 -    5680 -  439.17 equations/s - 20772.93 words/s - PRIM-BWD:  0.0457 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:01 - 0:27:05 -    5700 -  475.04 equations/s - 22417.40 words/s - PRIM-BWD:  0.0450 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:07 - 0:27:11 -    5720 -  438.18 equations/s - 20521.79 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:13 - 0:27:16 -    5740 -  448.78 equations/s - 20902.42 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:18 - 0:27:22 -    5760 -  460.45 equations/s - 21224.99 words/s - PRIM-BWD:  0.0447 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:24 - 0:27:27 -    5780 -  468.62 equations/s - 21984.14 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:30 - 0:27:33 -    5800 -  453.47 equations/s - 21252.23 words/s - PRIM-BWD:  0.0458 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:35 - 0:27:39 -    5820 -  463.41 equations/s - 22075.23 words/s - PRIM-BWD:  0.0460 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:41 - 0:27:45 -    5840 -  422.48 equations/s - 19799.32 words/s - PRIM-BWD:  0.0492 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:47 - 0:27:50 -    5860 -  466.52 equations/s - 21856.64 words/s - PRIM-BWD:  0.0448 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:52 - 0:27:56 -    5880 -  466.01 equations/s - 21815.85 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:08:58 - 0:28:01 -    5900 -  473.64 equations/s - 22087.87 words/s - PRIM-BWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:03 - 0:28:07 -    5920 -  453.62 equations/s - 21045.71 words/s - PRIM-BWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:09 - 0:28:12 -    5940 -  475.41 equations/s - 22483.78 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:14 - 0:28:18 -    5960 -  463.79 equations/s - 21834.59 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:20 - 0:28:23 -    5980 -  456.74 equations/s - 21098.52 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:25 - 0:28:28 -    6000 -  506.75 equations/s - 23539.36 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 06:09:30 - 0:28:33 -    6020 -  525.43 equations/s - 24117.69 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:35 - 0:28:39 -    6040 -  469.53 equations/s - 22133.86 words/s - PRIM-BWD:  0.0491 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:40 - 0:28:43 -    6060 -  518.53 equations/s - 24022.93 words/s - PRIM-BWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:47 - 0:28:50 -    6080 -  383.88 equations/s - 18375.00 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:52 - 0:28:55 -    6100 -  498.71 equations/s - 23414.69 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:09:58 - 0:29:01 -    6120 -  448.60 equations/s - 21422.00 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:03 - 0:29:07 -    6140 -  463.53 equations/s - 22104.98 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:08 - 0:29:11 -    6160 -  515.75 equations/s - 24272.30 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:14 - 0:29:17 -    6180 -  432.23 equations/s - 20424.97 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:20 - 0:29:23 -    6200 -  432.04 equations/s - 20073.20 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:25 - 0:29:29 -    6220 -  477.39 equations/s - 22547.74 words/s - PRIM-BWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:30 - 0:29:34 -    6240 -  524.83 equations/s - 24635.41 words/s - PRIM-BWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:36 - 0:29:39 -    6260 -  433.85 equations/s - 20387.91 words/s - PRIM-BWD:  0.0434 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:42 - 0:29:46 -    6280 -  409.95 equations/s - 19415.49 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:48 - 0:29:51 -    6300 -  471.51 equations/s - 22090.09 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:54 - 0:29:57 -    6320 -  417.70 equations/s - 19765.57 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:10:59 - 0:30:03 -    6340 -  482.64 equations/s - 22398.74 words/s - PRIM-BWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:05 - 0:30:08 -    6360 -  460.02 equations/s - 21946.51 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:10 - 0:30:13 -    6380 -  506.09 equations/s - 23317.32 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:15 - 0:30:18 -    6400 -  525.85 equations/s - 24649.09 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:20 - 0:30:23 -    6420 -  505.60 equations/s - 23859.87 words/s - PRIM-BWD:  0.0442 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:26 - 0:30:30 -    6440 -  399.42 equations/s - 18777.12 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:31 - 0:30:35 -    6460 -  471.92 equations/s - 22094.79 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:38 - 0:30:41 -    6480 -  414.41 equations/s - 19339.33 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:43 - 0:30:46 -    6500 -  486.12 equations/s - 22858.93 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:48 - 0:30:51 -    6520 -  557.16 equations/s - 25979.56 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:53 - 0:30:56 -    6540 -  476.63 equations/s - 22413.74 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:11:59 - 0:31:02 -    6560 -  436.49 equations/s - 20862.66 words/s - PRIM-BWD:  0.0461 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:04 - 0:31:07 -    6580 -  510.33 equations/s - 23743.30 words/s - PRIM-BWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:09 - 0:31:12 -    6600 -  502.24 equations/s - 23135.63 words/s - PRIM-BWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:14 - 0:31:18 -    6620 -  458.30 equations/s - 21395.18 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:21 - 0:31:25 -    6640 -  388.04 equations/s - 18620.08 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:27 - 0:31:30 -    6660 -  467.90 equations/s - 22022.24 words/s - PRIM-BWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:32 - 0:31:35 -    6680 -  476.34 equations/s - 22412.68 words/s - PRIM-BWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:37 - 0:31:41 -    6700 -  459.97 equations/s - 21662.69 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:43 - 0:31:47 -    6720 -  447.93 equations/s - 21227.52 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:49 - 0:31:52 -    6740 -  467.88 equations/s - 21850.41 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:12:55 - 0:31:58 -    6760 -  423.73 equations/s - 20034.70 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:01 - 0:32:05 -    6780 -  389.12 equations/s - 18460.14 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:07 - 0:32:10 -    6800 -  455.53 equations/s - 21440.74 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:13 - 0:32:16 -    6820 -  450.88 equations/s - 21129.69 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:17 - 0:32:21 -    6840 -  522.63 equations/s - 24435.02 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:23 - 0:32:27 -    6860 -  440.22 equations/s - 20814.90 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:29 - 0:32:32 -    6880 -  486.77 equations/s - 22680.68 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:34 - 0:32:37 -    6900 -  517.32 equations/s - 24396.57 words/s - PRIM-BWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:39 - 0:32:42 -    6920 -  501.37 equations/s - 23322.66 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:45 - 0:32:49 -    6940 -  377.79 equations/s - 17892.60 words/s - PRIM-BWD:  0.0461 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:51 - 0:32:55 -    6960 -  419.74 equations/s - 20076.99 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:13:57 - 0:33:01 -    6980 -  433.41 equations/s - 20491.95 words/s - PRIM-BWD:  0.0450 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:03 - 0:33:07 -    7000 -  445.61 equations/s - 20953.12 words/s - PRIM-BWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:09 - 0:33:12 -    7020 -  475.65 equations/s - 22132.58 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:14 - 0:33:17 -    7040 -  513.27 equations/s - 23698.93 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:19 - 0:33:23 -    7060 -  449.40 equations/s - 21394.43 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:25 - 0:33:29 -    7080 -  412.99 equations/s - 19309.22 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:31 - 0:33:34 -    7100 -  461.13 equations/s - 21721.01 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:37 - 0:33:40 -    7120 -  456.11 equations/s - 21430.91 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:42 - 0:33:45 -    7140 -  501.22 equations/s - 23630.46 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:48 - 0:33:52 -    7160 -  380.67 equations/s - 18149.33 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:14:54 - 0:33:57 -    7180 -  471.71 equations/s - 22118.06 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:00 - 0:34:04 -    7200 -  393.73 equations/s - 18892.53 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:06 - 0:34:09 -    7220 -  469.94 equations/s - 22244.00 words/s - PRIM-BWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:12 - 0:34:15 -    7240 -  417.21 equations/s - 19994.25 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:18 - 0:34:21 -    7260 -  454.73 equations/s - 21522.42 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:23 - 0:34:26 -    7280 -  496.17 equations/s - 23018.05 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:29 - 0:34:32 -    7300 -  407.36 equations/s - 19230.31 words/s - PRIM-BWD:  0.0434 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:35 - 0:34:39 -    7320 -  421.40 equations/s - 19838.87 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:42 - 0:34:45 -    7340 -  396.87 equations/s - 18950.07 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:48 - 0:34:51 -    7360 -  414.94 equations/s - 19584.41 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:53 - 0:34:57 -    7380 -  469.25 equations/s - 22122.44 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:15:59 - 0:35:03 -    7400 -  425.49 equations/s - 19917.63 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:04 - 0:35:07 -    7420 -  584.84 equations/s - 27228.23 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:09 - 0:35:12 -    7440 -  514.01 equations/s - 24112.70 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:15 - 0:35:18 -    7460 -  425.35 equations/s - 20059.27 words/s - PRIM-BWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:21 - 0:35:24 -    7480 -  408.17 equations/s - 19035.87 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:27 - 0:35:30 -    7500 -  414.43 equations/s - 19722.04 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:32 - 0:35:36 -    7520 -  477.85 equations/s - 22452.59 words/s - PRIM-BWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:38 - 0:35:42 -    7540 -  441.64 equations/s - 20697.34 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:44 - 0:35:48 -    7560 -  410.90 equations/s - 19444.49 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:50 - 0:35:53 -    7580 -  472.86 equations/s - 22202.03 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:16:54 - 0:35:58 -    7600 -  553.75 equations/s - 25595.18 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:00 - 0:36:03 -    7620 -  479.10 equations/s - 22237.82 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:05 - 0:36:09 -    7640 -  467.12 equations/s - 21830.01 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:10 - 0:36:14 -    7660 -  502.19 equations/s - 23367.54 words/s - PRIM-BWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:17 - 0:36:21 -    7680 -  375.17 equations/s - 17773.20 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:22 - 0:36:26 -    7700 -  487.33 equations/s - 22611.21 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:29 - 0:36:32 -    7720 -  403.33 equations/s - 19041.66 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:34 - 0:36:38 -    7740 -  481.90 equations/s - 22779.97 words/s - PRIM-BWD:  0.0463 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:40 - 0:36:43 -    7760 -  455.04 equations/s - 21455.48 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:44 - 0:36:48 -    7780 -  554.96 equations/s - 25704.98 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:50 - 0:36:53 -    7800 -  463.98 equations/s - 21539.03 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - ============ End of epoch 1 ============\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - 0/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:57 - 128/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:58 - 256/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:58 - 384/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:58 - 512/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:58 - 640/2001\n",
            "INFO - 12/05/22 06:17:54 - 0:36:58 - 768/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 896/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 1024/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 1152/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 1280/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 1408/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:58 - 1536/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - 1664/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - 1792/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - 1920/2001\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - 1417/2001 (70.81459270364817%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 06:17:55 - 0:36:59 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 0/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 128/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 256/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 384/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 512/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 640/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:36:59 - 768/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:37:00 - 896/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:37:00 - 1024/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:37:00 - 1152/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:37:00 - 1280/2001\n",
            "INFO - 12/05/22 06:17:56 - 0:37:00 - 1408/2001\n",
            "INFO - 12/05/22 06:17:57 - 0:37:00 - 1536/2001\n",
            "INFO - 12/05/22 06:17:57 - 0:37:00 - 1664/2001\n",
            "INFO - 12/05/22 06:17:57 - 0:37:00 - 1792/2001\n",
            "INFO - 12/05/22 06:17:57 - 0:37:00 - 1920/2001\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - 1419/2001 (70.91454272863568%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - epoch -> 1.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_xe_loss -> 1.140131\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc -> 70.814593\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_4 -> 79.069767\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_5 -> 64.788732\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_6 -> 73.484848\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_7 -> 75.496689\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_8 -> 69.957082\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_9 -> 71.176471\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_10 -> 63.975155\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_11 -> 74.045802\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_12 -> 67.333333\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_13 -> 73.684211\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_14 -> 69.811321\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_15 -> 73.611111\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_16 -> 67.567568\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_17 -> 72.093023\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_18 -> 71.739130\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_19 -> 68.181818\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_20 -> 75.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_21 -> 78.260870\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_22 -> 76.190476\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_23 -> 81.818182\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_24 -> 65.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_25 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_26 -> 80.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_27 -> 76.923077\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_28 -> 76.923077\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_29 -> 81.818182\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_32 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_34 -> 75.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_35 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_37 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_58 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_62 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_101 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_xe_loss -> 1.188669\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc -> 70.914543\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_4 -> 68.888889\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_5 -> 71.929825\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_6 -> 74.814815\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_8 -> 71.759259\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_9 -> 68.322981\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_10 -> 70.731707\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_11 -> 64.028777\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_12 -> 64.963504\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_13 -> 75.961538\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_14 -> 71.084337\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_15 -> 76.562500\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_16 -> 64.285714\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_17 -> 72.727273\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_18 -> 73.809524\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_19 -> 73.170732\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_20 -> 76.595745\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_21 -> 74.074074\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_22 -> 75.862069\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_23 -> 75.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_24 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_25 -> 75.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_26 -> 92.857143\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_27 -> 85.714286\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_28 -> 77.777778\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_29 -> 76.470588\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_30 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_32 -> 83.333333\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_34 -> 80.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_39 -> 33.333333\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_46 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_78 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_82 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_127 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - test_prim_bwd_acc_144 -> 100.000000\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - __log__:{\"epoch\": 1, \"valid_prim_bwd_xe_loss\": 1.1401313198381278, \"valid_prim_bwd_acc\": 70.81459270364817, \"valid_prim_bwd_acc_2\": 50.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 79.06976744186046, \"valid_prim_bwd_acc_5\": 64.78873239436619, \"valid_prim_bwd_acc_6\": 73.48484848484848, \"valid_prim_bwd_acc_7\": 75.49668874172185, \"valid_prim_bwd_acc_8\": 69.95708154506438, \"valid_prim_bwd_acc_9\": 71.17647058823529, \"valid_prim_bwd_acc_10\": 63.975155279503106, \"valid_prim_bwd_acc_11\": 74.04580152671755, \"valid_prim_bwd_acc_12\": 67.33333333333333, \"valid_prim_bwd_acc_13\": 73.6842105263158, \"valid_prim_bwd_acc_14\": 69.81132075471699, \"valid_prim_bwd_acc_15\": 73.61111111111111, \"valid_prim_bwd_acc_16\": 67.56756756756756, \"valid_prim_bwd_acc_17\": 72.09302325581395, \"valid_prim_bwd_acc_18\": 71.73913043478261, \"valid_prim_bwd_acc_19\": 68.18181818181819, \"valid_prim_bwd_acc_20\": 75.0, \"valid_prim_bwd_acc_21\": 78.26086956521739, \"valid_prim_bwd_acc_22\": 76.19047619047619, \"valid_prim_bwd_acc_23\": 81.81818181818181, \"valid_prim_bwd_acc_24\": 65.0, \"valid_prim_bwd_acc_25\": 66.66666666666667, \"valid_prim_bwd_acc_26\": 80.0, \"valid_prim_bwd_acc_27\": 76.92307692307692, \"valid_prim_bwd_acc_28\": 76.92307692307692, \"valid_prim_bwd_acc_29\": 81.81818181818181, \"valid_prim_bwd_acc_30\": 55.55555555555556, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 66.66666666666667, \"valid_prim_bwd_acc_33\": 100.0, \"valid_prim_bwd_acc_34\": 75.0, \"valid_prim_bwd_acc_35\": 100.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 50.0, \"valid_prim_bwd_acc_39\": 50.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 50.0, \"valid_prim_bwd_acc_44\": 0.0, \"valid_prim_bwd_acc_45\": 66.66666666666667, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 100.0, \"valid_prim_bwd_acc_61\": 0.0, \"valid_prim_bwd_acc_62\": 100.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 100.0, \"valid_prim_bwd_acc_103\": 0.0, \"valid_prim_bwd_acc_104\": 0.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 1.1886692914529065, \"test_prim_bwd_acc\": 70.91454272863568, \"test_prim_bwd_acc_3\": 66.66666666666667, \"test_prim_bwd_acc_4\": 68.88888888888889, \"test_prim_bwd_acc_5\": 71.9298245614035, \"test_prim_bwd_acc_6\": 74.81481481481481, \"test_prim_bwd_acc_7\": 71.73913043478261, \"test_prim_bwd_acc_8\": 71.75925925925925, \"test_prim_bwd_acc_9\": 68.32298136645963, \"test_prim_bwd_acc_10\": 70.73170731707317, \"test_prim_bwd_acc_11\": 64.02877697841727, \"test_prim_bwd_acc_12\": 64.96350364963503, \"test_prim_bwd_acc_13\": 75.96153846153847, \"test_prim_bwd_acc_14\": 71.08433734939759, \"test_prim_bwd_acc_15\": 76.5625, \"test_prim_bwd_acc_16\": 64.28571428571429, \"test_prim_bwd_acc_17\": 72.72727272727273, \"test_prim_bwd_acc_18\": 73.80952380952381, \"test_prim_bwd_acc_19\": 73.17073170731707, \"test_prim_bwd_acc_20\": 76.59574468085107, \"test_prim_bwd_acc_21\": 74.07407407407408, \"test_prim_bwd_acc_22\": 75.86206896551724, \"test_prim_bwd_acc_23\": 75.0, \"test_prim_bwd_acc_24\": 66.66666666666667, \"test_prim_bwd_acc_25\": 75.0, \"test_prim_bwd_acc_26\": 92.85714285714286, \"test_prim_bwd_acc_27\": 85.71428571428571, \"test_prim_bwd_acc_28\": 77.77777777777777, \"test_prim_bwd_acc_29\": 76.47058823529412, \"test_prim_bwd_acc_30\": 100.0, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 83.33333333333333, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 80.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 33.333333333333336, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 66.66666666666667, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 100.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 100.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 100.0, \"test_prim_bwd_acc_144\": 100.0}\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - New best score for valid_prim_bwd_acc: 70.814593\n",
            "INFO - 12/05/22 06:17:57 - 0:37:01 - Saving best-valid_prim_bwd_acc to ./dumped/deriv2fwd_modelbwd/6gswznivsf/best-valid_prim_bwd_acc.pth ...\n",
            "WARNING - 12/05/22 06:17:57 - 0:37:01 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:17:57 - 0:37:01 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:17:57 - 0:37:01 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:18:02 - 0:37:06 - Saving checkpoint to ./dumped/deriv2fwd_modelbwd/6gswznivsf/checkpoint.pth ...\n",
            "WARNING - 12/05/22 06:18:02 - 0:37:06 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:18:03 - 0:37:06 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:18:03 - 0:37:06 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:18:11 - 0:37:15 - ============ Starting epoch 2 ... ============\n",
            "INFO - 12/05/22 06:18:13 - 0:37:16 -    7820 -  112.68 equations/s -  5263.43 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:18 - 0:37:21 -    7840 -  490.66 equations/s - 22764.33 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:23 - 0:37:27 -    7860 -  473.15 equations/s - 22224.19 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:30 - 0:37:33 -    7880 -  383.58 equations/s - 18076.17 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:35 - 0:37:39 -    7900 -  462.68 equations/s - 21543.01 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:41 - 0:37:44 -    7920 -  466.20 equations/s - 21888.89 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:46 - 0:37:49 -    7940 -  544.54 equations/s - 25449.57 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:51 - 0:37:54 -    7960 -  507.48 equations/s - 23649.08 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:18:56 - 0:37:59 -    7980 -  510.94 equations/s - 23717.58 words/s - PRIM-BWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:01 - 0:38:04 -    8000 -  499.77 equations/s - 23528.72 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 06:19:06 - 0:38:10 -    8020 -  475.59 equations/s - 22395.86 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:11 - 0:38:15 -    8040 -  479.45 equations/s - 22473.14 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:17 - 0:38:21 -    8060 -  443.35 equations/s - 20808.32 words/s - PRIM-BWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:23 - 0:38:27 -    8080 -  431.36 equations/s - 20297.07 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:28 - 0:38:32 -    8100 -  514.28 equations/s - 24291.49 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:34 - 0:38:37 -    8120 -  468.69 equations/s - 22276.10 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:40 - 0:38:43 -    8140 -  423.68 equations/s - 20041.97 words/s - PRIM-BWD:  0.0381 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:45 - 0:38:48 -    8160 -  504.44 equations/s - 23577.26 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:50 - 0:38:54 -    8180 -  443.84 equations/s - 20546.32 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:19:56 - 0:38:59 -    8200 -  492.02 equations/s - 23103.98 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:00 - 0:39:04 -    8220 -  551.07 equations/s - 25469.96 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:06 - 0:39:10 -    8240 -  441.60 equations/s - 20581.93 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:11 - 0:39:15 -    8260 -  505.73 equations/s - 24201.99 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:17 - 0:39:21 -    8280 -  409.50 equations/s - 19307.96 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:24 - 0:39:28 -    8300 -  386.47 equations/s - 18444.66 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:30 - 0:39:33 -    8320 -  455.06 equations/s - 21524.88 words/s - PRIM-BWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:35 - 0:39:38 -    8340 -  501.20 equations/s - 23198.14 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:41 - 0:39:44 -    8360 -  432.02 equations/s - 20084.65 words/s - PRIM-BWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:46 - 0:39:50 -    8380 -  457.72 equations/s - 21674.36 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:52 - 0:39:55 -    8400 -  462.38 equations/s - 21438.14 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:20:57 - 0:40:01 -    8420 -  454.15 equations/s - 21374.31 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:04 - 0:40:07 -    8440 -  415.03 equations/s - 19631.24 words/s - PRIM-BWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:09 - 0:40:12 -    8460 -  507.58 equations/s - 23951.92 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:14 - 0:40:18 -    8480 -  475.33 equations/s - 22150.00 words/s - PRIM-BWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:19 - 0:40:23 -    8500 -  489.80 equations/s - 23172.16 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:25 - 0:40:29 -    8520 -  444.32 equations/s - 20883.68 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:30 - 0:40:34 -    8540 -  493.71 equations/s - 22890.27 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:36 - 0:40:39 -    8560 -  462.61 equations/s - 21661.16 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:41 - 0:40:45 -    8580 -  485.75 equations/s - 22547.56 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:48 - 0:40:51 -    8600 -  372.06 equations/s - 17794.13 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:53 - 0:40:57 -    8620 -  467.03 equations/s - 21974.87 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:21:59 - 0:41:03 -    8640 -  448.07 equations/s - 21192.74 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:04 - 0:41:08 -    8660 -  515.60 equations/s - 23969.17 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:10 - 0:41:13 -    8680 -  458.19 equations/s - 21413.26 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:15 - 0:41:19 -    8700 -  465.12 equations/s - 21870.04 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:20 - 0:41:24 -    8720 -  507.91 equations/s - 23701.53 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:26 - 0:41:29 -    8740 -  481.90 equations/s - 22848.85 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:31 - 0:41:35 -    8760 -  443.05 equations/s - 20969.89 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:36 - 0:41:40 -    8780 -  512.20 equations/s - 23914.85 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:41 - 0:41:45 -    8800 -  520.63 equations/s - 24346.03 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:48 - 0:41:51 -    8820 -  397.10 equations/s - 18948.22 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:53 - 0:41:57 -    8840 -  456.47 equations/s - 21048.55 words/s - PRIM-BWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:22:58 - 0:42:02 -    8860 -  533.87 equations/s - 24549.96 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:04 - 0:42:07 -    8880 -  443.96 equations/s - 20915.09 words/s - PRIM-BWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:10 - 0:42:13 -    8900 -  445.04 equations/s - 20846.47 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:15 - 0:42:19 -    8920 -  469.48 equations/s - 21888.14 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:20 - 0:42:24 -    8940 -  485.63 equations/s - 22666.17 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:26 - 0:42:30 -    8960 -  449.85 equations/s - 21221.95 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:32 - 0:42:35 -    8980 -  447.83 equations/s - 21174.94 words/s - PRIM-BWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:37 - 0:42:40 -    9000 -  538.28 equations/s - 25022.71 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:42 - 0:42:45 -    9020 -  478.59 equations/s - 22193.93 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:48 - 0:42:51 -    9040 -  420.24 equations/s - 19691.14 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:53 - 0:42:56 -    9060 -  516.46 equations/s - 23936.34 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:23:58 - 0:43:01 -    9080 -  526.25 equations/s - 24309.57 words/s - PRIM-BWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:03 - 0:43:06 -    9100 -  520.85 equations/s - 24581.04 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:07 - 0:43:11 -    9120 -  556.03 equations/s - 25779.23 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:13 - 0:43:16 -    9140 -  487.86 equations/s - 22764.53 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:18 - 0:43:22 -    9160 -  452.19 equations/s - 21463.65 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:24 - 0:43:28 -    9180 -  422.68 equations/s - 20128.18 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:30 - 0:43:34 -    9200 -  410.15 equations/s - 19407.28 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:36 - 0:43:40 -    9220 -  457.36 equations/s - 21337.83 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:41 - 0:43:44 -    9240 -  524.50 equations/s - 24632.03 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:47 - 0:43:50 -    9260 -  443.17 equations/s - 21119.82 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:52 - 0:43:55 -    9280 -  502.34 equations/s - 23659.89 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:24:57 - 0:44:01 -    9300 -  466.72 equations/s - 22018.77 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:03 - 0:44:06 -    9320 -  494.07 equations/s - 23146.71 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:08 - 0:44:12 -    9340 -  455.69 equations/s - 21531.39 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:14 - 0:44:18 -    9360 -  433.03 equations/s - 20495.81 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:20 - 0:44:23 -    9380 -  456.29 equations/s - 21429.85 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:25 - 0:44:28 -    9400 -  493.21 equations/s - 23319.97 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:31 - 0:44:34 -    9420 -  452.14 equations/s - 21307.51 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:36 - 0:44:40 -    9440 -  463.66 equations/s - 21626.85 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:41 - 0:44:44 -    9460 -  538.80 equations/s - 25155.14 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:46 - 0:44:50 -    9480 -  459.66 equations/s - 21517.16 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:52 - 0:44:55 -    9500 -  493.86 equations/s - 23049.58 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:25:57 - 0:45:00 -    9520 -  480.40 equations/s - 22382.28 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:02 - 0:45:06 -    9540 -  462.14 equations/s - 22002.04 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:08 - 0:45:11 -    9560 -  490.61 equations/s - 23021.13 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:13 - 0:45:17 -    9580 -  457.73 equations/s - 21555.30 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:19 - 0:45:23 -    9600 -  439.23 equations/s - 20567.44 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:25 - 0:45:28 -    9620 -  430.36 equations/s - 20266.58 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:30 - 0:45:34 -    9640 -  482.74 equations/s - 22582.95 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:35 - 0:45:39 -    9660 -  511.49 equations/s - 23635.53 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:41 - 0:45:45 -    9680 -  427.75 equations/s - 20275.14 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:46 - 0:45:50 -    9700 -  515.52 equations/s - 23855.92 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:52 - 0:45:55 -    9720 -  452.37 equations/s - 21188.08 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:26:57 - 0:46:00 -    9740 -  522.29 equations/s - 24205.85 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:02 - 0:46:05 -    9760 -  536.51 equations/s - 25173.86 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:08 - 0:46:11 -    9780 -  414.10 equations/s - 19719.37 words/s - PRIM-BWD:  0.0443 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:14 - 0:46:18 -    9800 -  385.50 equations/s - 18418.31 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:21 - 0:46:25 -    9820 -  376.98 equations/s - 17852.43 words/s - PRIM-BWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:27 - 0:46:30 -    9840 -  446.02 equations/s - 21137.13 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:33 - 0:46:36 -    9860 -  428.12 equations/s - 20345.39 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:38 - 0:46:42 -    9880 -  493.83 equations/s - 23355.19 words/s - PRIM-BWD:  0.0434 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:43 - 0:46:47 -    9900 -  473.49 equations/s - 22362.01 words/s - PRIM-BWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:49 - 0:46:53 -    9920 -  451.63 equations/s - 21149.46 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:27:55 - 0:46:59 -    9940 -  434.08 equations/s - 20343.75 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:00 - 0:47:04 -    9960 -  476.46 equations/s - 22254.14 words/s - PRIM-BWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:06 - 0:47:09 -    9980 -  487.92 equations/s - 22905.22 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:11 - 0:47:15 -   10000 -  467.21 equations/s - 21721.98 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 06:28:17 - 0:47:20 -   10020 -  466.57 equations/s - 22090.68 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:22 - 0:47:26 -   10040 -  447.83 equations/s - 20982.24 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:28 - 0:47:32 -   10060 -  433.36 equations/s - 20345.37 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:35 - 0:47:38 -   10080 -  388.41 equations/s - 18470.06 words/s - PRIM-BWD:  0.0398 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:41 - 0:47:45 -   10100 -  408.40 equations/s - 19600.38 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:46 - 0:47:50 -   10120 -  483.46 equations/s - 22798.63 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:53 - 0:47:56 -   10140 -  397.19 equations/s - 18787.77 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:28:59 - 0:48:02 -   10160 -  437.32 equations/s - 20651.31 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:04 - 0:48:08 -   10180 -  466.38 equations/s - 21664.23 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:09 - 0:48:13 -   10200 -  497.72 equations/s - 23493.45 words/s - PRIM-BWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:14 - 0:48:18 -   10220 -  515.85 equations/s - 24136.93 words/s - PRIM-BWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:20 - 0:48:23 -   10240 -  470.57 equations/s - 22248.34 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:26 - 0:48:29 -   10260 -  445.89 equations/s - 21145.63 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:31 - 0:48:34 -   10280 -  483.64 equations/s - 22635.00 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:37 - 0:48:40 -   10300 -  441.07 equations/s - 20981.96 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:42 - 0:48:46 -   10320 -  453.07 equations/s - 21297.09 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:47 - 0:48:51 -   10340 -  495.34 equations/s - 22990.67 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:52 - 0:48:56 -   10360 -  514.71 equations/s - 23766.69 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:29:57 - 0:49:01 -   10380 -  515.90 equations/s - 23913.24 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:03 - 0:49:07 -   10400 -  437.00 equations/s - 20486.93 words/s - PRIM-BWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:09 - 0:49:12 -   10420 -  455.15 equations/s - 21698.82 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:16 - 0:49:19 -   10440 -  382.83 equations/s - 18071.43 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:21 - 0:49:25 -   10460 -  439.74 equations/s - 20481.46 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:27 - 0:49:31 -   10480 -  432.74 equations/s - 20251.26 words/s - PRIM-BWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:32 - 0:49:36 -   10500 -  512.19 equations/s - 24031.19 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:38 - 0:49:42 -   10520 -  438.62 equations/s - 20570.90 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:43 - 0:49:47 -   10540 -  488.64 equations/s - 23279.85 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:49 - 0:49:52 -   10560 -  459.83 equations/s - 21615.82 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:30:56 - 0:49:59 -   10580 -  388.18 equations/s - 18333.69 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:01 - 0:50:04 -   10600 -  475.03 equations/s - 22348.14 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:08 - 0:50:11 -   10620 -  374.57 equations/s - 17891.94 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:13 - 0:50:17 -   10640 -  474.83 equations/s - 22326.13 words/s - PRIM-BWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:18 - 0:50:22 -   10660 -  478.01 equations/s - 22471.26 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:24 - 0:50:27 -   10680 -  463.69 equations/s - 21676.90 words/s - PRIM-BWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:29 - 0:50:33 -   10700 -  468.64 equations/s - 21879.25 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:35 - 0:50:39 -   10720 -  440.74 equations/s - 20347.95 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:41 - 0:50:45 -   10740 -  429.51 equations/s - 20435.13 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:47 - 0:50:51 -   10760 -  418.82 equations/s - 19898.10 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:53 - 0:50:56 -   10780 -  457.36 equations/s - 21597.08 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:31:59 - 0:51:03 -   10800 -  419.07 equations/s - 19977.00 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:05 - 0:51:09 -   10820 -  407.72 equations/s - 19439.51 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:10 - 0:51:14 -   10840 -  527.24 equations/s - 24870.57 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:16 - 0:51:19 -   10860 -  443.85 equations/s - 21208.63 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:21 - 0:51:25 -   10880 -  500.17 equations/s - 23056.29 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:27 - 0:51:30 -   10900 -  454.79 equations/s - 21095.02 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:32 - 0:51:35 -   10920 -  492.17 equations/s - 22827.05 words/s - PRIM-BWD:  0.0382 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:38 - 0:51:42 -   10940 -  413.87 equations/s - 19445.60 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:44 - 0:51:47 -   10960 -  468.09 equations/s - 22248.88 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:49 - 0:51:53 -   10980 -  440.88 equations/s - 20778.84 words/s - PRIM-BWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:32:55 - 0:51:59 -   11000 -  428.06 equations/s - 20058.72 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:01 - 0:52:05 -   11020 -  435.30 equations/s - 20288.00 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:06 - 0:52:10 -   11040 -  513.58 equations/s - 24245.19 words/s - PRIM-BWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:11 - 0:52:15 -   11060 -  519.61 equations/s - 24316.30 words/s - PRIM-BWD:  0.0398 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:17 - 0:52:21 -   11080 -  410.82 equations/s - 19202.10 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:24 - 0:52:27 -   11100 -  391.30 equations/s - 18557.47 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:30 - 0:52:33 -   11120 -  441.75 equations/s - 21059.74 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:35 - 0:52:38 -   11140 -  497.46 equations/s - 23778.29 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:41 - 0:52:44 -   11160 -  422.63 equations/s - 19985.10 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:47 - 0:52:51 -   11180 -  408.13 equations/s - 19504.28 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:53 - 0:52:57 -   11200 -  435.51 equations/s - 20594.01 words/s - PRIM-BWD:  0.0401 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:33:59 - 0:53:02 -   11220 -  457.15 equations/s - 21465.48 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:04 - 0:53:08 -   11240 -  463.86 equations/s - 21677.94 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:10 - 0:53:14 -   11260 -  434.88 equations/s - 20619.67 words/s - PRIM-BWD:  0.0383 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:15 - 0:53:19 -   11280 -  491.16 equations/s - 22837.80 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:21 - 0:53:24 -   11300 -  473.37 equations/s - 22175.90 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:26 - 0:53:29 -   11320 -  507.88 equations/s - 23669.35 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:31 - 0:53:35 -   11340 -  463.14 equations/s - 21588.45 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:38 - 0:53:41 -   11360 -  406.09 equations/s - 19374.25 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:42 - 0:53:46 -   11380 -  552.79 equations/s - 25528.49 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:48 - 0:53:51 -   11400 -  459.48 equations/s - 21757.07 words/s - PRIM-BWD:  0.0372 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:34:54 - 0:53:57 -   11420 -  415.93 equations/s - 19400.57 words/s - PRIM-BWD:  0.0425 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:00 - 0:54:03 -   11440 -  426.26 equations/s - 20101.04 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:06 - 0:54:10 -   11460 -  395.42 equations/s - 18526.73 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:13 - 0:54:16 -   11480 -  412.93 equations/s - 19572.35 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:19 - 0:54:22 -   11500 -  413.11 equations/s - 19848.24 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:25 - 0:54:28 -   11520 -  420.65 equations/s - 19806.97 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:30 - 0:54:34 -   11540 -  466.30 equations/s - 22095.80 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:35 - 0:54:39 -   11560 -  499.45 equations/s - 23308.60 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:42 - 0:54:46 -   11580 -  371.74 equations/s - 17655.40 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:47 - 0:54:51 -   11600 -  504.70 equations/s - 23798.13 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:54 - 0:54:57 -   11620 -  407.21 equations/s - 19452.62 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:35:59 - 0:55:02 -   11640 -  498.01 equations/s - 23344.84 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:05 - 0:55:09 -   11660 -  401.89 equations/s - 19201.56 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:11 - 0:55:15 -   11680 -  434.02 equations/s - 20387.00 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:17 - 0:55:20 -   11700 -  476.35 equations/s - 22463.72 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 -   11720 -  495.06 equations/s - 23157.41 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - ============ End of epoch 2 ============\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - 0/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:25 - 128/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:26 - 256/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:26 - 384/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:26 - 512/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:26 - 640/2001\n",
            "INFO - 12/05/22 06:36:22 - 0:55:26 - 768/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 896/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 1024/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 1152/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 1280/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 1408/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:26 - 1536/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - 1664/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - 1792/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - 1920/2001\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - 1420/2001 (70.96451774112944%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 06:36:23 - 0:55:27 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 0/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 128/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 256/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 384/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 512/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 640/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:27 - 768/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:28 - 896/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:28 - 1024/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:28 - 1152/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:28 - 1280/2001\n",
            "INFO - 12/05/22 06:36:24 - 0:55:28 - 1408/2001\n",
            "INFO - 12/05/22 06:36:25 - 0:55:28 - 1536/2001\n",
            "INFO - 12/05/22 06:36:25 - 0:55:28 - 1664/2001\n",
            "INFO - 12/05/22 06:36:25 - 0:55:28 - 1792/2001\n",
            "INFO - 12/05/22 06:36:25 - 0:55:28 - 1920/2001\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - 1435/2001 (71.71414292853574%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - epoch -> 2.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_xe_loss -> 1.265509\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc -> 70.964518\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_5 -> 63.380282\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_6 -> 73.484848\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_7 -> 76.158940\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_8 -> 71.244635\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_9 -> 72.941176\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_10 -> 65.838509\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_11 -> 76.335878\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_12 -> 68.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_13 -> 71.578947\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_14 -> 71.698113\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_15 -> 69.444444\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_16 -> 68.918919\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_17 -> 69.767442\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_18 -> 67.391304\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_19 -> 63.636364\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_20 -> 81.250000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_21 -> 82.608696\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_22 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_23 -> 68.181818\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_24 -> 55.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_25 -> 72.222222\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_26 -> 86.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_27 -> 69.230769\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_28 -> 69.230769\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_29 -> 63.636364\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_32 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_34 -> 75.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_39 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_58 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_61 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - valid_prim_bwd_acc_129 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_xe_loss -> 1.213473\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc -> 71.714143\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_3 -> 77.777778\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_4 -> 71.111111\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_5 -> 78.947368\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_6 -> 77.037037\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_7 -> 76.630435\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_8 -> 73.148148\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_9 -> 72.670807\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_10 -> 70.731707\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_11 -> 59.712230\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_12 -> 66.423358\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_13 -> 78.846154\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_14 -> 66.265060\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_15 -> 73.437500\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_16 -> 65.714286\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_17 -> 72.727273\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_18 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_19 -> 75.609756\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_20 -> 82.978723\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_21 -> 74.074074\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_22 -> 68.965517\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_23 -> 85.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_24 -> 52.380952\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_25 -> 85.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_26 -> 78.571429\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_27 -> 78.571429\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_28 -> 77.777778\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_29 -> 64.705882\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_31 -> 62.500000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_34 -> 60.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_46 -> 66.666667\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_78 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_127 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - test_prim_bwd_acc_144 -> 100.000000\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - __log__:{\"epoch\": 2, \"valid_prim_bwd_xe_loss\": 1.2655091064087096, \"valid_prim_bwd_acc\": 70.96451774112944, \"valid_prim_bwd_acc_2\": 50.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 74.4186046511628, \"valid_prim_bwd_acc_5\": 63.38028169014085, \"valid_prim_bwd_acc_6\": 73.48484848484848, \"valid_prim_bwd_acc_7\": 76.15894039735099, \"valid_prim_bwd_acc_8\": 71.24463519313305, \"valid_prim_bwd_acc_9\": 72.94117647058823, \"valid_prim_bwd_acc_10\": 65.83850931677019, \"valid_prim_bwd_acc_11\": 76.33587786259542, \"valid_prim_bwd_acc_12\": 68.0, \"valid_prim_bwd_acc_13\": 71.57894736842105, \"valid_prim_bwd_acc_14\": 71.69811320754717, \"valid_prim_bwd_acc_15\": 69.44444444444444, \"valid_prim_bwd_acc_16\": 68.91891891891892, \"valid_prim_bwd_acc_17\": 69.76744186046511, \"valid_prim_bwd_acc_18\": 67.3913043478261, \"valid_prim_bwd_acc_19\": 63.63636363636363, \"valid_prim_bwd_acc_20\": 81.25, \"valid_prim_bwd_acc_21\": 82.6086956521739, \"valid_prim_bwd_acc_22\": 66.66666666666667, \"valid_prim_bwd_acc_23\": 68.18181818181819, \"valid_prim_bwd_acc_24\": 55.0, \"valid_prim_bwd_acc_25\": 72.22222222222223, \"valid_prim_bwd_acc_26\": 86.66666666666667, \"valid_prim_bwd_acc_27\": 69.23076923076923, \"valid_prim_bwd_acc_28\": 69.23076923076923, \"valid_prim_bwd_acc_29\": 63.63636363636363, \"valid_prim_bwd_acc_30\": 55.55555555555556, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 66.66666666666667, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 75.0, \"valid_prim_bwd_acc_35\": 50.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 100.0, \"valid_prim_bwd_acc_39\": 100.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 50.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 100.0, \"valid_prim_bwd_acc_46\": 100.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 50.0, \"valid_prim_bwd_acc_56\": 0.0, \"valid_prim_bwd_acc_57\": 0.0, \"valid_prim_bwd_acc_58\": 100.0, \"valid_prim_bwd_acc_61\": 100.0, \"valid_prim_bwd_acc_62\": 50.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 0.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 50.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 100.0, \"test_prim_bwd_xe_loss\": 1.2134726551519162, \"test_prim_bwd_acc\": 71.71414292853574, \"test_prim_bwd_acc_3\": 77.77777777777777, \"test_prim_bwd_acc_4\": 71.11111111111111, \"test_prim_bwd_acc_5\": 78.94736842105263, \"test_prim_bwd_acc_6\": 77.03703703703704, \"test_prim_bwd_acc_7\": 76.6304347826087, \"test_prim_bwd_acc_8\": 73.14814814814815, \"test_prim_bwd_acc_9\": 72.67080745341615, \"test_prim_bwd_acc_10\": 70.73170731707317, \"test_prim_bwd_acc_11\": 59.71223021582734, \"test_prim_bwd_acc_12\": 66.42335766423358, \"test_prim_bwd_acc_13\": 78.84615384615384, \"test_prim_bwd_acc_14\": 66.26506024096386, \"test_prim_bwd_acc_15\": 73.4375, \"test_prim_bwd_acc_16\": 65.71428571428571, \"test_prim_bwd_acc_17\": 72.72727272727273, \"test_prim_bwd_acc_18\": 66.66666666666667, \"test_prim_bwd_acc_19\": 75.60975609756098, \"test_prim_bwd_acc_20\": 82.97872340425532, \"test_prim_bwd_acc_21\": 74.07407407407408, \"test_prim_bwd_acc_22\": 68.96551724137932, \"test_prim_bwd_acc_23\": 85.0, \"test_prim_bwd_acc_24\": 52.38095238095238, \"test_prim_bwd_acc_25\": 85.0, \"test_prim_bwd_acc_26\": 78.57142857142857, \"test_prim_bwd_acc_27\": 78.57142857142857, \"test_prim_bwd_acc_28\": 77.77777777777777, \"test_prim_bwd_acc_29\": 64.70588235294117, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 62.5, \"test_prim_bwd_acc_32\": 50.0, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 60.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 50.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 66.66666666666667, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 100.0, \"test_prim_bwd_acc_54\": 50.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 100.0, \"test_prim_bwd_acc_65\": 100.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 100.0, \"test_prim_bwd_acc_81\": 0.0, \"test_prim_bwd_acc_82\": 0.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 100.0, \"test_prim_bwd_acc_144\": 100.0}\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - New best score for valid_prim_bwd_acc: 70.964518\n",
            "INFO - 12/05/22 06:36:25 - 0:55:29 - Saving best-valid_prim_bwd_acc to ./dumped/deriv2fwd_modelbwd/6gswznivsf/best-valid_prim_bwd_acc.pth ...\n",
            "WARNING - 12/05/22 06:36:25 - 0:55:29 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:36:25 - 0:55:29 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:36:25 - 0:55:29 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:36:30 - 0:55:34 - Saving checkpoint to ./dumped/deriv2fwd_modelbwd/6gswznivsf/checkpoint.pth ...\n",
            "WARNING - 12/05/22 06:36:30 - 0:55:34 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:36:30 - 0:55:34 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:36:30 - 0:55:34 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:36:38 - 0:55:42 - ============ Starting epoch 3 ... ============\n",
            "INFO - 12/05/22 06:36:44 - 0:55:47 -   11740 -  115.96 equations/s -  5466.25 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:49 - 0:55:53 -   11760 -  478.09 equations/s - 22513.53 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:36:55 - 0:55:58 -   11780 -  470.64 equations/s - 22208.83 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:00 - 0:56:03 -   11800 -  482.22 equations/s - 22574.50 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:05 - 0:56:09 -   11820 -  469.27 equations/s - 22080.84 words/s - PRIM-BWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:11 - 0:56:15 -   11840 -  414.62 equations/s - 19583.83 words/s - PRIM-BWD:  0.0381 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:17 - 0:56:20 -   11860 -  476.72 equations/s - 22251.47 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:22 - 0:56:26 -   11880 -  466.04 equations/s - 21924.65 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:28 - 0:56:31 -   11900 -  474.59 equations/s - 22142.94 words/s - PRIM-BWD:  0.0390 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:34 - 0:56:38 -   11920 -  405.67 equations/s - 19249.93 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:40 - 0:56:43 -   11940 -  447.35 equations/s - 20687.80 words/s - PRIM-BWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:46 - 0:56:50 -   11960 -  409.82 equations/s - 19599.95 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:52 - 0:56:55 -   11980 -  444.83 equations/s - 21096.46 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:37:58 - 0:57:01 -   12000 -  435.17 equations/s - 20321.94 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 06:38:03 - 0:57:06 -   12020 -  483.07 equations/s - 22570.66 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:09 - 0:57:12 -   12040 -  454.46 equations/s - 21379.12 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:14 - 0:57:18 -   12060 -  447.59 equations/s - 21337.27 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:20 - 0:57:24 -   12080 -  447.12 equations/s - 21101.77 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:25 - 0:57:29 -   12100 -  470.90 equations/s - 22065.71 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:32 - 0:57:35 -   12120 -  413.74 equations/s - 19436.34 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:37 - 0:57:41 -   12140 -  477.88 equations/s - 22529.92 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:42 - 0:57:46 -   12160 -  491.28 equations/s - 22766.58 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:48 - 0:57:51 -   12180 -  451.65 equations/s - 21145.64 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:54 - 0:57:58 -   12200 -  397.98 equations/s - 18804.43 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:38:59 - 0:58:03 -   12220 -  527.02 equations/s - 24600.29 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:06 - 0:58:09 -   12240 -  391.28 equations/s - 18381.84 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:10 - 0:58:14 -   12260 -  542.89 equations/s - 25433.33 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:16 - 0:58:19 -   12280 -  479.96 equations/s - 22347.45 words/s - PRIM-BWD:  0.0381 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:21 - 0:58:25 -   12300 -  454.50 equations/s - 21220.96 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:27 - 0:58:30 -   12320 -  469.44 equations/s - 21615.08 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:33 - 0:58:36 -   12340 -  443.19 equations/s - 21091.68 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:39 - 0:58:42 -   12360 -  419.92 equations/s - 19646.59 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:43 - 0:58:47 -   12380 -  540.23 equations/s - 24977.29 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:49 - 0:58:52 -   12400 -  474.88 equations/s - 22135.10 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:39:54 - 0:58:57 -   12420 -  510.21 equations/s - 23927.90 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:01 - 0:59:04 -   12440 -  370.23 equations/s - 17628.39 words/s - PRIM-BWD:  0.0398 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:06 - 0:59:09 -   12460 -  516.17 equations/s - 23810.78 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:11 - 0:59:15 -   12480 -  456.00 equations/s - 21563.20 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:16 - 0:59:20 -   12500 -  508.94 equations/s - 24201.32 words/s - PRIM-BWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:23 - 0:59:27 -   12520 -  367.76 equations/s - 17359.62 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:28 - 0:59:32 -   12540 -  506.86 equations/s - 23504.06 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:34 - 0:59:38 -   12560 -  449.68 equations/s - 21114.89 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:39 - 0:59:43 -   12580 -  482.10 equations/s - 22324.37 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:46 - 0:59:49 -   12600 -  419.04 equations/s - 19652.53 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:51 - 0:59:54 -   12620 -  466.69 equations/s - 21819.59 words/s - PRIM-BWD:  0.0421 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:40:57 - 1:00:01 -   12640 -  419.07 equations/s - 19976.51 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:03 - 1:00:07 -   12660 -  415.41 equations/s - 19824.88 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:08 - 1:00:11 -   12680 -  541.57 equations/s - 25087.58 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:13 - 1:00:16 -   12700 -  517.39 equations/s - 24244.93 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:18 - 1:00:22 -   12720 -  495.28 equations/s - 23467.78 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:24 - 1:00:27 -   12740 -  442.83 equations/s - 20895.34 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:30 - 1:00:34 -   12760 -  412.98 equations/s - 19603.27 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:36 - 1:00:40 -   12780 -  431.57 equations/s - 20402.37 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:41 - 1:00:45 -   12800 -  504.92 equations/s - 23605.66 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:47 - 1:00:51 -   12820 -  402.66 equations/s - 18898.82 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:52 - 1:00:56 -   12840 -  509.89 equations/s - 23847.66 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:41:57 - 1:01:01 -   12860 -  523.74 equations/s - 24444.01 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:02 - 1:01:05 -   12880 -  572.94 equations/s - 26284.22 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:08 - 1:01:11 -   12900 -  429.81 equations/s - 20503.10 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:14 - 1:01:17 -   12920 -  431.90 equations/s - 20331.84 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:19 - 1:01:23 -   12940 -  457.29 equations/s - 21496.02 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:25 - 1:01:28 -   12960 -  471.51 equations/s - 22016.03 words/s - PRIM-BWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:30 - 1:01:34 -   12980 -  466.87 equations/s - 21914.34 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:36 - 1:01:39 -   13000 -  462.09 equations/s - 21761.88 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:41 - 1:01:44 -   13020 -  524.51 equations/s - 24391.51 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:46 - 1:01:50 -   13040 -  468.70 equations/s - 21904.94 words/s - PRIM-BWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:52 - 1:01:56 -   13060 -  411.06 equations/s - 19592.11 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:42:58 - 1:02:02 -   13080 -  444.33 equations/s - 21145.29 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:04 - 1:02:08 -   13100 -  431.15 equations/s - 20250.39 words/s - PRIM-BWD:  0.0372 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:09 - 1:02:12 -   13120 -  520.69 equations/s - 24019.05 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:14 - 1:02:18 -   13140 -  481.94 equations/s - 22602.80 words/s - PRIM-BWD:  0.0382 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:19 - 1:02:22 -   13160 -  585.04 equations/s - 26763.37 words/s - PRIM-BWD:  0.0373 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:25 - 1:02:28 -   13180 -  438.25 equations/s - 20748.97 words/s - PRIM-BWD:  0.0392 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:30 - 1:02:34 -   13200 -  437.14 equations/s - 20536.01 words/s - PRIM-BWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:36 - 1:02:40 -   13220 -  444.21 equations/s - 21154.63 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:41 - 1:02:45 -   13240 -  504.48 equations/s - 23585.82 words/s - PRIM-BWD:  0.0379 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:47 - 1:02:50 -   13260 -  447.31 equations/s - 21007.52 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:54 - 1:02:57 -   13280 -  378.55 equations/s - 18140.32 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:43:59 - 1:03:02 -   13300 -  498.28 equations/s - 23533.21 words/s - PRIM-BWD:  0.0367 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:05 - 1:03:08 -   13320 -  450.09 equations/s - 21066.86 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:10 - 1:03:13 -   13340 -  483.54 equations/s - 22543.91 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:16 - 1:03:19 -   13360 -  430.11 equations/s - 20610.50 words/s - PRIM-BWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:21 - 1:03:25 -   13380 -  457.22 equations/s - 21579.35 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:27 - 1:03:31 -   13400 -  437.43 equations/s - 20646.52 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:32 - 1:03:36 -   13420 -  529.44 equations/s - 24755.93 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:37 - 1:03:40 -   13440 -  546.52 equations/s - 25358.08 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:43 - 1:03:47 -   13460 -  384.53 equations/s - 18148.45 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:48 - 1:03:52 -   13480 -  505.93 equations/s - 23514.55 words/s - PRIM-BWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:54 - 1:03:57 -   13500 -  502.30 equations/s - 23476.36 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:44:58 - 1:04:02 -   13520 -  521.67 equations/s - 24202.04 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:03 - 1:04:06 -   13540 -  562.87 equations/s - 26111.28 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:08 - 1:04:12 -   13560 -  503.34 equations/s - 23488.22 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:13 - 1:04:17 -   13580 -  485.62 equations/s - 22832.26 words/s - PRIM-BWD:  0.0401 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:20 - 1:04:23 -   13600 -  410.56 equations/s - 19463.54 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:25 - 1:04:28 -   13620 -  488.82 equations/s - 22719.08 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:30 - 1:04:34 -   13640 -  489.00 equations/s - 22997.33 words/s - PRIM-BWD:  0.0424 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:36 - 1:04:40 -   13660 -  398.27 equations/s - 19111.59 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:42 - 1:04:46 -   13680 -  434.71 equations/s - 20725.22 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:48 - 1:04:52 -   13700 -  429.97 equations/s - 20297.58 words/s - PRIM-BWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:45:54 - 1:04:57 -   13720 -  480.51 equations/s - 22704.34 words/s - PRIM-BWD:  0.0392 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:00 - 1:05:03 -   13740 -  406.25 equations/s - 19223.79 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:06 - 1:05:09 -   13760 -  426.03 equations/s - 20207.69 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:12 - 1:05:15 -   13780 -  438.97 equations/s - 20453.07 words/s - PRIM-BWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:18 - 1:05:21 -   13800 -  421.36 equations/s - 19997.93 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:24 - 1:05:28 -   13820 -  407.82 equations/s - 19279.41 words/s - PRIM-BWD:  0.0375 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:29 - 1:05:32 -   13840 -  545.17 equations/s - 25433.96 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:34 - 1:05:37 -   13860 -  521.29 equations/s - 24719.84 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:40 - 1:05:43 -   13880 -  421.59 equations/s - 19822.23 words/s - PRIM-BWD:  0.0369 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:45 - 1:05:49 -   13900 -  466.35 equations/s - 21652.13 words/s - PRIM-BWD:  0.0363 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:51 - 1:05:54 -   13920 -  475.24 equations/s - 22621.44 words/s - PRIM-BWD:  0.0422 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:46:57 - 1:06:00 -   13940 -  439.04 equations/s - 20751.89 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:03 - 1:06:06 -   13960 -  420.82 equations/s - 19709.16 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:09 - 1:06:12 -   13980 -  401.40 equations/s - 19023.19 words/s - PRIM-BWD:  0.0429 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:14 - 1:06:18 -   14000 -  465.76 equations/s - 21770.87 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "INFO - 12/05/22 06:47:20 - 1:06:23 -   14020 -  495.21 equations/s - 23103.18 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:26 - 1:06:29 -   14040 -  434.45 equations/s - 20198.59 words/s - PRIM-BWD:  0.0386 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:32 - 1:06:35 -   14060 -  403.03 equations/s - 19146.10 words/s - PRIM-BWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:39 - 1:06:42 -   14080 -  375.09 equations/s - 17800.62 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:44 - 1:06:48 -   14100 -  446.41 equations/s - 20939.76 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:50 - 1:06:54 -   14120 -  429.64 equations/s - 20314.05 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:47:56 - 1:07:00 -   14140 -  429.74 equations/s - 19994.77 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:03 - 1:07:06 -   14160 -  393.16 equations/s - 18750.71 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:09 - 1:07:12 -   14180 -  456.52 equations/s - 21573.63 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:14 - 1:07:17 -   14200 -  471.60 equations/s - 22076.39 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:19 - 1:07:23 -   14220 -  483.77 equations/s - 22708.66 words/s - PRIM-BWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:24 - 1:07:28 -   14240 -  507.77 equations/s - 23460.43 words/s - PRIM-BWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:29 - 1:07:33 -   14260 -  517.24 equations/s - 23979.65 words/s - PRIM-BWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:34 - 1:07:38 -   14280 -  496.34 equations/s - 23214.16 words/s - PRIM-BWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:41 - 1:07:44 -   14300 -  416.08 equations/s - 19541.35 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:45 - 1:07:49 -   14320 -  520.88 equations/s - 24068.42 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:51 - 1:07:55 -   14340 -  431.61 equations/s - 20207.72 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:48:57 - 1:08:00 -   14360 -  485.82 equations/s - 22778.83 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:03 - 1:08:06 -   14380 -  415.35 equations/s - 19832.31 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:08 - 1:08:11 -   14400 -  507.57 equations/s - 23247.70 words/s - PRIM-BWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:14 - 1:08:17 -   14420 -  450.48 equations/s - 21155.11 words/s - PRIM-BWD:  0.0382 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:19 - 1:08:22 -   14440 -  466.50 equations/s - 22084.68 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:25 - 1:08:29 -   14460 -  418.49 equations/s - 19490.53 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:31 - 1:08:34 -   14480 -  469.78 equations/s - 22109.17 words/s - PRIM-BWD:  0.0401 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:37 - 1:08:40 -   14500 -  417.04 equations/s - 19562.38 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:43 - 1:08:46 -   14520 -  439.86 equations/s - 20894.74 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:48 - 1:08:52 -   14540 -  447.02 equations/s - 20755.53 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:53 - 1:08:57 -   14560 -  499.42 equations/s - 23287.22 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:49:59 - 1:09:03 -   14580 -  446.10 equations/s - 21107.91 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:04 - 1:09:08 -   14600 -  503.96 equations/s - 23644.11 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:10 - 1:09:13 -   14620 -  480.64 equations/s - 22532.34 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:15 - 1:09:19 -   14640 -  433.43 equations/s - 20583.15 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:21 - 1:09:24 -   14660 -  505.70 equations/s - 23667.15 words/s - PRIM-BWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:26 - 1:09:30 -   14680 -  451.28 equations/s - 21003.13 words/s - PRIM-BWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:32 - 1:09:35 -   14700 -  438.31 equations/s - 20850.98 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:38 - 1:09:41 -   14720 -  447.65 equations/s - 21110.14 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:44 - 1:09:48 -   14740 -  395.38 equations/s - 18638.22 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:50 - 1:09:53 -   14760 -  446.54 equations/s - 21062.05 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:50:55 - 1:09:58 -   14780 -  505.41 equations/s - 23674.33 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:01 - 1:10:04 -   14800 -  437.55 equations/s - 20828.81 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:06 - 1:10:10 -   14820 -  467.36 equations/s - 21599.19 words/s - PRIM-BWD:  0.0374 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:11 - 1:10:15 -   14840 -  498.18 equations/s - 23197.13 words/s - PRIM-BWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:18 - 1:10:21 -   14860 -  412.28 equations/s - 19080.95 words/s - PRIM-BWD:  0.0392 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:24 - 1:10:27 -   14880 -  422.08 equations/s - 20212.18 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:29 - 1:10:33 -   14900 -  456.86 equations/s - 21674.98 words/s - PRIM-BWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:35 - 1:10:38 -   14920 -  467.00 equations/s - 22053.95 words/s - PRIM-BWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:41 - 1:10:44 -   14940 -  429.19 equations/s - 20449.28 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:47 - 1:10:50 -   14960 -  438.07 equations/s - 20490.18 words/s - PRIM-BWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:52 - 1:10:55 -   14980 -  476.69 equations/s - 22148.30 words/s - PRIM-BWD:  0.0371 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:51:58 - 1:11:02 -   15000 -  419.55 equations/s - 19844.14 words/s - PRIM-BWD:  0.0377 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:05 - 1:11:08 -   15020 -  379.14 equations/s - 18151.16 words/s - PRIM-BWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:11 - 1:11:14 -   15040 -  450.75 equations/s - 21290.29 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:16 - 1:11:19 -   15060 -  497.35 equations/s - 23517.06 words/s - PRIM-BWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:22 - 1:11:25 -   15080 -  429.32 equations/s - 20291.55 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:27 - 1:11:31 -   15100 -  462.81 equations/s - 21859.12 words/s - PRIM-BWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:32 - 1:11:36 -   15120 -  498.71 equations/s - 23444.99 words/s - PRIM-BWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:39 - 1:11:42 -   15140 -  401.20 equations/s - 19273.02 words/s - PRIM-BWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:44 - 1:11:48 -   15160 -  479.91 equations/s - 22303.67 words/s - PRIM-BWD:  0.0377 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:50 - 1:11:53 -   15180 -  434.73 equations/s - 20154.42 words/s - PRIM-BWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:52:56 - 1:11:59 -   15200 -  445.56 equations/s - 21034.25 words/s - PRIM-BWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:01 - 1:12:05 -   15220 -  460.98 equations/s - 21362.69 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:08 - 1:12:11 -   15240 -  382.89 equations/s - 17990.73 words/s - PRIM-BWD:  0.0387 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:13 - 1:12:17 -   15260 -  486.00 equations/s - 22733.34 words/s - PRIM-BWD:  0.0401 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:19 - 1:12:23 -   15280 -  413.50 equations/s - 19263.49 words/s - PRIM-BWD:  0.0392 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:26 - 1:12:29 -   15300 -  409.89 equations/s - 19795.45 words/s - PRIM-BWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:32 - 1:12:36 -   15320 -  388.68 equations/s - 18563.01 words/s - PRIM-BWD:  0.0391 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:38 - 1:12:41 -   15340 -  470.69 equations/s - 21873.29 words/s - PRIM-BWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:43 - 1:12:47 -   15360 -  466.19 equations/s - 22100.19 words/s - PRIM-BWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:48 - 1:12:52 -   15380 -  517.81 equations/s - 24189.14 words/s - PRIM-BWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:53 - 1:12:56 -   15400 -  553.62 equations/s - 25814.82 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:53:59 - 1:13:03 -   15420 -  399.45 equations/s - 18997.19 words/s - PRIM-BWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:05 - 1:13:09 -   15440 -  415.38 equations/s - 19565.80 words/s - PRIM-BWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:11 - 1:13:15 -   15460 -  436.50 equations/s - 20462.40 words/s - PRIM-BWD:  0.0427 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:17 - 1:13:20 -   15480 -  461.41 equations/s - 21947.85 words/s - PRIM-BWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:22 - 1:13:25 -   15500 -  485.12 equations/s - 22793.63 words/s - PRIM-BWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:27 - 1:13:30 -   15520 -  513.86 equations/s - 23906.36 words/s - PRIM-BWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:32 - 1:13:36 -   15540 -  500.32 equations/s - 23262.09 words/s - PRIM-BWD:  0.0379 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:37 - 1:13:40 -   15560 -  526.19 equations/s - 24345.79 words/s - PRIM-BWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:43 - 1:13:46 -   15580 -  422.32 equations/s - 20240.52 words/s - PRIM-BWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:48 - 1:13:51 -   15600 -  512.41 equations/s - 23646.73 words/s - PRIM-BWD:  0.0430 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:53 - 1:13:56 -   15620 -  548.68 equations/s - 25194.08 words/s - PRIM-BWD:  0.0383 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - ============ End of epoch 3 ============\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - Creating valid iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - 0/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:58 - 128/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:59 - 256/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:59 - 384/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:59 - 512/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:59 - 640/2001\n",
            "INFO - 12/05/22 06:54:55 - 1:13:59 - 768/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 896/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 1024/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 1152/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 1280/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 1408/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:13:59 - 1536/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - 1664/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - 1792/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - 1920/2001\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - 1429/2001 (71.41429285357322%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - Creating test iterator for prim_bwd ...\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 06:54:56 - 1:14:00 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 0/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 128/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 256/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 384/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 512/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 640/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:00 - 768/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:01 - 896/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:01 - 1024/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:01 - 1152/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:01 - 1280/2001\n",
            "INFO - 12/05/22 06:54:57 - 1:14:01 - 1408/2001\n",
            "INFO - 12/05/22 06:54:58 - 1:14:01 - 1536/2001\n",
            "INFO - 12/05/22 06:54:58 - 1:14:01 - 1664/2001\n",
            "INFO - 12/05/22 06:54:58 - 1:14:01 - 1792/2001\n",
            "INFO - 12/05/22 06:54:58 - 1:14:01 - 1920/2001\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - 1399/2001 (69.91504247876063%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - epoch -> 3.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_xe_loss -> 1.197545\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc -> 71.414293\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_2 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_4 -> 69.767442\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_5 -> 67.605634\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_6 -> 73.484848\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_7 -> 74.834437\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_8 -> 70.815451\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_9 -> 72.352941\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_10 -> 63.975155\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_11 -> 74.045802\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_12 -> 66.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_13 -> 76.842105\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_14 -> 73.584906\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_15 -> 73.611111\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_16 -> 68.918919\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_17 -> 67.441860\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_18 -> 73.913043\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_19 -> 72.727273\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_20 -> 75.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_21 -> 82.608696\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_22 -> 57.142857\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_23 -> 81.818182\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_24 -> 65.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_25 -> 72.222222\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_26 -> 73.333333\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_27 -> 84.615385\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_28 -> 76.923077\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_29 -> 72.727273\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_32 -> 83.333333\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_33 -> 75.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_35 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_39 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_45 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_47 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_49 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_51 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_56 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_61 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_62 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_65 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_85 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_93 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_104 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - valid_prim_bwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_xe_loss -> 1.214464\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc -> 69.915042\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_3 -> 55.555556\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_4 -> 64.444444\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_5 -> 68.421053\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_6 -> 74.814815\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_8 -> 69.907407\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_9 -> 72.670807\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_10 -> 68.292683\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_11 -> 64.748201\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_12 -> 62.773723\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_13 -> 74.038462\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_14 -> 68.674699\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_15 -> 75.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_16 -> 60.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_17 -> 75.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_18 -> 61.904762\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_19 -> 80.487805\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_20 -> 78.723404\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_21 -> 62.962963\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_22 -> 75.862069\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_23 -> 85.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_24 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_25 -> 90.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_26 -> 71.428571\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_27 -> 64.285714\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_28 -> 77.777778\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_29 -> 70.588235\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_32 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_33 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_37 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_39 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_40 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_42 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_48 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_52 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_54 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_56 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_57 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_78 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_81 -> 50.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_82 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_84 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_127 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - test_prim_bwd_acc_144 -> 100.000000\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - __log__:{\"epoch\": 3, \"valid_prim_bwd_xe_loss\": 1.1975446450358804, \"valid_prim_bwd_acc\": 71.41429285357322, \"valid_prim_bwd_acc_2\": 0.0, \"valid_prim_bwd_acc_3\": 70.0, \"valid_prim_bwd_acc_4\": 69.76744186046511, \"valid_prim_bwd_acc_5\": 67.6056338028169, \"valid_prim_bwd_acc_6\": 73.48484848484848, \"valid_prim_bwd_acc_7\": 74.83443708609272, \"valid_prim_bwd_acc_8\": 70.81545064377683, \"valid_prim_bwd_acc_9\": 72.3529411764706, \"valid_prim_bwd_acc_10\": 63.975155279503106, \"valid_prim_bwd_acc_11\": 74.04580152671755, \"valid_prim_bwd_acc_12\": 66.0, \"valid_prim_bwd_acc_13\": 76.84210526315789, \"valid_prim_bwd_acc_14\": 73.58490566037736, \"valid_prim_bwd_acc_15\": 73.61111111111111, \"valid_prim_bwd_acc_16\": 68.91891891891892, \"valid_prim_bwd_acc_17\": 67.44186046511628, \"valid_prim_bwd_acc_18\": 73.91304347826087, \"valid_prim_bwd_acc_19\": 72.72727272727273, \"valid_prim_bwd_acc_20\": 75.0, \"valid_prim_bwd_acc_21\": 82.6086956521739, \"valid_prim_bwd_acc_22\": 57.142857142857146, \"valid_prim_bwd_acc_23\": 81.81818181818181, \"valid_prim_bwd_acc_24\": 65.0, \"valid_prim_bwd_acc_25\": 72.22222222222223, \"valid_prim_bwd_acc_26\": 73.33333333333333, \"valid_prim_bwd_acc_27\": 84.61538461538461, \"valid_prim_bwd_acc_28\": 76.92307692307692, \"valid_prim_bwd_acc_29\": 72.72727272727273, \"valid_prim_bwd_acc_30\": 66.66666666666667, \"valid_prim_bwd_acc_31\": 83.33333333333333, \"valid_prim_bwd_acc_32\": 83.33333333333333, \"valid_prim_bwd_acc_33\": 75.0, \"valid_prim_bwd_acc_34\": 100.0, \"valid_prim_bwd_acc_35\": 100.0, \"valid_prim_bwd_acc_36\": 100.0, \"valid_prim_bwd_acc_37\": 75.0, \"valid_prim_bwd_acc_39\": 100.0, \"valid_prim_bwd_acc_40\": 100.0, \"valid_prim_bwd_acc_41\": 100.0, \"valid_prim_bwd_acc_42\": 50.0, \"valid_prim_bwd_acc_44\": 50.0, \"valid_prim_bwd_acc_45\": 100.0, \"valid_prim_bwd_acc_46\": 0.0, \"valid_prim_bwd_acc_47\": 100.0, \"valid_prim_bwd_acc_49\": 100.0, \"valid_prim_bwd_acc_51\": 100.0, \"valid_prim_bwd_acc_53\": 100.0, \"valid_prim_bwd_acc_55\": 0.0, \"valid_prim_bwd_acc_56\": 100.0, \"valid_prim_bwd_acc_57\": 100.0, \"valid_prim_bwd_acc_58\": 0.0, \"valid_prim_bwd_acc_61\": 100.0, \"valid_prim_bwd_acc_62\": 100.0, \"valid_prim_bwd_acc_64\": 100.0, \"valid_prim_bwd_acc_65\": 100.0, \"valid_prim_bwd_acc_68\": 0.0, \"valid_prim_bwd_acc_71\": 0.0, \"valid_prim_bwd_acc_76\": 0.0, \"valid_prim_bwd_acc_80\": 100.0, \"valid_prim_bwd_acc_85\": 100.0, \"valid_prim_bwd_acc_90\": 0.0, \"valid_prim_bwd_acc_93\": 100.0, \"valid_prim_bwd_acc_95\": 0.0, \"valid_prim_bwd_acc_97\": 0.0, \"valid_prim_bwd_acc_101\": 50.0, \"valid_prim_bwd_acc_103\": 100.0, \"valid_prim_bwd_acc_104\": 100.0, \"valid_prim_bwd_acc_129\": 0.0, \"test_prim_bwd_xe_loss\": 1.2144640417828196, \"test_prim_bwd_acc\": 69.91504247876063, \"test_prim_bwd_acc_3\": 55.55555555555556, \"test_prim_bwd_acc_4\": 64.44444444444444, \"test_prim_bwd_acc_5\": 68.42105263157895, \"test_prim_bwd_acc_6\": 74.81481481481481, \"test_prim_bwd_acc_7\": 71.73913043478261, \"test_prim_bwd_acc_8\": 69.9074074074074, \"test_prim_bwd_acc_9\": 72.67080745341615, \"test_prim_bwd_acc_10\": 68.29268292682927, \"test_prim_bwd_acc_11\": 64.74820143884892, \"test_prim_bwd_acc_12\": 62.77372262773723, \"test_prim_bwd_acc_13\": 74.03846153846153, \"test_prim_bwd_acc_14\": 68.67469879518072, \"test_prim_bwd_acc_15\": 75.0, \"test_prim_bwd_acc_16\": 60.0, \"test_prim_bwd_acc_17\": 75.0, \"test_prim_bwd_acc_18\": 61.904761904761905, \"test_prim_bwd_acc_19\": 80.48780487804878, \"test_prim_bwd_acc_20\": 78.72340425531915, \"test_prim_bwd_acc_21\": 62.96296296296296, \"test_prim_bwd_acc_22\": 75.86206896551724, \"test_prim_bwd_acc_23\": 85.0, \"test_prim_bwd_acc_24\": 66.66666666666667, \"test_prim_bwd_acc_25\": 90.0, \"test_prim_bwd_acc_26\": 71.42857142857143, \"test_prim_bwd_acc_27\": 64.28571428571429, \"test_prim_bwd_acc_28\": 77.77777777777777, \"test_prim_bwd_acc_29\": 70.58823529411765, \"test_prim_bwd_acc_30\": 83.33333333333333, \"test_prim_bwd_acc_31\": 50.0, \"test_prim_bwd_acc_32\": 66.66666666666667, \"test_prim_bwd_acc_33\": 100.0, \"test_prim_bwd_acc_34\": 100.0, \"test_prim_bwd_acc_35\": 66.66666666666667, \"test_prim_bwd_acc_36\": 66.66666666666667, \"test_prim_bwd_acc_37\": 100.0, \"test_prim_bwd_acc_38\": 40.0, \"test_prim_bwd_acc_39\": 66.66666666666667, \"test_prim_bwd_acc_40\": 100.0, \"test_prim_bwd_acc_41\": 100.0, \"test_prim_bwd_acc_42\": 100.0, \"test_prim_bwd_acc_43\": 100.0, \"test_prim_bwd_acc_45\": 0.0, \"test_prim_bwd_acc_46\": 33.333333333333336, \"test_prim_bwd_acc_47\": 0.0, \"test_prim_bwd_acc_48\": 50.0, \"test_prim_bwd_acc_49\": 0.0, \"test_prim_bwd_acc_50\": 0.0, \"test_prim_bwd_acc_52\": 50.0, \"test_prim_bwd_acc_53\": 66.66666666666667, \"test_prim_bwd_acc_54\": 100.0, \"test_prim_bwd_acc_55\": 50.0, \"test_prim_bwd_acc_56\": 50.0, \"test_prim_bwd_acc_57\": 100.0, \"test_prim_bwd_acc_64\": 0.0, \"test_prim_bwd_acc_65\": 0.0, \"test_prim_bwd_acc_70\": 0.0, \"test_prim_bwd_acc_71\": 0.0, \"test_prim_bwd_acc_78\": 100.0, \"test_prim_bwd_acc_81\": 50.0, \"test_prim_bwd_acc_82\": 100.0, \"test_prim_bwd_acc_84\": 100.0, \"test_prim_bwd_acc_127\": 100.0, \"test_prim_bwd_acc_144\": 100.0}\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - New best score for valid_prim_bwd_acc: 71.414293\n",
            "INFO - 12/05/22 06:54:58 - 1:14:02 - Saving best-valid_prim_bwd_acc to ./dumped/deriv2fwd_modelbwd/6gswznivsf/best-valid_prim_bwd_acc.pth ...\n",
            "WARNING - 12/05/22 06:54:58 - 1:14:02 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:54:58 - 1:14:02 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:54:58 - 1:14:02 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:55:03 - 1:14:07 - Saving checkpoint to ./dumped/deriv2fwd_modelbwd/6gswznivsf/checkpoint.pth ...\n",
            "WARNING - 12/05/22 06:55:03 - 1:14:07 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:55:03 - 1:14:07 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:55:03 - 1:14:07 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:55:10 - 1:14:13 - ============ Starting epoch 4 ... ============\n",
            "INFO - 12/05/22 06:55:13 - 1:14:16 -   15640 -  128.24 equations/s -  5922.37 words/s - PRIM-BWD:  0.0419 - model LR: 1.0000e-04\n",
            "Traceback (most recent call last):\n",
            "  File \"./SymbolicMathematics/main.py\", line 232, in <module>\n",
            "    main(params)\n",
            "  File \"./SymbolicMathematics/main.py\", line 195, in main\n",
            "    trainer.enc_dec_step(task)\n",
            "  File \"/content/SymbolicMathematics/src/trainer.py\", line 469, in enc_dec_step\n",
            "    self.optimize(loss)\n",
            "  File \"/content/SymbolicMathematics/src/trainer.py\", line 213, in optimize\n",
            "    scaled_loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 396, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 173, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento y evaluación del modelo, tercer experimento\n",
        "\n",
        "Se entrenará el modelo considerando exactamente lo mismo del primer experimento, pero tomando el modelo de integrales entrenadas por fórmulas generadas por el método fordward."
      ],
      "metadata": {
        "id": "zmInhF5k_hzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f fwd.pth\n",
        "!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd.pth"
      ],
      "metadata": {
        "id": "aaQM49E6_5Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878324d6-01de-47c5-c628-f200f452d8be"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 06:55:18--  https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1849505077 (1.7G) [application/octet-stream]\n",
            "Saving to: ‘fwd.pth’\n",
            "\n",
            "fwd.pth             100%[===================>]   1.72G  58.1MB/s    in 31s     \n",
            "\n",
            "2022-12-05 06:55:50 (56.1 MB/s) - ‘fwd.pth’ saved [1849505077/1849505077]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 ./SymbolicMathematics/main.py --exp_name derivfwd_modelfwd --fp16 true --amp 2 --tasks \"prim_fwd\" --reload_model \"fwd.pth\" --reload_data \"prim_fwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test\" --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer \"adam,lr=0.0001\" --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_fwd_acc"
      ],
      "metadata": {
        "id": "QOX_EOQ7_0Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a68356-d9f6-4e4b-c7cf-df59044676b2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLURM job: False\n",
            "0 - Number of nodes: 1\n",
            "0 - Node ID        : 0\n",
            "0 - Local rank     : 0\n",
            "0 - Global rank    : 0\n",
            "0 - World size     : 1\n",
            "0 - GPUs per node  : 1\n",
            "0 - Master         : True\n",
            "0 - Multi-node     : False\n",
            "0 - Multi-GPU      : False\n",
            "0 - Hostname       : e1165a85fa0b\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - accumulate_gradients: 1\n",
            "                                     amp: 2\n",
            "                                     attention_dropout: 0\n",
            "                                     balanced: False\n",
            "                                     batch_size: 128\n",
            "                                     beam_early_stopping: True\n",
            "                                     beam_eval: False\n",
            "                                     beam_length_penalty: 1\n",
            "                                     beam_size: 1\n",
            "                                     clean_prefix_expr: True\n",
            "                                     clip_grad_norm: 5\n",
            "                                     command: python ./SymbolicMathematics/main.py --exp_name derivfwd_modelfwd --fp16 true --amp 2 --tasks prim_fwd --reload_model 'fwd.pth' --reload_data 'prim_fwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_fwd_acc --exp_id \"5k6dzaasp3\"\n",
            "                                     cpu: False\n",
            "                                     debug: False\n",
            "                                     debug_slurm: False\n",
            "                                     dropout: 0\n",
            "                                     dump_path: ./dumped/derivfwd_modelfwd/5k6dzaasp3\n",
            "                                     emb_dim: 1024\n",
            "                                     env_base_seed: 0\n",
            "                                     env_name: char_sp\n",
            "                                     epoch_size: 100000\n",
            "                                     eval_only: False\n",
            "                                     eval_verbose: 0\n",
            "                                     eval_verbose_print: False\n",
            "                                     exp_id: 5k6dzaasp3\n",
            "                                     exp_name: derivfwd_modelfwd\n",
            "                                     export_data: False\n",
            "                                     fp16: True\n",
            "                                     global_rank: 0\n",
            "                                     int_base: 10\n",
            "                                     is_master: True\n",
            "                                     is_slurm_job: False\n",
            "                                     leaf_probs: 0.75,0,0.25,0\n",
            "                                     local_rank: 0\n",
            "                                     master_port: -1\n",
            "                                     max_epoch: 100000\n",
            "                                     max_int: 10000\n",
            "                                     max_len: 512\n",
            "                                     max_ops: 10\n",
            "                                     max_ops_G: 4\n",
            "                                     multi_gpu: False\n",
            "                                     multi_node: False\n",
            "                                     n_coefficients: 0\n",
            "                                     n_dec_layers: 6\n",
            "                                     n_enc_layers: 6\n",
            "                                     n_gpu_per_node: 1\n",
            "                                     n_heads: 8\n",
            "                                     n_nodes: 1\n",
            "                                     n_variables: 1\n",
            "                                     node_id: 0\n",
            "                                     num_workers: 10\n",
            "                                     operators: add:2,sub:1\n",
            "                                     optimizer: adam,lr=0.0001\n",
            "                                     positive: False\n",
            "                                     precision: 10\n",
            "                                     reload_checkpoint: \n",
            "                                     reload_data: prim_fwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test\n",
            "                                     reload_model: fwd.pth\n",
            "                                     reload_size: 100000\n",
            "                                     rewrite_functions: \n",
            "                                     same_nb_ops_per_batch: False\n",
            "                                     save_periodic: 0\n",
            "                                     share_inout_emb: True\n",
            "                                     sinusoidal_embeddings: False\n",
            "                                     stopping_criterion: \n",
            "                                     tasks: prim_fwd\n",
            "                                     validation_metrics: valid_prim_fwd_acc\n",
            "                                     world_size: 1\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - The experiment will be stored in ./dumped/derivfwd_modelfwd/5k6dzaasp3\n",
            "                                     \n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - Running command: python ./SymbolicMathematics/main.py --exp_name derivfwd_modelfwd --fp16 true --amp 2 --tasks prim_fwd --reload_model 'fwd.pth' --reload_data 'prim_fwd,deriv_fwd.train,deriv_fwd.valid,deriv_fwd.test' --reload_size 100000 --emb_dim 1024 --n_enc_layers 6 --n_dec_layers 6 --n_heads 8 --optimizer 'adam,lr=0.0001' --batch_size 128 --epoch_size 100000 --validation_metrics valid_prim_fwd_acc\n",
            "\n",
            "WARNING - 12/05/22 06:56:07 - 0:00:00 - Signal handler installed.\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - Unary operators: []\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - Binary operators: ['add', 'sub']\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - words: {'<s>': 0, '</s>': 1, '<pad>': 2, '(': 3, ')': 4, '<SPECIAL_5>': 5, '<SPECIAL_6>': 6, '<SPECIAL_7>': 7, '<SPECIAL_8>': 8, '<SPECIAL_9>': 9, 'pi': 10, 'E': 11, 'x': 12, 'y': 13, 'z': 14, 't': 15, 'a0': 16, 'a1': 17, 'a2': 18, 'a3': 19, 'a4': 20, 'a5': 21, 'a6': 22, 'a7': 23, 'a8': 24, 'a9': 25, 'abs': 26, 'acos': 27, 'acosh': 28, 'acot': 29, 'acoth': 30, 'acsc': 31, 'acsch': 32, 'add': 33, 'asec': 34, 'asech': 35, 'asin': 36, 'asinh': 37, 'atan': 38, 'atanh': 39, 'cos': 40, 'cosh': 41, 'cot': 42, 'coth': 43, 'csc': 44, 'csch': 45, 'derivative': 46, 'div': 47, 'exp': 48, 'f': 49, 'g': 50, 'h': 51, 'inv': 52, 'ln': 53, 'mul': 54, 'pow': 55, 'pow2': 56, 'pow3': 57, 'pow4': 58, 'pow5': 59, 'rac': 60, 'sec': 61, 'sech': 62, 'sign': 63, 'sin': 64, 'sinh': 65, 'sqrt': 66, 'sub': 67, 'tan': 68, 'tanh': 69, 'I': 70, 'INT+': 71, 'INT-': 72, 'INT': 73, 'FLOAT': 74, '-': 75, '.': 76, '10^': 77, 'Y': 78, \"Y'\": 79, \"Y''\": 80, '0': 81, '1': 82, '2': 83, '3': 84, '4': 85, '5': 86, '6': 87, '7': 88, '8': 89, '9': 90}\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - 20001 possible leaves.\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - Checking expressions in [0.01, 0.1, 0.3, 0.5, 0.7, 0.9, 1.1, 2.1, 3.1, -0.01, -0.1, -0.3, -0.5, -0.7, -0.9, -1.1, -2.1, -3.1]\n",
            "INFO - 12/05/22 06:56:07 - 0:00:00 - Training tasks: prim_fwd\n",
            "INFO - 12/05/22 06:56:08 - 0:00:02 - Reloading modules from fwd.pth ...\n",
            "INFO - 12/05/22 06:56:11 - 0:00:04 - Number of parameters (encoder): 79866880\n",
            "INFO - 12/05/22 06:56:11 - 0:00:04 - Number of parameters (decoder): 105069659\n",
            "INFO - 12/05/22 06:56:11 - 0:00:04 - Found 261 parameters in model.\n",
            "INFO - 12/05/22 06:56:11 - 0:00:04 - Optimizers: model\n",
            "/usr/local/lib/python3.8/dist-packages/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)\n",
            "  warnings.warn(msg, DeprecatedFeatureWarning)\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "INFO - 12/05/22 06:56:11 - 0:00:05 - Creating train iterator for prim_fwd ...\n",
            "INFO - 12/05/22 06:56:11 - 0:00:05 - Loading data from deriv_fwd.train ...\n",
            "INFO - 12/05/22 06:56:11 - 0:00:05 - Loaded 100000 equations from the disk.\n",
            "INFO - 12/05/22 06:56:12 - 0:00:05 - ============ Starting epoch 0 ... ============\n",
            "INFO - 12/05/22 06:56:12 - 0:00:05 - Initialized random generator for worker 0, with seed [0, 0, 0] (base seed=0).\n",
            "/usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/content/SymbolicMathematics/src/optim.py:72: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "INFO - 12/05/22 06:56:18 - 0:00:11 -      20 -  384.28 equations/s - 15945.54 words/s - PRIM-FWD:  1.2341 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:22 - 0:00:16 -      40 -  564.79 equations/s - 23539.86 words/s - PRIM-FWD:  0.3378 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:27 - 0:00:20 -      60 -  571.28 equations/s - 23616.98 words/s - PRIM-FWD:  0.2271 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:32 - 0:00:26 -      80 -  479.06 equations/s - 20146.93 words/s - PRIM-FWD:  0.1783 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:37 - 0:00:30 -     100 -  551.16 equations/s - 23043.16 words/s - PRIM-FWD:  0.1504 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:42 - 0:00:35 -     120 -  522.77 equations/s - 22190.56 words/s - PRIM-FWD:  0.1464 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:46 - 0:00:39 -     140 -  597.19 equations/s - 24579.63 words/s - PRIM-FWD:  0.1328 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:51 - 0:00:44 -     160 -  556.97 equations/s - 23666.35 words/s - PRIM-FWD:  0.1250 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:56:55 - 0:00:49 -     180 -  564.80 equations/s - 23525.45 words/s - PRIM-FWD:  0.1159 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:01 - 0:00:54 -     200 -  467.71 equations/s - 19658.57 words/s - PRIM-FWD:  0.1132 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:04 - 0:00:58 -     220 -  685.70 equations/s - 27979.31 words/s - PRIM-FWD:  0.1019 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:10 - 0:01:03 -     240 -  456.58 equations/s - 19269.35 words/s - PRIM-FWD:  0.1018 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:15 - 0:01:08 -     260 -  550.83 equations/s - 22895.88 words/s - PRIM-FWD:  0.0946 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:19 - 0:01:13 -     280 -  530.59 equations/s - 22062.30 words/s - PRIM-FWD:  0.0917 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:24 - 0:01:17 -     300 -  621.71 equations/s - 25525.89 words/s - PRIM-FWD:  0.0944 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:28 - 0:01:22 -     320 -  552.02 equations/s - 22831.01 words/s - PRIM-FWD:  0.0852 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:33 - 0:01:26 -     340 -  561.55 equations/s - 23405.39 words/s - PRIM-FWD:  0.0873 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:38 - 0:01:31 -     360 -  478.98 equations/s - 20153.06 words/s - PRIM-FWD:  0.0902 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:43 - 0:01:36 -     380 -  572.37 equations/s - 23994.25 words/s - PRIM-FWD:  0.0846 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:47 - 0:01:40 -     400 -  575.45 equations/s - 23897.58 words/s - PRIM-FWD:  0.0840 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:52 - 0:01:45 -     420 -  541.00 equations/s - 22306.03 words/s - PRIM-FWD:  0.0782 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:57:56 - 0:01:49 -     440 -  620.65 equations/s - 25757.99 words/s - PRIM-FWD:  0.0814 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:02 - 0:01:55 -     460 -  438.35 equations/s - 18237.93 words/s - PRIM-FWD:  0.0768 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:07 - 0:02:00 -     480 -  480.05 equations/s - 19929.02 words/s - PRIM-FWD:  0.0831 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:12 - 0:02:05 -     500 -  550.90 equations/s - 22983.11 words/s - PRIM-FWD:  0.0766 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:16 - 0:02:10 -     520 -  542.05 equations/s - 22615.31 words/s - PRIM-FWD:  0.0801 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:21 - 0:02:15 -     540 -  521.73 equations/s - 21813.14 words/s - PRIM-FWD:  0.0756 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:26 - 0:02:19 -     560 -  560.01 equations/s - 23506.07 words/s - PRIM-FWD:  0.0749 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:30 - 0:02:24 -     580 -  576.02 equations/s - 23823.29 words/s - PRIM-FWD:  0.0717 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:35 - 0:02:29 -     600 -  531.03 equations/s - 22047.86 words/s - PRIM-FWD:  0.0711 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:40 - 0:02:34 -     620 -  512.14 equations/s - 21830.50 words/s - PRIM-FWD:  0.0717 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:45 - 0:02:38 -     640 -  558.39 equations/s - 23536.49 words/s - PRIM-FWD:  0.0670 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:49 - 0:02:42 -     660 -  625.07 equations/s - 25879.99 words/s - PRIM-FWD:  0.0693 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:54 - 0:02:47 -     680 -  511.75 equations/s - 21205.27 words/s - PRIM-FWD:  0.0704 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:58:58 - 0:02:52 -     700 -  572.46 equations/s - 23962.93 words/s - PRIM-FWD:  0.0636 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:03 - 0:02:56 -     720 -  564.69 equations/s - 23590.32 words/s - PRIM-FWD:  0.0647 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:07 - 0:03:00 -     740 -  652.05 equations/s - 26737.11 words/s - PRIM-FWD:  0.0646 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:12 - 0:03:05 -     760 -  542.47 equations/s - 22457.89 words/s - PRIM-FWD:  0.0637 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:16 - 0:03:10 -     780 -  533.28 equations/s - 22008.83 words/s - PRIM-FWD:  0.0664 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - ============ End of epoch 0 ============\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 0/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 128/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 256/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 384/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 512/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:10 - 640/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:11 - 768/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:11 - 896/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:11 - 1024/2001\n",
            "INFO - 12/05/22 06:59:17 - 0:03:11 - 1152/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1280/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1408/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1536/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1664/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1792/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:11 - 1920/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - 1168/2001 (58.370814592703645%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - 0/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - 128/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - 256/2001\n",
            "INFO - 12/05/22 06:59:18 - 0:03:12 - 384/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 512/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 640/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 768/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 896/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 1024/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 1152/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:12 - 1280/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:13 - 1408/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:13 - 1536/2001\n",
            "INFO - 12/05/22 06:59:19 - 0:03:13 - 1664/2001\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - 1792/2001\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - 1920/2001\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - 1172/2001 (58.57071464267866%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - epoch -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_xe_loss -> 3.241259\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc -> 58.370815\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_4 -> 79.069767\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_6 -> 65.909091\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_7 -> 68.211921\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_8 -> 64.806867\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_9 -> 68.823529\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_10 -> 57.142857\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_11 -> 63.358779\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_12 -> 53.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_13 -> 57.894737\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_14 -> 53.773585\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_15 -> 58.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_16 -> 51.351351\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_17 -> 48.837209\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_18 -> 56.521739\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_19 -> 56.818182\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_20 -> 43.750000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_21 -> 60.869565\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_22 -> 42.857143\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_23 -> 36.363636\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_24 -> 20.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_25 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_26 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_27 -> 46.153846\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_29 -> 27.272727\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_30 -> 11.111111\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_32 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_33 -> 25.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_34 -> 75.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_36 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_40 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_xe_loss -> 3.462938\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc -> 58.570715\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_4 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_5 -> 68.421053\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_6 -> 73.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_8 -> 67.592593\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_9 -> 62.732919\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_10 -> 67.073171\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_11 -> 52.517986\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_12 -> 56.204380\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_13 -> 58.653846\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_14 -> 59.036145\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_15 -> 48.437500\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_16 -> 45.714286\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_17 -> 54.545455\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_18 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_19 -> 56.097561\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_20 -> 61.702128\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_21 -> 48.148148\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_22 -> 44.827586\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_23 -> 65.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_24 -> 38.095238\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_25 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_26 -> 42.857143\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_27 -> 35.714286\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_28 -> 22.222222\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_29 -> 17.647059\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_31 -> 12.500000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_33 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_34 -> 40.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_35 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_36 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_37 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_41 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_43 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_53 -> 33.333333\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - __log__:{\"epoch\": 0, \"valid_prim_fwd_xe_loss\": 3.24125922268358, \"valid_prim_fwd_acc\": 58.370814592703645, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 80.0, \"valid_prim_fwd_acc_4\": 79.06976744186046, \"valid_prim_fwd_acc_5\": 66.19718309859155, \"valid_prim_fwd_acc_6\": 65.9090909090909, \"valid_prim_fwd_acc_7\": 68.21192052980132, \"valid_prim_fwd_acc_8\": 64.8068669527897, \"valid_prim_fwd_acc_9\": 68.82352941176471, \"valid_prim_fwd_acc_10\": 57.142857142857146, \"valid_prim_fwd_acc_11\": 63.3587786259542, \"valid_prim_fwd_acc_12\": 53.333333333333336, \"valid_prim_fwd_acc_13\": 57.89473684210526, \"valid_prim_fwd_acc_14\": 53.77358490566038, \"valid_prim_fwd_acc_15\": 58.333333333333336, \"valid_prim_fwd_acc_16\": 51.351351351351354, \"valid_prim_fwd_acc_17\": 48.83720930232558, \"valid_prim_fwd_acc_18\": 56.52173913043478, \"valid_prim_fwd_acc_19\": 56.81818181818182, \"valid_prim_fwd_acc_20\": 43.75, \"valid_prim_fwd_acc_21\": 60.869565217391305, \"valid_prim_fwd_acc_22\": 42.857142857142854, \"valid_prim_fwd_acc_23\": 36.36363636363637, \"valid_prim_fwd_acc_24\": 20.0, \"valid_prim_fwd_acc_25\": 33.333333333333336, \"valid_prim_fwd_acc_26\": 66.66666666666667, \"valid_prim_fwd_acc_27\": 46.15384615384615, \"valid_prim_fwd_acc_28\": 53.84615384615385, \"valid_prim_fwd_acc_29\": 27.272727272727273, \"valid_prim_fwd_acc_30\": 11.11111111111111, \"valid_prim_fwd_acc_31\": 66.66666666666667, \"valid_prim_fwd_acc_32\": 0.0, \"valid_prim_fwd_acc_33\": 25.0, \"valid_prim_fwd_acc_34\": 75.0, \"valid_prim_fwd_acc_35\": 50.0, \"valid_prim_fwd_acc_36\": 50.0, \"valid_prim_fwd_acc_37\": 75.0, \"valid_prim_fwd_acc_39\": 0.0, \"valid_prim_fwd_acc_40\": 66.66666666666667, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 0.0, \"valid_prim_fwd_acc_45\": 0.0, \"valid_prim_fwd_acc_46\": 0.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 0.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 0.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 0.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 50.0, \"valid_prim_fwd_acc_103\": 0.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 3.462937964611444, \"test_prim_fwd_acc\": 58.57071464267866, \"test_prim_fwd_acc_3\": 66.66666666666667, \"test_prim_fwd_acc_4\": 66.66666666666667, \"test_prim_fwd_acc_5\": 68.42105263157895, \"test_prim_fwd_acc_6\": 73.33333333333333, \"test_prim_fwd_acc_7\": 71.73913043478261, \"test_prim_fwd_acc_8\": 67.5925925925926, \"test_prim_fwd_acc_9\": 62.732919254658384, \"test_prim_fwd_acc_10\": 67.07317073170732, \"test_prim_fwd_acc_11\": 52.51798561151079, \"test_prim_fwd_acc_12\": 56.2043795620438, \"test_prim_fwd_acc_13\": 58.65384615384615, \"test_prim_fwd_acc_14\": 59.036144578313255, \"test_prim_fwd_acc_15\": 48.4375, \"test_prim_fwd_acc_16\": 45.714285714285715, \"test_prim_fwd_acc_17\": 54.54545454545455, \"test_prim_fwd_acc_18\": 50.0, \"test_prim_fwd_acc_19\": 56.09756097560975, \"test_prim_fwd_acc_20\": 61.702127659574465, \"test_prim_fwd_acc_21\": 48.148148148148145, \"test_prim_fwd_acc_22\": 44.827586206896555, \"test_prim_fwd_acc_23\": 65.0, \"test_prim_fwd_acc_24\": 38.095238095238095, \"test_prim_fwd_acc_25\": 50.0, \"test_prim_fwd_acc_26\": 42.857142857142854, \"test_prim_fwd_acc_27\": 35.714285714285715, \"test_prim_fwd_acc_28\": 22.22222222222222, \"test_prim_fwd_acc_29\": 17.647058823529413, \"test_prim_fwd_acc_30\": 66.66666666666667, \"test_prim_fwd_acc_31\": 12.5, \"test_prim_fwd_acc_32\": 33.333333333333336, \"test_prim_fwd_acc_33\": 33.333333333333336, \"test_prim_fwd_acc_34\": 40.0, \"test_prim_fwd_acc_35\": 0.0, \"test_prim_fwd_acc_36\": 33.333333333333336, \"test_prim_fwd_acc_37\": 0.0, \"test_prim_fwd_acc_38\": 20.0, \"test_prim_fwd_acc_39\": 0.0, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 0.0, \"test_prim_fwd_acc_42\": 0.0, \"test_prim_fwd_acc_43\": 33.333333333333336, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 33.333333333333336, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 33.333333333333336, \"test_prim_fwd_acc_54\": 0.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - New best score for valid_prim_fwd_acc: 58.370815\n",
            "INFO - 12/05/22 06:59:20 - 0:03:13 - Saving best-valid_prim_fwd_acc to ./dumped/derivfwd_modelfwd/5k6dzaasp3/best-valid_prim_fwd_acc.pth ...\n",
            "WARNING - 12/05/22 06:59:20 - 0:03:13 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:59:20 - 0:03:13 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:59:20 - 0:03:13 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:59:24 - 0:03:17 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 06:59:24 - 0:03:17 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 06:59:24 - 0:03:17 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 06:59:24 - 0:03:17 - Saving model optimizer ...\n",
            "INFO - 12/05/22 06:59:30 - 0:03:24 - ============ Starting epoch 1 ... ============\n",
            "INFO - 12/05/22 06:59:35 - 0:03:28 -     800 -  139.88 equations/s -  5850.26 words/s - PRIM-FWD:  0.0628 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:39 - 0:03:32 -     820 -  617.25 equations/s - 25776.39 words/s - PRIM-FWD:  0.0640 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:43 - 0:03:37 -     840 -  545.04 equations/s - 22533.82 words/s - PRIM-FWD:  0.0646 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:49 - 0:03:42 -     860 -  480.07 equations/s - 20069.99 words/s - PRIM-FWD:  0.0650 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:54 - 0:03:47 -     880 -  523.13 equations/s - 21742.09 words/s - PRIM-FWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 06:59:59 - 0:03:53 -     900 -  462.38 equations/s - 19567.95 words/s - PRIM-FWD:  0.0616 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:04 - 0:03:57 -     920 -  563.67 equations/s - 23348.18 words/s - PRIM-FWD:  0.0577 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:09 - 0:04:03 -     940 -  460.66 equations/s - 19454.50 words/s - PRIM-FWD:  0.0569 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:14 - 0:04:07 -     960 -  551.39 equations/s - 23123.74 words/s - PRIM-FWD:  0.0585 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:18 - 0:04:11 -     980 -  616.68 equations/s - 25487.28 words/s - PRIM-FWD:  0.0590 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:24 - 0:04:17 -    1000 -  471.88 equations/s - 19595.83 words/s - PRIM-FWD:  0.0604 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:28 - 0:04:21 -    1020 -  560.60 equations/s - 23289.48 words/s - PRIM-FWD:  0.0614 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:33 - 0:04:27 -    1040 -  497.19 equations/s - 21074.15 words/s - PRIM-FWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:38 - 0:04:31 -    1060 -  528.40 equations/s - 21928.50 words/s - PRIM-FWD:  0.0576 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:43 - 0:04:36 -    1080 -  537.78 equations/s - 22582.46 words/s - PRIM-FWD:  0.0575 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:47 - 0:04:41 -    1100 -  578.20 equations/s - 23865.28 words/s - PRIM-FWD:  0.0579 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:52 - 0:04:45 -    1120 -  609.75 equations/s - 25310.82 words/s - PRIM-FWD:  0.0587 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:00:56 - 0:04:50 -    1140 -  549.04 equations/s - 22897.44 words/s - PRIM-FWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:01 - 0:04:54 -    1160 -  517.45 equations/s - 21607.41 words/s - PRIM-FWD:  0.0540 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:06 - 0:04:59 -    1180 -  538.27 equations/s - 22272.62 words/s - PRIM-FWD:  0.0551 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:10 - 0:05:04 -    1200 -  578.13 equations/s - 24195.74 words/s - PRIM-FWD:  0.0586 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:15 - 0:05:08 -    1220 -  558.73 equations/s - 23343.09 words/s - PRIM-FWD:  0.0533 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:20 - 0:05:13 -    1240 -  549.44 equations/s - 23035.32 words/s - PRIM-FWD:  0.0553 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:25 - 0:05:18 -    1260 -  484.14 equations/s - 20631.06 words/s - PRIM-FWD:  0.0532 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:29 - 0:05:23 -    1280 -  567.55 equations/s - 23512.62 words/s - PRIM-FWD:  0.0555 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:34 - 0:05:27 -    1300 -  570.92 equations/s - 23916.93 words/s - PRIM-FWD:  0.0503 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:38 - 0:05:31 -    1320 -  632.75 equations/s - 26412.25 words/s - PRIM-FWD:  0.0504 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:42 - 0:05:35 -    1340 -  604.62 equations/s - 25082.41 words/s - PRIM-FWD:  0.0549 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:47 - 0:05:40 -    1360 -  566.08 equations/s - 23759.14 words/s - PRIM-FWD:  0.0528 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:51 - 0:05:44 -    1380 -  590.83 equations/s - 24534.36 words/s - PRIM-FWD:  0.0547 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:01:56 - 0:05:49 -    1400 -  516.61 equations/s - 21735.95 words/s - PRIM-FWD:  0.0531 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:01 - 0:05:54 -    1420 -  535.55 equations/s - 22763.75 words/s - PRIM-FWD:  0.0523 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:05 - 0:05:58 -    1440 -  612.94 equations/s - 25274.14 words/s - PRIM-FWD:  0.0539 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:09 - 0:06:03 -    1460 -  596.05 equations/s - 24768.37 words/s - PRIM-FWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:14 - 0:06:07 -    1480 -  583.13 equations/s - 24261.93 words/s - PRIM-FWD:  0.0504 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:18 - 0:06:12 -    1500 -  544.42 equations/s - 22459.82 words/s - PRIM-FWD:  0.0535 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:23 - 0:06:16 -    1520 -  568.47 equations/s - 23595.69 words/s - PRIM-FWD:  0.0545 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:28 - 0:06:21 -    1540 -  488.28 equations/s - 20615.40 words/s - PRIM-FWD:  0.0525 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:32 - 0:06:25 -    1560 -  647.95 equations/s - 26846.66 words/s - PRIM-FWD:  0.0454 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - ============ End of epoch 1 ============\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - 0/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - 128/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - 256/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - 384/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:26 - 512/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:27 - 640/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:27 - 768/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:27 - 896/2001\n",
            "INFO - 12/05/22 07:02:33 - 0:06:27 - 1024/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1152/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1280/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1408/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1536/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1664/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1792/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:27 - 1920/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - 1192/2001 (59.57021489255372%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - 0/2001\n",
            "INFO - 12/05/22 07:02:34 - 0:06:28 - 128/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 256/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 384/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 512/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 640/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 768/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 896/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 1024/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 1152/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:28 - 1280/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:29 - 1408/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:29 - 1536/2001\n",
            "INFO - 12/05/22 07:02:35 - 0:06:29 - 1664/2001\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - 1792/2001\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - 1920/2001\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - 1200/2001 (59.97001499250375%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - epoch -> 1.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_xe_loss -> 3.225889\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc -> 59.570215\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_4 -> 79.069767\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_5 -> 67.605634\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_6 -> 66.666667\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_7 -> 70.860927\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_8 -> 65.665236\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_9 -> 67.647059\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_10 -> 57.142857\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_11 -> 59.541985\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_12 -> 58.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_13 -> 55.789474\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_14 -> 55.660377\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_15 -> 61.111111\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_16 -> 52.702703\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_17 -> 51.162791\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_18 -> 58.695652\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_19 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_20 -> 46.875000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_21 -> 65.217391\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_22 -> 38.095238\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_23 -> 54.545455\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_24 -> 20.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_25 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_27 -> 46.153846\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_28 -> 46.153846\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_29 -> 36.363636\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_30 -> 44.444444\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_31 -> 83.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_32 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_34 -> 75.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_35 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_37 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_40 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_45 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_xe_loss -> 3.396894\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc -> 59.970015\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_4 -> 64.444444\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_5 -> 71.929825\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_6 -> 76.296296\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_8 -> 67.129630\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_9 -> 61.490683\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_10 -> 65.243902\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_11 -> 56.834532\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_12 -> 51.824818\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_13 -> 64.423077\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_14 -> 57.831325\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_15 -> 45.312500\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_16 -> 48.571429\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_17 -> 56.818182\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_18 -> 54.761905\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_19 -> 60.975610\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_20 -> 68.085106\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_21 -> 40.740741\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_22 -> 62.068966\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_23 -> 60.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_24 -> 28.571429\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_25 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_26 -> 42.857143\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_27 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_28 -> 44.444444\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_29 -> 35.294118\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_31 -> 12.500000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_33 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_34 -> 40.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_35 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_37 -> 40.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_38 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_42 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_53 -> 33.333333\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_54 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - __log__:{\"epoch\": 1, \"valid_prim_fwd_xe_loss\": 3.2258887493509105, \"valid_prim_fwd_acc\": 59.57021489255372, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 70.0, \"valid_prim_fwd_acc_4\": 79.06976744186046, \"valid_prim_fwd_acc_5\": 67.6056338028169, \"valid_prim_fwd_acc_6\": 66.66666666666667, \"valid_prim_fwd_acc_7\": 70.86092715231788, \"valid_prim_fwd_acc_8\": 65.66523605150215, \"valid_prim_fwd_acc_9\": 67.6470588235294, \"valid_prim_fwd_acc_10\": 57.142857142857146, \"valid_prim_fwd_acc_11\": 59.541984732824424, \"valid_prim_fwd_acc_12\": 58.0, \"valid_prim_fwd_acc_13\": 55.78947368421053, \"valid_prim_fwd_acc_14\": 55.660377358490564, \"valid_prim_fwd_acc_15\": 61.111111111111114, \"valid_prim_fwd_acc_16\": 52.7027027027027, \"valid_prim_fwd_acc_17\": 51.16279069767442, \"valid_prim_fwd_acc_18\": 58.69565217391305, \"valid_prim_fwd_acc_19\": 50.0, \"valid_prim_fwd_acc_20\": 46.875, \"valid_prim_fwd_acc_21\": 65.21739130434783, \"valid_prim_fwd_acc_22\": 38.095238095238095, \"valid_prim_fwd_acc_23\": 54.54545454545455, \"valid_prim_fwd_acc_24\": 20.0, \"valid_prim_fwd_acc_25\": 50.0, \"valid_prim_fwd_acc_26\": 60.0, \"valid_prim_fwd_acc_27\": 46.15384615384615, \"valid_prim_fwd_acc_28\": 46.15384615384615, \"valid_prim_fwd_acc_29\": 36.36363636363637, \"valid_prim_fwd_acc_30\": 44.44444444444444, \"valid_prim_fwd_acc_31\": 83.33333333333333, \"valid_prim_fwd_acc_32\": 0.0, \"valid_prim_fwd_acc_33\": 50.0, \"valid_prim_fwd_acc_34\": 75.0, \"valid_prim_fwd_acc_35\": 100.0, \"valid_prim_fwd_acc_36\": 100.0, \"valid_prim_fwd_acc_37\": 50.0, \"valid_prim_fwd_acc_39\": 0.0, \"valid_prim_fwd_acc_40\": 33.333333333333336, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 0.0, \"valid_prim_fwd_acc_45\": 33.333333333333336, \"valid_prim_fwd_acc_46\": 100.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 0.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 100.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 0.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 50.0, \"valid_prim_fwd_acc_103\": 0.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 3.3968944482836707, \"test_prim_fwd_acc\": 59.97001499250375, \"test_prim_fwd_acc_3\": 66.66666666666667, \"test_prim_fwd_acc_4\": 64.44444444444444, \"test_prim_fwd_acc_5\": 71.9298245614035, \"test_prim_fwd_acc_6\": 76.29629629629629, \"test_prim_fwd_acc_7\": 71.19565217391305, \"test_prim_fwd_acc_8\": 67.12962962962963, \"test_prim_fwd_acc_9\": 61.49068322981366, \"test_prim_fwd_acc_10\": 65.2439024390244, \"test_prim_fwd_acc_11\": 56.83453237410072, \"test_prim_fwd_acc_12\": 51.824817518248175, \"test_prim_fwd_acc_13\": 64.42307692307692, \"test_prim_fwd_acc_14\": 57.83132530120482, \"test_prim_fwd_acc_15\": 45.3125, \"test_prim_fwd_acc_16\": 48.57142857142857, \"test_prim_fwd_acc_17\": 56.81818181818182, \"test_prim_fwd_acc_18\": 54.76190476190476, \"test_prim_fwd_acc_19\": 60.97560975609756, \"test_prim_fwd_acc_20\": 68.08510638297872, \"test_prim_fwd_acc_21\": 40.74074074074074, \"test_prim_fwd_acc_22\": 62.06896551724138, \"test_prim_fwd_acc_23\": 60.0, \"test_prim_fwd_acc_24\": 28.571428571428573, \"test_prim_fwd_acc_25\": 50.0, \"test_prim_fwd_acc_26\": 42.857142857142854, \"test_prim_fwd_acc_27\": 50.0, \"test_prim_fwd_acc_28\": 44.44444444444444, \"test_prim_fwd_acc_29\": 35.294117647058826, \"test_prim_fwd_acc_30\": 83.33333333333333, \"test_prim_fwd_acc_31\": 12.5, \"test_prim_fwd_acc_32\": 50.0, \"test_prim_fwd_acc_33\": 33.333333333333336, \"test_prim_fwd_acc_34\": 40.0, \"test_prim_fwd_acc_35\": 33.333333333333336, \"test_prim_fwd_acc_36\": 100.0, \"test_prim_fwd_acc_37\": 40.0, \"test_prim_fwd_acc_38\": 0.0, \"test_prim_fwd_acc_39\": 0.0, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 50.0, \"test_prim_fwd_acc_42\": 50.0, \"test_prim_fwd_acc_43\": 66.66666666666667, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 33.333333333333336, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 33.333333333333336, \"test_prim_fwd_acc_54\": 50.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - New best score for valid_prim_fwd_acc: 59.570215\n",
            "INFO - 12/05/22 07:02:36 - 0:06:29 - Saving best-valid_prim_fwd_acc to ./dumped/derivfwd_modelfwd/5k6dzaasp3/best-valid_prim_fwd_acc.pth ...\n",
            "WARNING - 12/05/22 07:02:36 - 0:06:29 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:02:36 - 0:06:29 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:02:36 - 0:06:29 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:02:41 - 0:06:34 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 07:02:41 - 0:06:34 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:02:41 - 0:06:34 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:02:41 - 0:06:34 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:02:49 - 0:06:42 - ============ Starting epoch 2 ... ============\n",
            "INFO - 12/05/22 07:02:52 - 0:06:46 -    1580 -  126.30 equations/s -  5245.61 words/s - PRIM-FWD:  0.0508 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:02:57 - 0:06:50 -    1600 -  557.26 equations/s - 23353.10 words/s - PRIM-FWD:  0.0510 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:01 - 0:06:55 -    1620 -  552.88 equations/s - 23128.60 words/s - PRIM-FWD:  0.0544 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:06 - 0:06:59 -    1640 -  549.03 equations/s - 22795.27 words/s - PRIM-FWD:  0.0491 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:11 - 0:07:04 -    1660 -  557.22 equations/s - 22989.32 words/s - PRIM-FWD:  0.0465 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:15 - 0:07:08 -    1680 -  649.71 equations/s - 26342.58 words/s - PRIM-FWD:  0.0492 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:20 - 0:07:13 -    1700 -  472.35 equations/s - 20100.24 words/s - PRIM-FWD:  0.0460 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:25 - 0:07:18 -    1720 -  536.29 equations/s - 22409.51 words/s - PRIM-FWD:  0.0488 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:29 - 0:07:23 -    1740 -  575.46 equations/s - 24057.71 words/s - PRIM-FWD:  0.0507 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:34 - 0:07:27 -    1760 -  595.60 equations/s - 24501.83 words/s - PRIM-FWD:  0.0513 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:38 - 0:07:32 -    1780 -  540.66 equations/s - 22503.85 words/s - PRIM-FWD:  0.0489 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:42 - 0:07:36 -    1800 -  619.05 equations/s - 25835.48 words/s - PRIM-FWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:47 - 0:07:40 -    1820 -  548.21 equations/s - 22937.77 words/s - PRIM-FWD:  0.0451 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:52 - 0:07:45 -    1840 -  561.21 equations/s - 23383.81 words/s - PRIM-FWD:  0.0475 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:03:56 - 0:07:49 -    1860 -  618.20 equations/s - 25399.90 words/s - PRIM-FWD:  0.0505 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:00 - 0:07:53 -    1880 -  593.26 equations/s - 24883.70 words/s - PRIM-FWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:05 - 0:07:58 -    1900 -  581.18 equations/s - 23846.50 words/s - PRIM-FWD:  0.0484 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:10 - 0:08:03 -    1920 -  494.84 equations/s - 20786.74 words/s - PRIM-FWD:  0.0437 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:14 - 0:08:08 -    1940 -  547.37 equations/s - 22470.27 words/s - PRIM-FWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:19 - 0:08:12 -    1960 -  540.43 equations/s - 22518.75 words/s - PRIM-FWD:  0.0480 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:24 - 0:08:17 -    1980 -  511.29 equations/s - 21414.56 words/s - PRIM-FWD:  0.0490 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:29 - 0:08:22 -    2000 -  585.47 equations/s - 24557.23 words/s - PRIM-FWD:  0.0470 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:33 - 0:08:26 -    2020 -  579.06 equations/s - 24057.32 words/s - PRIM-FWD:  0.0447 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:38 - 0:08:31 -    2040 -  494.22 equations/s - 20721.92 words/s - PRIM-FWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:42 - 0:08:36 -    2060 -  606.88 equations/s - 25273.43 words/s - PRIM-FWD:  0.0494 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:47 - 0:08:40 -    2080 -  584.92 equations/s - 23868.70 words/s - PRIM-FWD:  0.0464 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:52 - 0:08:45 -    2100 -  504.86 equations/s - 21065.25 words/s - PRIM-FWD:  0.0457 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:04:56 - 0:08:50 -    2120 -  575.36 equations/s - 24353.70 words/s - PRIM-FWD:  0.0462 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:01 - 0:08:55 -    2140 -  515.07 equations/s - 21544.95 words/s - PRIM-FWD:  0.0446 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:06 - 0:08:59 -    2160 -  558.53 equations/s - 23144.92 words/s - PRIM-FWD:  0.0468 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:10 - 0:09:04 -    2180 -  562.88 equations/s - 23476.93 words/s - PRIM-FWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:15 - 0:09:08 -    2200 -  548.62 equations/s - 22837.11 words/s - PRIM-FWD:  0.0470 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:19 - 0:09:13 -    2220 -  610.95 equations/s - 25362.24 words/s - PRIM-FWD:  0.0445 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:24 - 0:09:17 -    2240 -  534.26 equations/s - 22500.81 words/s - PRIM-FWD:  0.0486 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:29 - 0:09:22 -    2260 -  564.98 equations/s - 23542.17 words/s - PRIM-FWD:  0.0466 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:33 - 0:09:26 -    2280 -  623.67 equations/s - 25619.31 words/s - PRIM-FWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:37 - 0:09:30 -    2300 -  633.49 equations/s - 26169.46 words/s - PRIM-FWD:  0.0453 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:42 - 0:09:35 -    2320 -  520.30 equations/s - 21744.83 words/s - PRIM-FWD:  0.0476 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:45 - 0:09:39 -    2340 -  685.60 equations/s - 27848.14 words/s - PRIM-FWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - ============ End of epoch 2 ============\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - 0/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:40 - 128/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:41 - 256/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:41 - 384/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:41 - 512/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:41 - 640/2001\n",
            "INFO - 12/05/22 07:05:47 - 0:09:41 - 768/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 896/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 1024/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 1152/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 1280/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 1408/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:41 - 1536/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - 1664/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - 1792/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - 1920/2001\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - 1223/2001 (61.11944027986007%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:05:48 - 0:09:42 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 0/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 128/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 256/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 384/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 512/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 640/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 768/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 896/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:42 - 1024/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:43 - 1152/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:43 - 1280/2001\n",
            "INFO - 12/05/22 07:05:49 - 0:09:43 - 1408/2001\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - 1536/2001\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - 1664/2001\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - 1792/2001\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - 1920/2001\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - 1217/2001 (60.81959020489755%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - epoch -> 2.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_xe_loss -> 3.309892\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc -> 61.119440\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_5 -> 67.605634\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_6 -> 69.696970\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_7 -> 72.847682\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_8 -> 67.381974\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_9 -> 69.411765\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_10 -> 59.627329\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_11 -> 67.175573\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_12 -> 52.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_13 -> 58.947368\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_14 -> 59.433962\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_15 -> 56.944444\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_16 -> 52.702703\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_17 -> 53.488372\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_18 -> 56.521739\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_19 -> 47.727273\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_20 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_21 -> 56.521739\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_22 -> 52.380952\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_23 -> 54.545455\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_24 -> 25.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_25 -> 38.888889\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_27 -> 46.153846\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_28 -> 76.923077\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_29 -> 36.363636\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_30 -> 44.444444\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_31 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_36 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_40 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_45 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_103 -> 100.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_xe_loss -> 3.551303\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc -> 60.819590\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_4 -> 62.222222\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_5 -> 75.438596\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_6 -> 74.814815\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_7 -> 70.652174\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_8 -> 68.055556\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_9 -> 63.354037\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_10 -> 70.731707\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_11 -> 50.359712\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_12 -> 59.124088\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_13 -> 61.538462\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_14 -> 57.831325\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_15 -> 48.437500\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_16 -> 47.142857\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_17 -> 56.818182\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_18 -> 52.380952\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_19 -> 63.414634\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_20 -> 65.957447\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_21 -> 55.555556\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_22 -> 58.620690\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_23 -> 65.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_24 -> 38.095238\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_25 -> 60.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_26 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_27 -> 57.142857\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_28 -> 33.333333\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_29 -> 23.529412\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_31 -> 12.500000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_34 -> 40.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_37 -> 60.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_41 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_43 -> 33.333333\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_53 -> 66.666667\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - __log__:{\"epoch\": 2, \"valid_prim_fwd_xe_loss\": 3.3098920052912995, \"valid_prim_fwd_acc\": 61.11944027986007, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 80.0, \"valid_prim_fwd_acc_4\": 74.4186046511628, \"valid_prim_fwd_acc_5\": 67.6056338028169, \"valid_prim_fwd_acc_6\": 69.6969696969697, \"valid_prim_fwd_acc_7\": 72.8476821192053, \"valid_prim_fwd_acc_8\": 67.38197424892704, \"valid_prim_fwd_acc_9\": 69.41176470588235, \"valid_prim_fwd_acc_10\": 59.62732919254658, \"valid_prim_fwd_acc_11\": 67.17557251908397, \"valid_prim_fwd_acc_12\": 52.666666666666664, \"valid_prim_fwd_acc_13\": 58.94736842105263, \"valid_prim_fwd_acc_14\": 59.43396226415094, \"valid_prim_fwd_acc_15\": 56.94444444444444, \"valid_prim_fwd_acc_16\": 52.7027027027027, \"valid_prim_fwd_acc_17\": 53.48837209302326, \"valid_prim_fwd_acc_18\": 56.52173913043478, \"valid_prim_fwd_acc_19\": 47.72727272727273, \"valid_prim_fwd_acc_20\": 50.0, \"valid_prim_fwd_acc_21\": 56.52173913043478, \"valid_prim_fwd_acc_22\": 52.38095238095238, \"valid_prim_fwd_acc_23\": 54.54545454545455, \"valid_prim_fwd_acc_24\": 25.0, \"valid_prim_fwd_acc_25\": 38.888888888888886, \"valid_prim_fwd_acc_26\": 60.0, \"valid_prim_fwd_acc_27\": 46.15384615384615, \"valid_prim_fwd_acc_28\": 76.92307692307692, \"valid_prim_fwd_acc_29\": 36.36363636363637, \"valid_prim_fwd_acc_30\": 44.44444444444444, \"valid_prim_fwd_acc_31\": 66.66666666666667, \"valid_prim_fwd_acc_32\": 33.333333333333336, \"valid_prim_fwd_acc_33\": 50.0, \"valid_prim_fwd_acc_34\": 100.0, \"valid_prim_fwd_acc_35\": 50.0, \"valid_prim_fwd_acc_36\": 50.0, \"valid_prim_fwd_acc_37\": 75.0, \"valid_prim_fwd_acc_39\": 50.0, \"valid_prim_fwd_acc_40\": 66.66666666666667, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 0.0, \"valid_prim_fwd_acc_45\": 66.66666666666667, \"valid_prim_fwd_acc_46\": 100.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 0.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 0.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 0.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 50.0, \"valid_prim_fwd_acc_103\": 100.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 3.5513032668236195, \"test_prim_fwd_acc\": 60.81959020489755, \"test_prim_fwd_acc_3\": 66.66666666666667, \"test_prim_fwd_acc_4\": 62.22222222222222, \"test_prim_fwd_acc_5\": 75.43859649122807, \"test_prim_fwd_acc_6\": 74.81481481481481, \"test_prim_fwd_acc_7\": 70.65217391304348, \"test_prim_fwd_acc_8\": 68.05555555555556, \"test_prim_fwd_acc_9\": 63.35403726708075, \"test_prim_fwd_acc_10\": 70.73170731707317, \"test_prim_fwd_acc_11\": 50.35971223021583, \"test_prim_fwd_acc_12\": 59.12408759124087, \"test_prim_fwd_acc_13\": 61.53846153846154, \"test_prim_fwd_acc_14\": 57.83132530120482, \"test_prim_fwd_acc_15\": 48.4375, \"test_prim_fwd_acc_16\": 47.142857142857146, \"test_prim_fwd_acc_17\": 56.81818181818182, \"test_prim_fwd_acc_18\": 52.38095238095238, \"test_prim_fwd_acc_19\": 63.41463414634146, \"test_prim_fwd_acc_20\": 65.95744680851064, \"test_prim_fwd_acc_21\": 55.55555555555556, \"test_prim_fwd_acc_22\": 58.62068965517241, \"test_prim_fwd_acc_23\": 65.0, \"test_prim_fwd_acc_24\": 38.095238095238095, \"test_prim_fwd_acc_25\": 60.0, \"test_prim_fwd_acc_26\": 50.0, \"test_prim_fwd_acc_27\": 57.142857142857146, \"test_prim_fwd_acc_28\": 33.333333333333336, \"test_prim_fwd_acc_29\": 23.529411764705884, \"test_prim_fwd_acc_30\": 83.33333333333333, \"test_prim_fwd_acc_31\": 12.5, \"test_prim_fwd_acc_32\": 50.0, \"test_prim_fwd_acc_33\": 66.66666666666667, \"test_prim_fwd_acc_34\": 40.0, \"test_prim_fwd_acc_35\": 66.66666666666667, \"test_prim_fwd_acc_36\": 66.66666666666667, \"test_prim_fwd_acc_37\": 60.0, \"test_prim_fwd_acc_38\": 20.0, \"test_prim_fwd_acc_39\": 0.0, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 0.0, \"test_prim_fwd_acc_42\": 0.0, \"test_prim_fwd_acc_43\": 33.333333333333336, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 33.333333333333336, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 66.66666666666667, \"test_prim_fwd_acc_54\": 0.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - New best score for valid_prim_fwd_acc: 61.119440\n",
            "INFO - 12/05/22 07:05:50 - 0:09:43 - Saving best-valid_prim_fwd_acc to ./dumped/derivfwd_modelfwd/5k6dzaasp3/best-valid_prim_fwd_acc.pth ...\n",
            "WARNING - 12/05/22 07:05:50 - 0:09:43 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:05:50 - 0:09:43 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:05:50 - 0:09:43 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:05:55 - 0:09:49 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 07:05:55 - 0:09:49 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:05:55 - 0:09:49 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:05:55 - 0:09:49 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:06:02 - 0:09:55 - ============ Starting epoch 3 ... ============\n",
            "INFO - 12/05/22 07:06:05 - 0:09:59 -    2360 -  127.68 equations/s -  5334.16 words/s - PRIM-FWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:10 - 0:10:03 -    2380 -  578.84 equations/s - 24442.96 words/s - PRIM-FWD:  0.0467 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:14 - 0:10:07 -    2400 -  593.13 equations/s - 24588.90 words/s - PRIM-FWD:  0.0439 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:19 - 0:10:13 -    2420 -  477.59 equations/s - 20047.21 words/s - PRIM-FWD:  0.0449 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:23 - 0:10:17 -    2440 -  658.53 equations/s - 27204.12 words/s - PRIM-FWD:  0.0467 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:28 - 0:10:21 -    2460 -  614.66 equations/s - 25545.16 words/s - PRIM-FWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:33 - 0:10:26 -    2480 -  498.84 equations/s - 21134.06 words/s - PRIM-FWD:  0.0455 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:37 - 0:10:31 -    2500 -  558.98 equations/s - 23039.85 words/s - PRIM-FWD:  0.0438 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:42 - 0:10:36 -    2520 -  500.09 equations/s - 21005.08 words/s - PRIM-FWD:  0.0458 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:47 - 0:10:41 -    2540 -  514.47 equations/s - 21794.97 words/s - PRIM-FWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:51 - 0:10:45 -    2560 -  618.64 equations/s - 25426.45 words/s - PRIM-FWD:  0.0472 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:06:56 - 0:10:49 -    2580 -  557.83 equations/s - 23259.25 words/s - PRIM-FWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:01 - 0:10:54 -    2600 -  505.15 equations/s - 21008.14 words/s - PRIM-FWD:  0.0431 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:06 - 0:10:59 -    2620 -  552.75 equations/s - 23000.05 words/s - PRIM-FWD:  0.0433 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:10 - 0:11:03 -    2640 -  599.64 equations/s - 25107.68 words/s - PRIM-FWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:14 - 0:11:08 -    2660 -  580.62 equations/s - 23911.39 words/s - PRIM-FWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:20 - 0:11:13 -    2680 -  469.49 equations/s - 19508.81 words/s - PRIM-FWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:25 - 0:11:18 -    2700 -  511.75 equations/s - 21608.70 words/s - PRIM-FWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:29 - 0:11:23 -    2720 -  586.40 equations/s - 24177.24 words/s - PRIM-FWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:35 - 0:11:28 -    2740 -  468.44 equations/s - 19564.79 words/s - PRIM-FWD:  0.0408 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:40 - 0:11:33 -    2760 -  516.89 equations/s - 21574.92 words/s - PRIM-FWD:  0.0459 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:45 - 0:11:38 -    2780 -  490.72 equations/s - 20462.73 words/s - PRIM-FWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:49 - 0:11:42 -    2800 -  622.16 equations/s - 25924.77 words/s - PRIM-FWD:  0.0434 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:54 - 0:11:47 -    2820 -  500.29 equations/s - 21124.41 words/s - PRIM-FWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:07:59 - 0:11:52 -    2840 -  552.10 equations/s - 23140.84 words/s - PRIM-FWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:03 - 0:11:56 -    2860 -  602.64 equations/s - 24845.95 words/s - PRIM-FWD:  0.0448 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:08 - 0:12:01 -    2880 -  554.54 equations/s - 22974.40 words/s - PRIM-FWD:  0.0440 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:12 - 0:12:05 -    2900 -  632.03 equations/s - 26248.08 words/s - PRIM-FWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:17 - 0:12:10 -    2920 -  512.39 equations/s - 21481.54 words/s - PRIM-FWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:22 - 0:12:15 -    2940 -  515.61 equations/s - 21513.76 words/s - PRIM-FWD:  0.0436 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:26 - 0:12:19 -    2960 -  621.64 equations/s - 25762.76 words/s - PRIM-FWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:31 - 0:12:24 -    2980 -  522.49 equations/s - 22101.12 words/s - PRIM-FWD:  0.0441 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:36 - 0:12:29 -    3000 -  479.10 equations/s - 20187.02 words/s - PRIM-FWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:41 - 0:12:34 -    3020 -  514.08 equations/s - 21771.10 words/s - PRIM-FWD:  0.0444 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:46 - 0:12:39 -    3040 -  532.16 equations/s - 22517.72 words/s - PRIM-FWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:51 - 0:12:44 -    3060 -  486.26 equations/s - 20651.99 words/s - PRIM-FWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:08:55 - 0:12:49 -    3080 -  603.52 equations/s - 24784.30 words/s - PRIM-FWD:  0.0409 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:00 - 0:12:53 -    3100 -  537.64 equations/s - 22361.81 words/s - PRIM-FWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:04 - 0:12:58 -    3120 -  577.66 equations/s - 23925.12 words/s - PRIM-FWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:06 - 0:12:59 - ============ End of epoch 3 ============\n",
            "INFO - 12/05/22 07:09:06 - 0:12:59 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:09:06 - 0:12:59 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 07:09:06 - 0:12:59 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:09:06 - 0:12:59 - 0/2001\n",
            "INFO - 12/05/22 07:09:06 - 0:13:00 - 128/2001\n",
            "INFO - 12/05/22 07:09:06 - 0:13:00 - 256/2001\n",
            "INFO - 12/05/22 07:09:06 - 0:13:00 - 384/2001\n",
            "INFO - 12/05/22 07:09:06 - 0:13:00 - 512/2001\n",
            "INFO - 12/05/22 07:09:06 - 0:13:00 - 640/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 768/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 896/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 1024/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 1152/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 1280/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 1408/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:00 - 1536/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:01 - 1664/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:01 - 1792/2001\n",
            "INFO - 12/05/22 07:09:07 - 0:13:01 - 1920/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 1200/2001 (59.97001499250375%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 0/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 128/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 256/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 384/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 512/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 640/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 768/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:01 - 896/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:02 - 1024/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:02 - 1152/2001\n",
            "INFO - 12/05/22 07:09:08 - 0:13:02 - 1280/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1408/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1536/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1664/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1792/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1920/2001\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - 1178/2001 (58.87056471764118%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - epoch -> 3.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_xe_loss -> 3.696495\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc -> 59.970015\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_4 -> 81.395349\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_5 -> 69.014085\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_6 -> 68.939394\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_7 -> 66.887417\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_8 -> 66.523605\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_9 -> 67.647059\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_10 -> 57.142857\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_11 -> 65.648855\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_12 -> 56.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_13 -> 60.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_14 -> 60.377358\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_15 -> 55.555556\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_16 -> 55.405405\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_17 -> 55.813953\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_18 -> 52.173913\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_19 -> 47.727273\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_20 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_21 -> 60.869565\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_22 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_23 -> 63.636364\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_24 -> 25.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_25 -> 44.444444\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_26 -> 46.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_27 -> 46.153846\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_28 -> 38.461538\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_29 -> 36.363636\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_30 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_34 -> 75.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_37 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_40 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_45 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_71 -> 100.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_xe_loss -> 4.042404\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc -> 58.870565\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_3 -> 77.777778\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_4 -> 62.222222\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_5 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_6 -> 73.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_8 -> 67.129630\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_9 -> 63.975155\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_10 -> 65.243902\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_11 -> 53.956835\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_12 -> 54.014599\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_13 -> 62.500000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_14 -> 59.036145\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_15 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_16 -> 41.428571\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_17 -> 52.272727\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_18 -> 42.857143\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_19 -> 48.780488\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_20 -> 68.085106\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_21 -> 40.740741\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_22 -> 51.724138\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_23 -> 65.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_24 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_25 -> 60.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_26 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_27 -> 42.857143\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_28 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_29 -> 29.411765\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_31 -> 12.500000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_34 -> 40.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_35 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_37 -> 60.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_38 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_39 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_43 -> 66.666667\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_53 -> 33.333333\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - __log__:{\"epoch\": 3, \"valid_prim_fwd_xe_loss\": 3.6964953192349137, \"valid_prim_fwd_acc\": 59.97001499250375, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 70.0, \"valid_prim_fwd_acc_4\": 81.3953488372093, \"valid_prim_fwd_acc_5\": 69.01408450704226, \"valid_prim_fwd_acc_6\": 68.93939393939394, \"valid_prim_fwd_acc_7\": 66.88741721854305, \"valid_prim_fwd_acc_8\": 66.52360515021459, \"valid_prim_fwd_acc_9\": 67.6470588235294, \"valid_prim_fwd_acc_10\": 57.142857142857146, \"valid_prim_fwd_acc_11\": 65.64885496183206, \"valid_prim_fwd_acc_12\": 56.0, \"valid_prim_fwd_acc_13\": 60.0, \"valid_prim_fwd_acc_14\": 60.37735849056604, \"valid_prim_fwd_acc_15\": 55.55555555555556, \"valid_prim_fwd_acc_16\": 55.4054054054054, \"valid_prim_fwd_acc_17\": 55.81395348837209, \"valid_prim_fwd_acc_18\": 52.17391304347826, \"valid_prim_fwd_acc_19\": 47.72727272727273, \"valid_prim_fwd_acc_20\": 50.0, \"valid_prim_fwd_acc_21\": 60.869565217391305, \"valid_prim_fwd_acc_22\": 33.333333333333336, \"valid_prim_fwd_acc_23\": 63.63636363636363, \"valid_prim_fwd_acc_24\": 25.0, \"valid_prim_fwd_acc_25\": 44.44444444444444, \"valid_prim_fwd_acc_26\": 46.666666666666664, \"valid_prim_fwd_acc_27\": 46.15384615384615, \"valid_prim_fwd_acc_28\": 38.46153846153846, \"valid_prim_fwd_acc_29\": 36.36363636363637, \"valid_prim_fwd_acc_30\": 33.333333333333336, \"valid_prim_fwd_acc_31\": 50.0, \"valid_prim_fwd_acc_32\": 33.333333333333336, \"valid_prim_fwd_acc_33\": 50.0, \"valid_prim_fwd_acc_34\": 75.0, \"valid_prim_fwd_acc_35\": 50.0, \"valid_prim_fwd_acc_36\": 100.0, \"valid_prim_fwd_acc_37\": 50.0, \"valid_prim_fwd_acc_39\": 50.0, \"valid_prim_fwd_acc_40\": 66.66666666666667, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 0.0, \"valid_prim_fwd_acc_45\": 33.333333333333336, \"valid_prim_fwd_acc_46\": 100.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 0.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 100.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 0.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 0.0, \"valid_prim_fwd_acc_103\": 0.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 4.042404121425616, \"test_prim_fwd_acc\": 58.87056471764118, \"test_prim_fwd_acc_3\": 77.77777777777777, \"test_prim_fwd_acc_4\": 62.22222222222222, \"test_prim_fwd_acc_5\": 66.66666666666667, \"test_prim_fwd_acc_6\": 73.33333333333333, \"test_prim_fwd_acc_7\": 71.73913043478261, \"test_prim_fwd_acc_8\": 67.12962962962963, \"test_prim_fwd_acc_9\": 63.975155279503106, \"test_prim_fwd_acc_10\": 65.2439024390244, \"test_prim_fwd_acc_11\": 53.9568345323741, \"test_prim_fwd_acc_12\": 54.01459854014598, \"test_prim_fwd_acc_13\": 62.5, \"test_prim_fwd_acc_14\": 59.036144578313255, \"test_prim_fwd_acc_15\": 50.0, \"test_prim_fwd_acc_16\": 41.42857142857143, \"test_prim_fwd_acc_17\": 52.27272727272727, \"test_prim_fwd_acc_18\": 42.857142857142854, \"test_prim_fwd_acc_19\": 48.78048780487805, \"test_prim_fwd_acc_20\": 68.08510638297872, \"test_prim_fwd_acc_21\": 40.74074074074074, \"test_prim_fwd_acc_22\": 51.724137931034484, \"test_prim_fwd_acc_23\": 65.0, \"test_prim_fwd_acc_24\": 33.333333333333336, \"test_prim_fwd_acc_25\": 60.0, \"test_prim_fwd_acc_26\": 50.0, \"test_prim_fwd_acc_27\": 42.857142857142854, \"test_prim_fwd_acc_28\": 33.333333333333336, \"test_prim_fwd_acc_29\": 29.41176470588235, \"test_prim_fwd_acc_30\": 66.66666666666667, \"test_prim_fwd_acc_31\": 12.5, \"test_prim_fwd_acc_32\": 33.333333333333336, \"test_prim_fwd_acc_33\": 66.66666666666667, \"test_prim_fwd_acc_34\": 40.0, \"test_prim_fwd_acc_35\": 33.333333333333336, \"test_prim_fwd_acc_36\": 66.66666666666667, \"test_prim_fwd_acc_37\": 60.0, \"test_prim_fwd_acc_38\": 0.0, \"test_prim_fwd_acc_39\": 0.0, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 50.0, \"test_prim_fwd_acc_42\": 0.0, \"test_prim_fwd_acc_43\": 66.66666666666667, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 33.333333333333336, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 33.333333333333336, \"test_prim_fwd_acc_54\": 0.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 07:09:09 - 0:13:02 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 07:09:09 - 0:13:02 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:09:09 - 0:13:03 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:09:09 - 0:13:03 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:09:14 - 0:13:08 - ============ Starting epoch 4 ... ============\n",
            "INFO - 12/05/22 07:09:17 - 0:13:10 -    3140 -  208.06 equations/s -  8488.74 words/s - PRIM-FWD:  0.0426 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:21 - 0:13:14 -    3160 -  593.66 equations/s - 24423.19 words/s - PRIM-FWD:  0.0481 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:25 - 0:13:19 -    3180 -  617.44 equations/s - 25727.34 words/s - PRIM-FWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:30 - 0:13:23 -    3200 -  597.97 equations/s - 24829.66 words/s - PRIM-FWD:  0.0413 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:34 - 0:13:27 -    3220 -  591.97 equations/s - 24548.21 words/s - PRIM-FWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:38 - 0:13:32 -    3240 -  578.80 equations/s - 23902.53 words/s - PRIM-FWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:44 - 0:13:37 -    3260 -  482.37 equations/s - 20551.23 words/s - PRIM-FWD:  0.0402 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:48 - 0:13:41 -    3280 -  572.62 equations/s - 23874.64 words/s - PRIM-FWD:  0.0377 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:53 - 0:13:46 -    3300 -  505.70 equations/s - 21025.57 words/s - PRIM-FWD:  0.0403 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:09:57 - 0:13:51 -    3320 -  615.56 equations/s - 25584.33 words/s - PRIM-FWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:02 - 0:13:56 -    3340 -  496.51 equations/s - 20597.17 words/s - PRIM-FWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:07 - 0:14:00 -    3360 -  548.50 equations/s - 22809.08 words/s - PRIM-FWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:12 - 0:14:05 -    3380 -  546.81 equations/s - 22516.48 words/s - PRIM-FWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:16 - 0:14:09 -    3400 -  612.66 equations/s - 25153.38 words/s - PRIM-FWD:  0.0406 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:20 - 0:14:13 -    3420 -  621.55 equations/s - 25492.00 words/s - PRIM-FWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:25 - 0:14:18 -    3440 -  568.04 equations/s - 23658.50 words/s - PRIM-FWD:  0.0410 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:29 - 0:14:22 -    3460 -  637.58 equations/s - 26454.74 words/s - PRIM-FWD:  0.0423 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:33 - 0:14:26 -    3480 -  627.12 equations/s - 25639.74 words/s - PRIM-FWD:  0.0369 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:37 - 0:14:30 -    3500 -  577.65 equations/s - 24177.28 words/s - PRIM-FWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:42 - 0:14:35 -    3520 -  559.76 equations/s - 23343.03 words/s - PRIM-FWD:  0.0452 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:46 - 0:14:39 -    3540 -  596.80 equations/s - 24696.91 words/s - PRIM-FWD:  0.0428 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:51 - 0:14:44 -    3560 -  548.93 equations/s - 23094.93 words/s - PRIM-FWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:10:56 - 0:14:49 -    3580 -  475.95 equations/s - 20007.69 words/s - PRIM-FWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:01 - 0:14:55 -    3600 -  490.49 equations/s - 20551.66 words/s - PRIM-FWD:  0.0392 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:05 - 0:14:59 -    3620 -  625.52 equations/s - 26408.48 words/s - PRIM-FWD:  0.0417 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:10 - 0:15:03 -    3640 -  542.23 equations/s - 22855.16 words/s - PRIM-FWD:  0.0435 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:15 - 0:15:09 -    3660 -  469.71 equations/s - 19883.14 words/s - PRIM-FWD:  0.0432 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:21 - 0:15:14 -    3680 -  493.10 equations/s - 20562.08 words/s - PRIM-FWD:  0.0447 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:25 - 0:15:18 -    3700 -  638.23 equations/s - 26266.23 words/s - PRIM-FWD:  0.0404 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:29 - 0:15:23 -    3720 -  562.34 equations/s - 22951.93 words/s - PRIM-FWD:  0.0419 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:34 - 0:15:27 -    3740 -  585.43 equations/s - 24211.30 words/s - PRIM-FWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:38 - 0:15:31 -    3760 -  570.33 equations/s - 23777.36 words/s - PRIM-FWD:  0.0418 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:42 - 0:15:36 -    3780 -  585.76 equations/s - 24223.99 words/s - PRIM-FWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:47 - 0:15:40 -    3800 -  578.13 equations/s - 23827.93 words/s - PRIM-FWD:  0.0412 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:52 - 0:15:45 -    3820 -  556.04 equations/s - 23053.35 words/s - PRIM-FWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:11:56 - 0:15:49 -    3840 -  580.06 equations/s - 24025.20 words/s - PRIM-FWD:  0.0379 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:01 - 0:15:54 -    3860 -  502.91 equations/s - 21035.73 words/s - PRIM-FWD:  0.0416 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:06 - 0:15:59 -    3880 -  520.55 equations/s - 21627.94 words/s - PRIM-FWD:  0.0390 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:10 - 0:16:03 -    3900 -  625.66 equations/s - 25693.81 words/s - PRIM-FWD:  0.0376 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - ============ End of epoch 4 ============\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - 0/2001\n",
            "INFO - 12/05/22 07:12:12 - 0:16:06 - 128/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 256/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 384/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 512/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 640/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 768/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 896/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 1024/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:06 - 1152/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:07 - 1280/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:07 - 1408/2001\n",
            "INFO - 12/05/22 07:12:13 - 0:16:07 - 1536/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 1664/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 1792/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 1920/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 1168/2001 (58.370814592703645%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 0/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 128/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 256/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 384/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:07 - 512/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:08 - 640/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:08 - 768/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:08 - 896/2001\n",
            "INFO - 12/05/22 07:12:14 - 0:16:08 - 1024/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1152/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1280/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1408/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1536/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1664/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:08 - 1792/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - 1920/2001\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - 1180/2001 (58.97051474262869%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - epoch -> 4.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_xe_loss -> 4.105194\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc -> 58.370815\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_3 -> 80.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_4 -> 81.395349\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_6 -> 70.454545\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_7 -> 72.847682\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_8 -> 66.523605\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_9 -> 65.294118\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_10 -> 52.795031\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_11 -> 62.595420\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_12 -> 54.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_13 -> 55.789474\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_14 -> 54.716981\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_15 -> 48.611111\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_16 -> 55.405405\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_17 -> 51.162791\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_18 -> 52.173913\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_19 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_20 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_21 -> 65.217391\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_22 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_23 -> 45.454545\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_24 -> 20.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_25 -> 38.888889\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_26 -> 60.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_27 -> 30.769231\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_28 -> 46.153846\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_29 -> 36.363636\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_30 -> 55.555556\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_31 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_32 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_34 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_36 -> 100.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_40 -> 66.666667\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_44 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_80 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_101 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_xe_loss -> 4.424781\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc -> 58.970515\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_3 -> 77.777778\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_4 -> 68.888889\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_5 -> 70.175439\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_6 -> 76.296296\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_7 -> 71.195652\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_8 -> 66.666667\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_9 -> 64.596273\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_10 -> 67.073171\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_11 -> 53.237410\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_12 -> 55.474453\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_13 -> 57.692308\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_14 -> 59.036145\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_15 -> 42.187500\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_16 -> 42.857143\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_17 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_18 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_19 -> 58.536585\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_20 -> 61.702128\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_21 -> 44.444444\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_22 -> 41.379310\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_23 -> 65.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_24 -> 47.619048\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_25 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_26 -> 42.857143\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_27 -> 28.571429\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_28 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_29 -> 29.411765\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_30 -> 66.666667\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_31 -> 25.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_32 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_33 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_34 -> 20.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_35 -> 66.666667\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_36 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_37 -> 80.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_38 -> 20.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_39 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_41 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_43 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_46 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_53 -> 33.333333\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - __log__:{\"epoch\": 4, \"valid_prim_fwd_xe_loss\": 4.105193558006153, \"valid_prim_fwd_acc\": 58.370814592703645, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 80.0, \"valid_prim_fwd_acc_4\": 81.3953488372093, \"valid_prim_fwd_acc_5\": 66.19718309859155, \"valid_prim_fwd_acc_6\": 70.45454545454545, \"valid_prim_fwd_acc_7\": 72.8476821192053, \"valid_prim_fwd_acc_8\": 66.52360515021459, \"valid_prim_fwd_acc_9\": 65.29411764705883, \"valid_prim_fwd_acc_10\": 52.79503105590062, \"valid_prim_fwd_acc_11\": 62.595419847328245, \"valid_prim_fwd_acc_12\": 54.0, \"valid_prim_fwd_acc_13\": 55.78947368421053, \"valid_prim_fwd_acc_14\": 54.716981132075475, \"valid_prim_fwd_acc_15\": 48.611111111111114, \"valid_prim_fwd_acc_16\": 55.4054054054054, \"valid_prim_fwd_acc_17\": 51.16279069767442, \"valid_prim_fwd_acc_18\": 52.17391304347826, \"valid_prim_fwd_acc_19\": 50.0, \"valid_prim_fwd_acc_20\": 50.0, \"valid_prim_fwd_acc_21\": 65.21739130434783, \"valid_prim_fwd_acc_22\": 33.333333333333336, \"valid_prim_fwd_acc_23\": 45.45454545454545, \"valid_prim_fwd_acc_24\": 20.0, \"valid_prim_fwd_acc_25\": 38.888888888888886, \"valid_prim_fwd_acc_26\": 60.0, \"valid_prim_fwd_acc_27\": 30.76923076923077, \"valid_prim_fwd_acc_28\": 46.15384615384615, \"valid_prim_fwd_acc_29\": 36.36363636363637, \"valid_prim_fwd_acc_30\": 55.55555555555556, \"valid_prim_fwd_acc_31\": 33.333333333333336, \"valid_prim_fwd_acc_32\": 0.0, \"valid_prim_fwd_acc_33\": 50.0, \"valid_prim_fwd_acc_34\": 50.0, \"valid_prim_fwd_acc_35\": 50.0, \"valid_prim_fwd_acc_36\": 100.0, \"valid_prim_fwd_acc_37\": 75.0, \"valid_prim_fwd_acc_39\": 50.0, \"valid_prim_fwd_acc_40\": 66.66666666666667, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 0.0, \"valid_prim_fwd_acc_45\": 0.0, \"valid_prim_fwd_acc_46\": 0.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 0.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 0.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 0.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 0.0, \"valid_prim_fwd_acc_103\": 0.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 4.4247812312105665, \"test_prim_fwd_acc\": 58.97051474262869, \"test_prim_fwd_acc_3\": 77.77777777777777, \"test_prim_fwd_acc_4\": 68.88888888888889, \"test_prim_fwd_acc_5\": 70.17543859649123, \"test_prim_fwd_acc_6\": 76.29629629629629, \"test_prim_fwd_acc_7\": 71.19565217391305, \"test_prim_fwd_acc_8\": 66.66666666666667, \"test_prim_fwd_acc_9\": 64.59627329192547, \"test_prim_fwd_acc_10\": 67.07317073170732, \"test_prim_fwd_acc_11\": 53.23741007194245, \"test_prim_fwd_acc_12\": 55.47445255474452, \"test_prim_fwd_acc_13\": 57.69230769230769, \"test_prim_fwd_acc_14\": 59.036144578313255, \"test_prim_fwd_acc_15\": 42.1875, \"test_prim_fwd_acc_16\": 42.857142857142854, \"test_prim_fwd_acc_17\": 50.0, \"test_prim_fwd_acc_18\": 50.0, \"test_prim_fwd_acc_19\": 58.53658536585366, \"test_prim_fwd_acc_20\": 61.702127659574465, \"test_prim_fwd_acc_21\": 44.44444444444444, \"test_prim_fwd_acc_22\": 41.37931034482759, \"test_prim_fwd_acc_23\": 65.0, \"test_prim_fwd_acc_24\": 47.61904761904762, \"test_prim_fwd_acc_25\": 50.0, \"test_prim_fwd_acc_26\": 42.857142857142854, \"test_prim_fwd_acc_27\": 28.571428571428573, \"test_prim_fwd_acc_28\": 33.333333333333336, \"test_prim_fwd_acc_29\": 29.41176470588235, \"test_prim_fwd_acc_30\": 66.66666666666667, \"test_prim_fwd_acc_31\": 25.0, \"test_prim_fwd_acc_32\": 33.333333333333336, \"test_prim_fwd_acc_33\": 33.333333333333336, \"test_prim_fwd_acc_34\": 20.0, \"test_prim_fwd_acc_35\": 66.66666666666667, \"test_prim_fwd_acc_36\": 33.333333333333336, \"test_prim_fwd_acc_37\": 80.0, \"test_prim_fwd_acc_38\": 20.0, \"test_prim_fwd_acc_39\": 33.333333333333336, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 50.0, \"test_prim_fwd_acc_42\": 0.0, \"test_prim_fwd_acc_43\": 33.333333333333336, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 0.0, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 33.333333333333336, \"test_prim_fwd_acc_54\": 0.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 07:12:15 - 0:16:09 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 07:12:15 - 0:16:09 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:12:15 - 0:16:09 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:12:15 - 0:16:09 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:12:23 - 0:16:16 - ============ Starting epoch 5 ... ============\n",
            "INFO - 12/05/22 07:12:25 - 0:16:19 -    3920 -  165.91 equations/s -  7014.94 words/s - PRIM-FWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:30 - 0:16:23 -    3940 -  564.96 equations/s - 23689.55 words/s - PRIM-FWD:  0.0405 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:35 - 0:16:28 -    3960 -  555.40 equations/s - 23242.10 words/s - PRIM-FWD:  0.0414 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:39 - 0:16:32 -    3980 -  587.77 equations/s - 24613.72 words/s - PRIM-FWD:  0.0396 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:43 - 0:16:37 -    4000 -  567.95 equations/s - 23650.18 words/s - PRIM-FWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:48 - 0:16:41 -    4020 -  567.23 equations/s - 23449.87 words/s - PRIM-FWD:  0.0367 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:53 - 0:16:46 -    4040 -  525.60 equations/s - 21881.20 words/s - PRIM-FWD:  0.0399 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:12:57 - 0:16:50 -    4060 -  692.11 equations/s - 28442.78 words/s - PRIM-FWD:  0.0400 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:01 - 0:16:55 -    4080 -  528.09 equations/s - 22033.63 words/s - PRIM-FWD:  0.0364 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:06 - 0:17:00 -    4100 -  519.95 equations/s - 21894.32 words/s - PRIM-FWD:  0.0376 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:11 - 0:17:04 -    4120 -  557.60 equations/s - 23202.62 words/s - PRIM-FWD:  0.0411 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:16 - 0:17:09 -    4140 -  524.57 equations/s - 22134.72 words/s - PRIM-FWD:  0.0415 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:20 - 0:17:13 -    4160 -  620.38 equations/s - 25736.28 words/s - PRIM-FWD:  0.0374 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:25 - 0:17:18 -    4180 -  557.38 equations/s - 23190.58 words/s - PRIM-FWD:  0.0389 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:29 - 0:17:22 -    4200 -  583.77 equations/s - 24475.11 words/s - PRIM-FWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:34 - 0:17:27 -    4220 -  528.01 equations/s - 22423.78 words/s - PRIM-FWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:38 - 0:17:32 -    4240 -  555.38 equations/s - 23477.45 words/s - PRIM-FWD:  0.0380 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:43 - 0:17:36 -    4260 -  552.90 equations/s - 23121.17 words/s - PRIM-FWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:49 - 0:17:42 -    4280 -  446.21 equations/s - 19132.14 words/s - PRIM-FWD:  0.0383 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:54 - 0:17:47 -    4300 -  511.48 equations/s - 21314.39 words/s - PRIM-FWD:  0.0407 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:13:58 - 0:17:51 -    4320 -  591.00 equations/s - 24723.13 words/s - PRIM-FWD:  0.0420 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:02 - 0:17:56 -    4340 -  617.49 equations/s - 25809.66 words/s - PRIM-FWD:  0.0388 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:07 - 0:18:00 -    4360 -  590.22 equations/s - 24779.76 words/s - PRIM-FWD:  0.0385 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:11 - 0:18:05 -    4380 -  526.61 equations/s - 22211.84 words/s - PRIM-FWD:  0.0393 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:16 - 0:18:10 -    4400 -  529.89 equations/s - 22018.58 words/s - PRIM-FWD:  0.0374 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:21 - 0:18:14 -    4420 -  597.65 equations/s - 24896.97 words/s - PRIM-FWD:  0.0395 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:25 - 0:18:18 -    4440 -  595.03 equations/s - 24726.52 words/s - PRIM-FWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:30 - 0:18:23 -    4460 -  484.70 equations/s - 20330.31 words/s - PRIM-FWD:  0.0383 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:35 - 0:18:28 -    4480 -  546.49 equations/s - 22866.35 words/s - PRIM-FWD:  0.0397 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:39 - 0:18:33 -    4500 -  548.16 equations/s - 22857.28 words/s - PRIM-FWD:  0.0369 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:44 - 0:18:37 -    4520 -  585.72 equations/s - 24218.38 words/s - PRIM-FWD:  0.0358 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:48 - 0:18:41 -    4540 -  598.08 equations/s - 24690.94 words/s - PRIM-FWD:  0.0340 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:52 - 0:18:46 -    4560 -  618.28 equations/s - 25640.46 words/s - PRIM-FWD:  0.0347 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:14:57 - 0:18:50 -    4580 -  557.64 equations/s - 22921.88 words/s - PRIM-FWD:  0.0366 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:02 - 0:18:55 -    4600 -  518.78 equations/s - 21515.11 words/s - PRIM-FWD:  0.0394 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:06 - 0:18:59 -    4620 -  613.80 equations/s - 25448.30 words/s - PRIM-FWD:  0.0401 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:10 - 0:19:04 -    4640 -  593.01 equations/s - 24782.11 words/s - PRIM-FWD:  0.0344 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:15 - 0:19:08 -    4660 -  547.22 equations/s - 22753.49 words/s - PRIM-FWD:  0.0384 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:21 - 0:19:14 -    4680 -  459.24 equations/s - 19395.04 words/s - PRIM-FWD:  0.0368 - model LR: 1.0000e-04\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - ============ End of epoch 5 ============\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - Creating valid iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - Loading data from deriv_fwd.valid ...\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - 0/2001\n",
            "INFO - 12/05/22 07:15:23 - 0:19:17 - 128/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 256/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 384/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 512/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 640/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 768/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 896/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 1024/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:17 - 1152/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:18 - 1280/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:18 - 1408/2001\n",
            "INFO - 12/05/22 07:15:24 - 0:19:18 - 1536/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 1664/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 1792/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 1920/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 1157/2001 (57.821089455272364%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - Creating test iterator for prim_fwd ...\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - Loading data from deriv_fwd.test ...\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - Loaded 2001 equations from the disk.\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 0/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 128/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 256/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 384/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:18 - 512/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:19 - 640/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:19 - 768/2001\n",
            "INFO - 12/05/22 07:15:25 - 0:19:19 - 896/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1024/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1152/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1280/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1408/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1536/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1664/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:19 - 1792/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - 1920/2001\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - 1189/2001 (59.42028985507246%) equations were evaluated correctly.\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - epoch -> 5.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_xe_loss -> 3.873750\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc -> 57.821089\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_2 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_3 -> 70.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_4 -> 74.418605\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_5 -> 66.197183\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_6 -> 68.939394\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_7 -> 68.874172\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_8 -> 65.236052\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_9 -> 65.294118\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_10 -> 52.795031\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_11 -> 58.778626\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_12 -> 54.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_13 -> 55.789474\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_14 -> 53.773585\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_15 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_16 -> 55.405405\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_17 -> 48.837209\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_18 -> 54.347826\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_19 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_20 -> 46.875000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_21 -> 60.869565\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_22 -> 42.857143\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_23 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_24 -> 25.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_25 -> 27.777778\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_26 -> 46.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_27 -> 61.538462\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_28 -> 53.846154\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_29 -> 27.272727\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_30 -> 33.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_31 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_32 -> 16.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_33 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_34 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_35 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_36 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_37 -> 75.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_39 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_40 -> 66.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_41 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_44 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_46 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_51 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_53 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_55 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_58 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_61 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_62 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_64 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_68 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_76 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_80 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_85 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_90 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_93 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_95 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_97 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_101 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_103 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_104 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - valid_prim_fwd_acc_129 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_xe_loss -> 3.952216\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc -> 59.420290\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_3 -> 66.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_4 -> 71.111111\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_5 -> 68.421053\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_6 -> 74.074074\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_7 -> 71.739130\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_8 -> 62.962963\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_9 -> 63.975155\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_10 -> 66.463415\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_11 -> 53.237410\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_12 -> 52.554745\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_13 -> 64.423077\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_14 -> 55.421687\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_15 -> 48.437500\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_16 -> 44.285714\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_17 -> 54.545455\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_18 -> 45.238095\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_19 -> 58.536585\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_20 -> 65.957447\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_21 -> 48.148148\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_22 -> 58.620690\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_23 -> 75.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_24 -> 42.857143\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_25 -> 45.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_26 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_27 -> 42.857143\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_28 -> 44.444444\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_29 -> 29.411765\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_30 -> 83.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_31 -> 12.500000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_32 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_33 -> 66.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_34 -> 40.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_35 -> 33.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_36 -> 66.666667\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_37 -> 60.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_38 -> 40.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_39 -> 33.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_40 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_41 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_42 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_43 -> 100.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_45 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_46 -> 33.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_47 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_48 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_49 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_50 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_52 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_53 -> 33.333333\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_54 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_55 -> 50.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_56 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_57 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_64 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_65 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_70 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_71 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_78 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_81 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_82 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_84 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_127 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - test_prim_fwd_acc_144 -> 0.000000\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - __log__:{\"epoch\": 5, \"valid_prim_fwd_xe_loss\": 3.873749785873665, \"valid_prim_fwd_acc\": 57.821089455272364, \"valid_prim_fwd_acc_2\": 50.0, \"valid_prim_fwd_acc_3\": 70.0, \"valid_prim_fwd_acc_4\": 74.4186046511628, \"valid_prim_fwd_acc_5\": 66.19718309859155, \"valid_prim_fwd_acc_6\": 68.93939393939394, \"valid_prim_fwd_acc_7\": 68.87417218543047, \"valid_prim_fwd_acc_8\": 65.23605150214593, \"valid_prim_fwd_acc_9\": 65.29411764705883, \"valid_prim_fwd_acc_10\": 52.79503105590062, \"valid_prim_fwd_acc_11\": 58.778625954198475, \"valid_prim_fwd_acc_12\": 54.666666666666664, \"valid_prim_fwd_acc_13\": 55.78947368421053, \"valid_prim_fwd_acc_14\": 53.77358490566038, \"valid_prim_fwd_acc_15\": 50.0, \"valid_prim_fwd_acc_16\": 55.4054054054054, \"valid_prim_fwd_acc_17\": 48.83720930232558, \"valid_prim_fwd_acc_18\": 54.34782608695652, \"valid_prim_fwd_acc_19\": 50.0, \"valid_prim_fwd_acc_20\": 46.875, \"valid_prim_fwd_acc_21\": 60.869565217391305, \"valid_prim_fwd_acc_22\": 42.857142857142854, \"valid_prim_fwd_acc_23\": 50.0, \"valid_prim_fwd_acc_24\": 25.0, \"valid_prim_fwd_acc_25\": 27.77777777777778, \"valid_prim_fwd_acc_26\": 46.666666666666664, \"valid_prim_fwd_acc_27\": 61.53846153846154, \"valid_prim_fwd_acc_28\": 53.84615384615385, \"valid_prim_fwd_acc_29\": 27.272727272727273, \"valid_prim_fwd_acc_30\": 33.333333333333336, \"valid_prim_fwd_acc_31\": 50.0, \"valid_prim_fwd_acc_32\": 16.666666666666668, \"valid_prim_fwd_acc_33\": 50.0, \"valid_prim_fwd_acc_34\": 100.0, \"valid_prim_fwd_acc_35\": 50.0, \"valid_prim_fwd_acc_36\": 50.0, \"valid_prim_fwd_acc_37\": 75.0, \"valid_prim_fwd_acc_39\": 50.0, \"valid_prim_fwd_acc_40\": 66.66666666666667, \"valid_prim_fwd_acc_41\": 100.0, \"valid_prim_fwd_acc_42\": 0.0, \"valid_prim_fwd_acc_44\": 50.0, \"valid_prim_fwd_acc_45\": 0.0, \"valid_prim_fwd_acc_46\": 100.0, \"valid_prim_fwd_acc_47\": 0.0, \"valid_prim_fwd_acc_49\": 0.0, \"valid_prim_fwd_acc_51\": 0.0, \"valid_prim_fwd_acc_53\": 100.0, \"valid_prim_fwd_acc_55\": 0.0, \"valid_prim_fwd_acc_56\": 0.0, \"valid_prim_fwd_acc_57\": 0.0, \"valid_prim_fwd_acc_58\": 0.0, \"valid_prim_fwd_acc_61\": 0.0, \"valid_prim_fwd_acc_62\": 50.0, \"valid_prim_fwd_acc_64\": 100.0, \"valid_prim_fwd_acc_65\": 0.0, \"valid_prim_fwd_acc_68\": 0.0, \"valid_prim_fwd_acc_71\": 0.0, \"valid_prim_fwd_acc_76\": 0.0, \"valid_prim_fwd_acc_80\": 100.0, \"valid_prim_fwd_acc_85\": 0.0, \"valid_prim_fwd_acc_90\": 0.0, \"valid_prim_fwd_acc_93\": 0.0, \"valid_prim_fwd_acc_95\": 0.0, \"valid_prim_fwd_acc_97\": 0.0, \"valid_prim_fwd_acc_101\": 50.0, \"valid_prim_fwd_acc_103\": 0.0, \"valid_prim_fwd_acc_104\": 0.0, \"valid_prim_fwd_acc_129\": 0.0, \"test_prim_fwd_xe_loss\": 3.9522160001542197, \"test_prim_fwd_acc\": 59.42028985507246, \"test_prim_fwd_acc_3\": 66.66666666666667, \"test_prim_fwd_acc_4\": 71.11111111111111, \"test_prim_fwd_acc_5\": 68.42105263157895, \"test_prim_fwd_acc_6\": 74.07407407407408, \"test_prim_fwd_acc_7\": 71.73913043478261, \"test_prim_fwd_acc_8\": 62.96296296296296, \"test_prim_fwd_acc_9\": 63.975155279503106, \"test_prim_fwd_acc_10\": 66.46341463414635, \"test_prim_fwd_acc_11\": 53.23741007194245, \"test_prim_fwd_acc_12\": 52.55474452554745, \"test_prim_fwd_acc_13\": 64.42307692307692, \"test_prim_fwd_acc_14\": 55.42168674698795, \"test_prim_fwd_acc_15\": 48.4375, \"test_prim_fwd_acc_16\": 44.285714285714285, \"test_prim_fwd_acc_17\": 54.54545454545455, \"test_prim_fwd_acc_18\": 45.23809523809524, \"test_prim_fwd_acc_19\": 58.53658536585366, \"test_prim_fwd_acc_20\": 65.95744680851064, \"test_prim_fwd_acc_21\": 48.148148148148145, \"test_prim_fwd_acc_22\": 58.62068965517241, \"test_prim_fwd_acc_23\": 75.0, \"test_prim_fwd_acc_24\": 42.857142857142854, \"test_prim_fwd_acc_25\": 45.0, \"test_prim_fwd_acc_26\": 50.0, \"test_prim_fwd_acc_27\": 42.857142857142854, \"test_prim_fwd_acc_28\": 44.44444444444444, \"test_prim_fwd_acc_29\": 29.41176470588235, \"test_prim_fwd_acc_30\": 83.33333333333333, \"test_prim_fwd_acc_31\": 12.5, \"test_prim_fwd_acc_32\": 50.0, \"test_prim_fwd_acc_33\": 66.66666666666667, \"test_prim_fwd_acc_34\": 40.0, \"test_prim_fwd_acc_35\": 33.333333333333336, \"test_prim_fwd_acc_36\": 66.66666666666667, \"test_prim_fwd_acc_37\": 60.0, \"test_prim_fwd_acc_38\": 40.0, \"test_prim_fwd_acc_39\": 33.333333333333336, \"test_prim_fwd_acc_40\": 0.0, \"test_prim_fwd_acc_41\": 0.0, \"test_prim_fwd_acc_42\": 0.0, \"test_prim_fwd_acc_43\": 100.0, \"test_prim_fwd_acc_45\": 0.0, \"test_prim_fwd_acc_46\": 33.333333333333336, \"test_prim_fwd_acc_47\": 0.0, \"test_prim_fwd_acc_48\": 0.0, \"test_prim_fwd_acc_49\": 0.0, \"test_prim_fwd_acc_50\": 0.0, \"test_prim_fwd_acc_52\": 0.0, \"test_prim_fwd_acc_53\": 33.333333333333336, \"test_prim_fwd_acc_54\": 0.0, \"test_prim_fwd_acc_55\": 50.0, \"test_prim_fwd_acc_56\": 0.0, \"test_prim_fwd_acc_57\": 0.0, \"test_prim_fwd_acc_64\": 0.0, \"test_prim_fwd_acc_65\": 0.0, \"test_prim_fwd_acc_70\": 0.0, \"test_prim_fwd_acc_71\": 0.0, \"test_prim_fwd_acc_78\": 0.0, \"test_prim_fwd_acc_81\": 0.0, \"test_prim_fwd_acc_82\": 0.0, \"test_prim_fwd_acc_84\": 0.0, \"test_prim_fwd_acc_127\": 0.0, \"test_prim_fwd_acc_144\": 0.0}\n",
            "INFO - 12/05/22 07:15:26 - 0:19:20 - Saving checkpoint to ./dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth ...\n",
            "WARNING - 12/05/22 07:15:26 - 0:19:20 - Saving decoder parameters ...\n",
            "WARNING - 12/05/22 07:15:26 - 0:19:20 - Saving encoder parameters ...\n",
            "WARNING - 12/05/22 07:15:26 - 0:19:20 - Saving model optimizer ...\n",
            "INFO - 12/05/22 07:15:32 - 0:19:25 - ============ Starting epoch 6 ... ============\n",
            "INFO - 12/05/22 07:15:33 - 0:19:27 -    4700 -  198.17 equations/s -  8332.12 words/s - PRIM-FWD:  0.0396 - model LR: 1.0000e-04\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6gyTII3e2BfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4549369-5aeb-4ef8-da10-be31d690dcfa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/dumped_exp12042022.zip /content/dumped"
      ],
      "metadata": {
        "id": "864-_zGs2CVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09418ef4-0599-42e2-a4ac-8e35a2eae5c7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/dumped/ (stored 0%)\n",
            "  adding: content/dumped/derivfwd_modelbwd/ (stored 0%)\n",
            "  adding: content/dumped/derivfwd_modelbwd/3snai9209s/ (stored 0%)\n",
            "  adding: content/dumped/derivfwd_modelbwd/3snai9209s/checkpoint.pth (deflated 27%)\n",
            "  adding: content/dumped/derivfwd_modelbwd/3snai9209s/train.log (deflated 89%)\n",
            "  adding: content/dumped/derivfwd_modelbwd/3snai9209s/params.pkl (deflated 40%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/ (stored 0%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/6gswznivsf/ (stored 0%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/6gswznivsf/checkpoint.pth (deflated 27%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/6gswznivsf/best-valid_prim_bwd_acc.pth (deflated 27%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/6gswznivsf/train.log (deflated 87%)\n",
            "  adding: content/dumped/deriv2fwd_modelbwd/6gswznivsf/params.pkl (deflated 40%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/ (stored 0%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/5k6dzaasp3/ (stored 0%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/5k6dzaasp3/checkpoint.pth (deflated 22%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/5k6dzaasp3/best-valid_prim_fwd_acc.pth (deflated 22%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/5k6dzaasp3/train.log (deflated 90%)\n",
            "  adding: content/dumped/derivfwd_modelfwd/5k6dzaasp3/params.pkl (deflated 40%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp dumped_exp12042022.zip \"drive/MyDrive/Aprendizaje Profundo/Proyecto final libretas/.\""
      ],
      "metadata": {
        "id": "8qRfmGZ_JdVH"
      },
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "G8op6FLXF-52",
        "6vs0v329g2Z4",
        "FS1m03-7g2aA",
        "oedX-JosGs9V",
        "IjBjvqYQG0P3"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}